# From Classic NLP Models to LLMs

Welcome to the third installment of the journey to becoming a GeoAI engineer. In this article, we focus on how we came to talk about GPT, Gemini & co.

## Introduction

In recent decades, Natural Language Processing (NLP) has undergone an extraordinary evolution, transitioning from "simple" statistical methods to modern **Large Language Models (LLMs)** with billions of parameters.

This revolution has not only been historical but, above all, **architectural**: new ideas (primarily we will discuss Transformers) have overcome limits previously considered insurmountable. In this structured guide (or rather, I hope it is), we retrace the fundamental stages of this evolution, from classic NLP models to today's LLMs, **analyzing for each phase the design choices, the limitations encountered, and the trade-offs**.

The goal is to build a **clear conceptual map**: to understand _why_ the Transformer changed everything, when it makes sense to use an LLM (and when not), and how LLMs fit into modern AI systems (RAG, agents, multimodality).

## NLP Before Transformers

Before the _deep learning_ era, NLP was dominated by **statistical and manual feature-based methods**. Some key approaches included:

- **Bag-of-Words & TF-IDF:** A document is represented as a _bag of words_, ignoring order. Each word is a dimension in the (often huge and sparse) vector; TF-IDF weights attenuate its importance if the word is too common. This model is simple but completely loses syntactic structure: _"black cat"_ and _"cat black"_ are identical for Bag-of-Words. Furthermore, words like _"blue"_ and _"light blue"_ are orthogonal vectors, implying no semantic similarity.
- **N-gram Models:** They introduce a minimum of context by considering sequences of _N_ words. A **trigram**, for example, estimates the probability of a word based on the 2 preceding ones (N=3). They work well for short or very frequent phrases (e.g., _"thank you very" â†’ "much"_), but have significant limitations: they generate non-zero probabilities only for phrases seen or very similar to the training data, suffer from **sparseness** (rare combinations are not covered), and use a rigid fixed-length context[\[1\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20fell%20short%3A). In practice, _"I want to eat a slice of \___"_ can hardly guess _"cake"_ if it has never seen that exact sequence[\[2\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=N,words%20to%20guess%20the%20third). Furthermore, they treat words as atomic symbols (no notion that "cat" and "feline" are related).
- **Probabilistic and Linear Models:** Examples include **Naive Bayes** for text classification (assumes words in the document are independent given the topic) or **Hidden Markov Models (HMMs)** for sequential tasks (e.g., _Part-of-Speech tagging_, speech analysis). Naive Bayes is fast and often effective on simple texts (e.g., spam vs. ham) but the assumption of independence between words is crude. HMMs model dependencies as state transitions but are limited by low-order Markov assumptions (typically state bigrams) and require manual definition of features/observations. In general, these approaches **required manual feature engineering** (e.g., counts, lists of relevant words) and struggled to capture deep semantic relationships or long-range dependencies.

**Main Problems and Attempted Solutions (pre-Transformer):**

| **Problem** | **Historical Solution** | **Inherent Limitation** |
| --- | --- | --- |
| **No Semantic Understanding** - Words as unique IDs (one-hot), no similarity between related terms. | Bag-of-Words, TF-IDF, count-based linear models. | **Sparsity & no meaning**: huge and sparse vectors; no concept of synonyms or polysemy (for the model "dog" and "puppy" have nothing in common)[\[1\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20fell%20short%3A). |
| **Limited Local Context Usage** - Word order matters, but unigram models ignore sequence. | N-grams (bigram, trigram, â€¦) that consider windows of _N-1_ preceding words. | **Rigid window and data sparsity**: capture only short dependencies; rare combinations of unseen words cannot be predicted[\[3\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the%2Devolution%2Dof%2Dlanguage%2Dmodels%2D101f10e86eba#:~:text=,handle%20long%20dependencies%20or%20variations). Limited memory to N-1 words, no long-term dependencies. |
| **Hand-crafted Features and Simplistic Assumptions** - Simple generative models (NB, HMM) or "shallow" neural networks. | HMM for sequences; linear models with features (e.g., word presence, counts). | **Limited scalability and generalization**: require defining the right features a priori. Strong assumptions (Markov, independence) lead to loss of contextual information and uncaught correlations. |

These solutions, while effective in restricted contexts, highlighted **structural gaps**. Unable to represent the _meaning_ of words or retain long phrases in memory, they paved the way for methods capable of **capturing distributed semantics and longer dependencies**. This is where neural _embeddings_ and recurrent neural network models emerged.

## 2. Classic Embeddings: word2vec, GloVe, fastText

To overcome the pure symbolic representation of words, **word embeddings** emerged in the 2010s, which are dense representations where each word is a continuous vector in a low-dimensional space. The basic idea is **distributional**: _"a word is defined by the company it keeps"_ (Firth). Models like **word2vec** (Mikolov et al., 2013) introduced unsupervised training techniques on large corpora to obtain word vectors that **capture semantic similarities**[\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20mattered%3A). For example, word2vec produces vectors such that:

- _"king" - "man" + "woman" â‰ˆ "queen"_[\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D)
- _"Paris" - "France" + "Italy" â‰ˆ "Rome"_

This indicates that the model has learned analogical relationships and semantic clusters: similar words (king, queen) have nearby vectors, and the vector difference between king and queen is similar to that between man and woman.

**How do these embeddings work?** Word2vec offers two main approaches: **CBOW** (_Continuous Bag-of-Words_) predicts a word given its surrounding context, while **Skip-gram** does the opposite (predicts the context given the target word). In both cases, the neural network trains the internal representations (embeddings) so that words appearing in similar contexts have similar vectors[\[6\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=Word2Vec%3A). Another popular model, **GloVe** (Pennington et al., 2014), starts from global word-word co-occurrence statistics and factorizes a matrix, also obtaining dense vectors. Variants like **fastText** (Bojanowski et al., 2016) introduced the use of sub-words, building character/n-gram embeddings useful for capturing morphological similarities and handling rare or out-of-vocabulary words.

**What static embeddings solve:**
- **Reduced sparsity:** it moves from huge and sparse vectors (one-hot on vocabularies of tens of thousands of words) to dense vectors of typically 50-300 dimensions. This alleviates memory problems and allows for generalization: if _"cat"_ and _"feline"_ have nearby vectors, the model can transfer knowledge from one to the other even if one of them was rare in the corpus[\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20mattered%3A).
- **Semantic similarity:** for the first time, the machine has a _notion_ of meaning. Related words (by usage context) are found close in the vector space; clusters of similar words emerge automatically (e.g., {Monday, Tuesday, â€¦} grouped, {Rome, Milan, â€¦} grouped).

What they _don't_ solve: the crucial problem with classic embeddings is that they are **static**. Each word in the dictionary corresponds to a single fixed vector, regardless of the context in which it appears. This is limiting because many words are **polysemous**: the meaning of _"bank"_ depends on the context (financial institution vs. river edge). A static embedding of _"bank"_ will end up being a sort of average of the two meanings, unable to represent them precisely. An example in Italian: _"Java"_ can refer to a programming language or an island; a single vector cannot distinctly reflect both possibilities.

To clarify, let's consider some sentences with the word **"porto"**:
\- _"The_ _port_ _of Genoa is one of the largest in the Mediterranean."_ (seaport)
\- _"After dinner, I'll have a glass of_ _Porto."_ (fortified wine)

A static embedding model will assign the word "porto" the exact same vector in both sentences, unable to grasp that in the first case it is a place noun and in the second a proper noun for wine. This **semantic ambiguity** remains unresolved. In practice, a model with static embeddings "thinks" that _"porto (harbor)"_ and _"porto (wine)"_ are a single concept, losing crucial information.

Below is a comparison between **static embeddings** and **contextual embeddings**:

| **Static Embeddings (word2vec, GloVe)** | **Contextual Embeddings (ELMo, BERT, GPT)** |
| --- | --- |
| A fixed vector for each _word-type_ in the vocabulary, independent of context. | Dynamic vector for each _occurrence_ of the word, calculated based on surrounding words. |
| Capture **global** similarities between words (e.g., _"bank"_ close to _"finance"_ and _"money"_ in general). | Capture the **specific meaning** in that sentence (e.g., _"bank"_ in _"river bank"_ will have an embedding close to _"shore"_, while in _"bank director"_ it will be close to _"financial institution"_)[\[7\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=%2A%20,%28River%20edge). |
| Are pre-trained once on a generic corpus; direct use or as initialization in NLP models. | Derived from deep models (RNN/Transformer) pre-trained on large corpora with linguistic objectives (e.g., language model). They require on-the-fly computation but provide richer understanding. |
| **Limitation:** do not handle polysemy or long-range syntactic dependencies. Context beyond the local window is ignored. | **Advantage:** incorporate arbitrarily long context: the entire utterance (or paragraph) influences each word's vector, also reflecting syntactic structure and distant information. |

**Concrete example:** Sentence 1: _"I need to go to the_ _bank_ _to deposit a check"_ vs Sentence 2: _"We sit on the_ _bank_ _of the river"_. A static model has only one vector v("bank"). A contextual model (like BERT or GPT) will produce different v1("bank") and v2("bank"): in the financial context, v1("bank") will be close to vectors for _"money"_, _"teller"_, while in the natural context, v2("bank") will be close to _"shore"_, _"water"_. This is a huge step forward: the machine "understands" which meaning is at play by observing the surrounding words[\[7\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=%2A%20,%28River%20edge).

**In summary:** word2vec, GloVe, and similar models marked a turning point by introducing distributed semantics and mitigating the sparsity problem[\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20mattered%3A). However, they left open the question of **context**: how to represent entire sentences, or words that change meaning depending on where they appear? The initial answer to this came with **recurrent models**, designed to _model sequences_.

## 3\. RNN, LSTM, GRU: The Attempt to Model Context

While embeddings produced static representations of words, **Recurrent Neural Networks (RNNs)** aimed to model entire **text sequences** as dynamic inputs. A classic RNN is a neural network that processes one element at a time (e.g., a word), **recycling** a hidden state vector that carries information from one step to the next. Formally, at time _t_, it takes as input the representation of the _t_-th token \$x_t\$ and the previous state \$h_{t-1}\$, producing a new state \$h_t = f(h_{t-1}, x_t)\$ and possibly an output (if doing language modeling, the output can be the distribution over the next word). In vector notation, a simple RNN does something like: \$h_t = \\tanh(W \\cdot \[h_{t-1}, x_t\])\$, where \$W\$ contains the weights (same at each step).

**In simple terms,** an RNN reads text _as we would, word by word_, updating a kind of "internal memory" that accumulates the information read so far. This theoretically allows for **long-range dependencies** to be taken into account, because the influence of a word could make its way through the hidden state along the entire sequence. For example, in _"The book that the professor assigned wasâ€¦"_ an RNN could, when predicting the final adjective, _remember_ the distant subject _"the book"_ instead of getting confused with _"the professor"_. This _"memory"_ capability was the big advantage over n-gram models.

However, RNNs in practice have shown serious **difficulties in capturing long-term dependencies**. The main problem is known as **vanishing gradient**: during training, gradients propagated backward through many time steps **attenuate exponentially**, almost to the point of vanishing[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to). Intuitively, every time we apply the chain rule through a time step, we multiply by the weight matrix and by the derivative of the activation function. If these values have a norm < 1 (as often with \$\\tanh\$ or \$\\sigma\$ in saturating regions), after 10, 20, 50 multiplications the product becomes ~0. This means that errors occurring at step 50 cannot effectively backpropagate to step 1: the network _forgets_ the beginning of the sequence while training the end. In simple words, basic RNNs "have short memory". This explains why they had _difficulty learning long-range dependencies_ such as complex syntactic ones or global context[\[9\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=In%20previous%20parts%20of%20the,between%20words%20that%20are%20several).

In parallel, there is the opposite case, **exploding gradient** (gradients that explode): if the weights or derivatives are >1, the norm grows exponentially and leads to enormous values, causing parameters to become NaN. Fortunately, this is easier to manage (often solved with _gradient clipping_) and to detect immediately (training visibly diverges)[\[10\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=It%20is%20easy%20to%20imagine,it%E2%80%99s%20not%20obvious%20when%20they). Vanishing gradients, on the other hand, are insidious: training seems to proceed, but in reality, the network does not learn long-term relationships because updates from the distant past are practically zero[\[11\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=parameters%2C%20we%20could%20get%20exploding,Your).

To mitigate the vanishing gradient, **Hochreiter & Schmidhuber (1997)** introduced **Long Short-Term Memory (LSTM)**, a variant of RNN with a more complex internal architecture[\[12\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=use%20Long%20Short,deal%20with%20vanishing%20gradients%20and). LSTM adds **gating**: in each cell there are input, output, and especially a _forget gate_, which regulate how much old information to keep and how much to overwrite. In practice, LSTM maintains a _cell state_ \$c_t\$ that can propagate (almost) unchanged if the model deems it appropriate, overcoming repeated multiplications by 0.&lt;span&gt;something&lt;/span&gt;. Gradients can flow through \$c_t\$ more easily, avoiding vanishing. LSTM can thus _"remember"_ information for more steps, autonomously deciding when to forget. A simpler analogue introduced later is the **GRU (Gated Recurrent Unit)**, which combines some gates and simplifies the unit: it works well in many cases with fewer parameters. These models **were explicitly designed to learn long-term dependencies** in sequences[\[13\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=perhaps%20most%20widely%20used%20models,deal%20with%20vanishing%20gradients%20and).

**Although LSTM/GRU brought significant improvements**, some structural limitations of recurrent approaches remain:

- **Very long dependencies still problematic:** In theory, an LSTM can retain information for hundreds of steps, but in practice, beyond a certain length (e.g., 100 tokens), its effectiveness decreases. Furthermore, the _context_ is entirely compressed into a fixed-size hidden vector (e.g., 256 or 512): there's a physical limit to how much distinguishable information it can carry. If the text is very long (a document), even an LSTM struggles to recall details from the beginning of the document when it reaches the end.
- **Backpropagation Through Time (BPTT) costly and fragile:** Training an RNN requires _unrolling_ the network over time and backpropagating at each step. This means that if we have sequences of 50 words, the effective network on which we compute gradients has 50 layers (all sharing weights, but computationally it's like a deep network of 50 layers). It's a heavy and **sequential** computation: the 50 steps cannot be parallelized because step _t_ depends on the state of _t-1_. This is a major bottleneck: even with GPUs, the RNN must proceed in series, unlike feed-forward models that process all elements simultaneously. This makes training slow on long sequences[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental). Furthermore, the longer the unroll, the more unstable the gradients become (vanishing/exploding). Often _truncated BPTT_ was used: backpropagation was truncated to, for example, 20 steps back, intentionally breaking overly long dependencies to stabilize and speed up[\[15\]](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#:~:text=Alternatively%2C%20we%20can%20truncate%20the,simpler%20and%20more%20stable%20models). But by doing so, the network doesn't truly learn beyond that artificial window.
- **Do not scale well in terms of data and model:** To leverage large datasets or very capacious models, more parallelization and batching would be needed, which is not trivial with RNNs. At the time (2015-2017), LSTMs with hundreds of millions of parameters were already in use (e.g., sequence-to-sequence neural translators with attention), but increasing their size led to diminishing returns: training became onerous, and other parts like the attention mechanism (introduced to compensate for the shortcomings of pure recurrence) dominated the benefits.

In summary, RNNs/LSTMs introduced the **idea of differentiable temporal memory** and enabled significant progress (e.g., _Google Neural Machine Translation 2016_ used bidirectional LSTMs + attention). But their recurrent nature posed limits on **speed** and **ability to model long contexts**. It was clear that to make a further leap, a different architecture was needed, one more suitable for parallel computing and capable of **looking at the entire context more directly**. From these needs, the **Transformer** was born.

## 4. The Transformer Revolution

In 2017, Vaswani et al. published _"Attention Is All You Need"_, introducing the **Transformer**, an architecture that completely eliminates recurrence in favor of a generalized **self-attention** mechanism[\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including). This conceptually simple change triggered a revolution: the Transformer proved capable of scaling much better, being trained in parallel, and achieving superior performance on tasks like translation in a fraction of the training time of recurrent models[\[17\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=mechanism,small%20fraction%20of%20the%20training).

Let's look at the key concepts of the Transformer:

- **Self-Attention:** is the core of everything. In a self-attention layer, each position/token in the sequence **"pays attention"** to all others, to decide which words are most relevant for interpreting that position. Technically, for each token _i_, a **query vector** \$q_i\$ is calculated, and for each potential reference _j_, a **key vector** \$k_j\$ (both obtained by projecting the initial embeddings). An affinity score \$s_{ij} = q_i \\cdot k_j\$ is calculated (higher if word _j_ is relevant for interpreting _i_). These scores activate a weighted sum of the **value** \$v_j\$ (another projection of each token) to produce the output for position _i_. In formula:

dove \$Q\$ Ã¨ la matrice delle query di tutti i token, \$K\$ delle key, \$V\$ delle value, e \$\\sqrt{d_k}\$ Ã¨ un fattore di normalizzazione (dimensione dei vettori chiave). La softmax produce pesi che evidenziano le posizioni piÃ¹ affini alla query. Il risultato Ã¨ che l'output per il token _i_ Ã¨ una combinazione delle rappresentazioni di _tutti_ gli altri token, **ponderata** in base alla rilevanza. Ad esempio, in _"She put down the glass because it was_ _fragile."_, the word _"fragile"_ as a query will assign high attention to _"glass"_ and much less to _"she"_, correctly resolving the reference of _"was fragile"_[\[18\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,%E2%80%9D).

- **Multi-Head Attention:** instead of a single Q,K,V projection, the Transformer performs several in parallel (_multi-head_). Each _head_ is like an attention channel that can focus on a different type of relationship (e.g., one head might focus on subject-verb syntactic relationships, another on pronominal coreferences, etc.). The results from different heads (operating on reduced dimensional subspaces) are concatenated and linearly combined. This enriches expressive capacity: the model can **simultaneously** consider multiple aspects of dependency for each token[\[19\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order).
- **Positional Encoding:** a "disadvantage" of pure self-attention is that it treats inputs as an unordered set â€“ if we swap two tokens, the attention scores change, but the architecture lacks intrinsic sequential positional information. For this reason, a **positional encoding** (fixed sinusoidal, or learned) is added to each initial embedding. Thus, the input vectors contain both the word's meaning and its absolute/relative position. In this way, the network can distinguish "the cat ate the mouse" from "the mouse ate the cat" based on positional offsets[\[20\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,are%20added%20to%20preserve%20order).
- **Encoder-Decoder vs. variants:** The original model is Encoder-Decoder: an **Encoder** reads the input sequence (e.g., sentence in original language) producing contextual representations; a **Decoder** generates the output sequence (e.g., translated sentence) one token at a time, "looking" at both the encoder's internal states (via _cross-attention_ between decoder and encoder) and the already generated tokens (via autoregressive self-attention with a future mask). Today, simplified configurations exist: models like **BERT** use only the encoder (bidirectional self-attention over the entire input, for comprehension tasks), while models like **GPT** use only the decoder (unidirectional, autoregressive self-attention, for text generation)[\[21\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Original%20architecture%3A).

**Why the Transformer scales (and changed everything):**

- _Full Parallelization:_ Unlike an RNN, the Transformer has no sequential dependencies _within_ each layer. Each self-attention layer can process **all tokens in parallel** using optimized matrix operations on GPUs/TensorCores. Sequential dependency remains only in the decoder's autoregression during **inference** (step-by-step generation), but during _training_, the decoder can also be trained in parallel using masks (the entire output sequence "shifted" into input). In fact, if previously I had to take 10 steps one after another to analyze 10 words, now I can take 1 step that covers 10 positions with attention. The gain in efficiency is enormous, especially on long sequences[\[22\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order)[\[23\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,of%20sequential%20computation%2C%20however%2C%20remains). Transformers trained on gigantic datasets became feasible _only thanks_ to this characteristic.
- _Effortless Long-Range:_ In a single attention layer, each token can interact _directly_ with any other, even 50 positions away, _in a single step_. In an RNN, connecting distant tokens requires many steps, and gradients might vanish along the way. The Transformer, on the other hand, calculates global dependencies **in one go**. This naturally leads to capturing long-term relationships (e.g., who is the subject of a distant verb, distant agreements, etc.) much better than RNNs. Formally, in Transformers, the number of operations required to connect any two positions is constant (1 per layer, or a few layers for higher-order relationships)[\[24\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=hidden%20representations%20in%20parallel%20for,as%20described%20in%20section%C2%A0%2016), whereas in an RNN, it is proportional to their distance in the sequence.
- _Greater Expressiveness:_ The combination of multi-head attention and position-wise feed-forward (each layer also has an FFN network that individually processes each position after attention) provides the model with enormous representational capacity. A Transformer with enough heads and layers can effectively simulate even sequential operations, but it has the freedom to learn very different structures (e.g., topological ordering of words to represent the syntactic tree, etc.). The community has discovered that Transformers spontaneously tend to learn things like _grammatical patterns_, _semantic relationships_, and even perform _forms of latent reasoning_ when scaled. In short, their generalization capacity with increasing size is superior to that of previous models.

**RNN vs Transformer - Summary Comparison:**

- _Parallelism:_ RNNs process 1 token at a time (difficult to parallelize), Transformers process N tokens in parallel (much more efficient on modern hardware)[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental).
- _Long-Term Memory:_ RNNs/LSTMs store in the hidden vector with potential attenuation, Transformers with self-attention directly look at every relevant part of the sequence (theoretically infinite context, limited only by manageable input length).
- _Complexity:_ The cost of self-attention grows \$O(N^2)\$ with length (because it compares every pair of tokens). This is more expensive than an RNN (\$O(N)\$ per step), but the trade-off is largely offset by parallel execution. On short/medium sequences, the Transformer is significantly faster with the same resources; on _very_ long sequences (e.g., thousands of tokens), it can become memory and time-intensive, but many _sparse attention_ algorithms have been introduced to mitigate this.
- _Data Requirements:_ In general, Transformers have more parameters and flexibility, so they tend to require a lot of data to express their potential. Fortunately, the era of big data has provided immense corpora; furthermore, self-supervised training (language modeling) has made it possible to use virtually the entire internet as data.
- _Modular Architecture:_ The Transformer is easier to distribute in GPU clusters (each layer is a series of standard matrix operations). Moreover, it is modular: parts can be replaced (e.g., different attention schemes, different positional encodings) without disrupting everything. This has led to an ecosystem of rapid variants and improvements.

**Conceptual (step-by-step) overview of a Transformer encoder layer:**
1.  **Input Embedding + Positional Encoding:** a positional vector is added to each token.
2.  **Self-Attention (multi-head):** \$Q,K,V\$ are calculated for each token and attention is computed. Each token "gathers" relevant information from other tokens. (In the autoregressive decoder, a mask would be applied to prevent looking ahead).
3.  **Add & Norm:** there is a residual skip connection that adds the original attention input to its output, followed by layer normalization. This aids gradient flow and stability (the model essentially learns a kind of identity + corrections).
4.  **Feed-Forward Network (FFN):** an MLP applied separately to each position (same network for all tokens). Usually 2 layers with ReLU/GELU, it expands and contracts dimension (e.g., from d_model=512 to 2048 and back). It serves to introduce non-linearity and mix the information synthesized by attention.
5.  **Add & Norm:** another residual connection summing the FFN input (which was the output of the attention sublayer) to the FFN output, followed by normalization.

A stack of N such layers forms the Encoder. In the Decoder, each layer additionally has a _cross-attention_ block after self-attention, where the _queries_ come from the previous decoder layer and the _keys/values_ from the final Encoder output, allowing the decoder to condition generation on the encoded input (useful in seq2seq tasks like translation).

The net result: **the Transformer can model long and complex dependencies efficiently, and is highly scalable**. From 2018 onwards, every record in NLP (translation, QA, summarization, etc.) has been shattered by Transformer-based models. This architecture is the foundation of practically all modern _Large Language Models_.

## 5. From Transformers to LLMs

With the Transformer as the new fundamental building block, the next step was to **scale models and datasets** to previously unthinkable levels. Two major families of pre-trained language models emerged:

-   **Autoregressive models (GPT-like):** use only the decoder half of the Transformer and are trained as traditional _language models_ - predicting the next token given all preceding context. Example: given the sequence "The capital of **France** is \[MASK\].", the model (with a mask preventing it from seeing _"Paris"_ if it were after) learns to plausibly continue the sentence (in this case with "Paris"). These models learn to generate text _one step at a time_, and thanks to attention, have a broad view of the already generated context. **GPT-2** (OpenAI, 2019) with 1.5 billion parameters was a shock due to its ability to generate coherent and long texts[\[25\]](https://arxiv.org/abs/2005.14165#:~:text=approaches.%20Specifically%2C%20we%20train%20GPT,At%20the%20same%20time%2C%20we). **GPT-3** (2020, 175 billion) then showed that by increasing parameters by an order of magnitude, new abilities emerged, such as **few-shot learning**: without further training, by providing only a few examples in the prompt, GPT-3 solves new tasks[\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on)[\[25\]](https://arxiv.org/abs/2005.14165#:~:text=approaches.%20Specifically%2C%20we%20train%20GPT,At%20the%20same%20time%2C%20we). In practice, GPT-3 demonstrated that an enormous model trained on _almost the entire internet_ in an autoregressive manner can _"learn to learn"_ from prompts, generalizing to many tasks previously solvable only with specific models. This inaugurated the era of **generative LLMs** like ChatGPT.

- **Auto-encoder models (BERT-type):** use only the encoder part of the Transformer and are trained with _bidirectional_ tasks like **Masked Language Modeling (MLM)**. In BERT (Devlin et al., 2018), 15% of the input words are randomly masked, and the model must predict them by looking _both left and right_ (thus fully leveraging bidirectional attention)[\[27\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=BERT%20is%20trained%20on%20two,clever%20tasks). Additionally, BERT was also trained with an auxiliary task of _Next Sentence Prediction_ (deciding if two sentences were in sequence in the original text), encouraging discourse understanding[\[28\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=1,it%20learn%20relationships%20between%20sentences). BERT provided powerful contextual embeddings that, with light fine-tuning, radically improved dozens of NLP tasks (from classification, to QA, to NER). These models are not designed for arbitrary generation (they do not have an autoregressive decoder), but they excel in **understanding** and in producing representations to be fed as input to simple classifiers. BERT was a watershed moment: within a few months, practically every NLP benchmark was dominated by BERT variants. Among these: _RoBERTa_ (2019, better trained and without NSP), _ALBERT_ (2019, smaller thanks to factorization and sharing), _DistilBERT_ (2019, compressed), etc.

Subsequently, attention increasingly shifted towards **large generative models**, particularly with the release of GPT-3. The community discovered that _scaling model and data size leads to substantial and sometimes qualitatively new improvements_. This was formalized in the **Scaling Laws** by Kaplan et al. (OpenAI 2020): _perplexity_ (a measure of language model quality) decreases approximately following a power law as parameters, data, and compute employed increase[\[29\]](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves). No sign of saturation appeared on the horizon: _bigger is better_. In other words, if you double parameters (and proportionally data and computation), the error decreases predictably (albeit with slightly diminishing returns). This encouraged a **"gigantism"** in models: GPT-3 with 175B was followed by even larger models like **Megatron-Turing NLG** (Microsoft-Nvidia, 530B, 2021) and various Chinese models of 200+ billion.

However, in 2022, a DeepMind study (**Chinchilla** by Hoffmann et al.) recalibrated the perspective: it was found that many LLMs were _under-trained_ relative to their size. Chinchilla (70B parameters) was trained with 4 times more data than used for GPT-3, showing superior performance to GPT-3 despite having < half the parameters, because it had utilized the compute budget better[\[30\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,Transformers%20to%2022%20Billion%20Parameters). In practice, for a fixed _compute_ budget, there is an optimal balance between model size and the number of training tokens: models that are too large trained on too little data do not reach their full potential; it is better to reduce parameters and prolong training. This led to a shift: not just increasing parameters, but ensuring _much more data_ (which is a problem, because after scraping all textual internet data, synthetic or multimodal data are needed to continue).

**Emergent Abilities:** A fascinating phenomenon observed with LLMs is the appearance of capabilities _not present in smaller models_. Wei et al. (2022) have cataloged various **emergent abilities** that appear beyond certain parameter/data thresholds[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up). For example, models under 10B parameters fail to perform simple arithmetic calculations or multi-step logical reasoning, while models like GPT-3 (175B) can perform 3-digit sums, zero-shot translations, explain jokes, write simple code, etc. These abilities _do not scale linearly_ but seem to activate suddenly when the model surpasses a "critical point" of knowledge and generalization. There is debate as to why - some say they actually emerge gradually but become measurable beyond a certain noise level; others that models begin to perform _meta-learning_. The fact remains that very large LLMs show qualitatively different behaviors: they can follow instructions (**instruction following**), plan reasoning through _chain-of-thought_, interface with tools if appropriately instructed, etc., while small models tend to return disconnected phrases or fail to grasp more complex prompts.

**"Bigger is better" - until when?** The drive to scale has led to enormous progress, but it doesn't solve _all_ problems. Beyond computational cost (training GPT-3 cost on the order of millions of \$, GPT-4 even more), there are **practical limits**: enormous models are difficult to update, deploy, and run with low latency. Furthermore, some fragilities (e.g., tendency to _hallucinate_ facts, bias) persist even when enlarging the model - simply put, large models hallucinate more convincingly ðŸ˜…. Studies like Chinchilla suggest that there's no need to endlessly increase parameters if they cannot be fed with adequate data. Today, much research focuses on _efficient scaling_: better data selection, specialized architectures for long contexts (Transformers with sparse or recurrent attention), **smaller but specialized** models (the so-called emerging **Small Language Models**). An example is **Alpaca** (Stanford, 2023): a model of only 7 billion (based on LLaMA) which, with instruction fine-tuning, manages to behave similarly to ChatGPT on many common requests - indicating that with the right specialization, a 100B monster is not always needed to deliver value.

In conclusion, from 2018 to today we have moved from Transformer ~110M parameters (BERT-base) to LLMs of hundreds of billions. This **scaling** has unlocked latent capabilities and opened new applications. But it has also highlighted problems of **alignment** (avoiding toxic outputs, ensuring veracity) and **efficiency**. This brings us to the current context, where an LLM is rarely used "alone": it is integrated into broader systems to be made reliable, updatable, and useful in real application contexts.

## 6. LLMs as _system components_, not just models

A modern AI engineer knows that using a powerful LLM **"raw" and isolated** is often not enough. Today, LLMs are typically encapsulated in broader architectures where other components mitigate their limitations and enhance their capabilities. Here are the main roles and integrations:

- **Prompt Engineering and Formatting:** The _prompt_ is the immediate interface with an LLM. Given that these models are _task-agnostic_ (they don't have a predefined goal beyond generating plausible text), the user must define the **instruction** or question clearly and often include _additional context and examples_. Designing good prompts is an art: e.g., providing the desired format in the request, or concatenating a short example conversation that shows the model how it should respond. For complex systems, prompts are sometimes built automatically by combining various pieces (instructions, retrieved knowledge, conversation memory, etc.) - this is referred to as prompt **orchestration**. Good prompt engineering can improve reliability and accuracy without touching the underlying model. In production, prompts also need to be **version-controlled**: small changes (an extra sentence, a different example) can significantly alter the output. Therefore, logging and testing on prompts are necessary to ensure that behavior remains stable as prompts vary and potentially across different model versions.
- **Retrieval-Augmented Generation (RAG):** One of the major problems with LLMs is that their _knowledge_ is static (limited to training data) and sometimes inaccurate. The RAG technique seeks to **anchor** the LLM to up-to-date and factual information by integrating a _retrieval_ (search) component into the loop. In practice, when faced with a question or task, the system first performs a search in an external knowledge base (documents, databases, web) and then constructs a prompt that includes the found content as **context** for the LLM. The LLM is then guided to formulate the response based on that context instead of its internal knowledge alone[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static). For example, if we ask: _"What is the inflation rate in Italy this year?"_, a base LLM (trained up to 2021) can only guess and risks fabricating information; with RAG, the system will search reliable sources for the latest data and provide them to the model, which will summarize them correctly. **Benefits:** RAG addresses both the problem of outdated knowledge (because it inserts updated information) and reduces hallucinations (the model is "anchored" to explicit sources)[\[33\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues). Furthermore, it allows for smaller LLMs with limited general knowledge but integrated with extensive **external memories**. Many "LLM-enabled" applications (such as chatbots on company documentation, customer support assistants, Q&A engines on specific data) use RAG: they "break down" the user prompt into queries to a search engine (often on a **vector database** with semantic embeddings of documents) and package the results into a final prompt for the LLM.

- **Tool use and API calling:** Latest generation LLMs can be seen as linguistic "brains" that reason but have no direct interaction with the external world (apart from text). To extend their capabilities, they are endowed with the ability to **call external tools**. For example, an LLM integrated into an assistant could, upon request, invoke: calculators, weather services, SQL databases, Python functions, search engines, etc. This requires an architecture that intercepts when the model "decides" to use a tool. Various approaches exist: one is the **ReAct** (Reason+Act) pattern, where the model explicitly produces a _chain-of-thought_ and action commands (e.g., Search("latest inflation news Italy")), which the system executes, then returns the result to the model, allowing it to continue generation[\[34\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=decision,and%20coordinate%20with%20other%20agents)[\[35\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=Overview%20of%20LLM,to%20the%20LLM%20prompt%20through). Another approach is to provide the LLM with a list of available functions (with relevant documentation in the prompt) and have it emit a special syntax when it wants to invoke them (see, for example, _OpenAI function calling_ or _LangChain tools_). The key idea is that the LLM acts as an intelligent **controller**: it understands which tool is needed and with what parameters, delegates the sub-task, and then incorporates the response into its flow. This enormously increases reliability on tasks where a pure LLM would be weak: precise mathematical calculations, data lookup, real-time web interactions, image manipulation, etc. In practice, the LLM transitions from a _soloist_ to an _orchestrator_ of a network of services.
- **Agents (LLM-driven agents):** An _agent_ is a more complex system that combines the above mechanisms (memory, search, tools) to pursue higher-level goals autonomously. An LLM agent typically: receives an objective (e.g., _"Book a flight from Milan to New York for next Friday under â‚¬500"_), then plans a series of actions (searches for flights, compares prices, perhaps asks the user for confirmation, finally calls the booking API). During this process, the LLM might need to **iterate**: reflect on partial results, update the plan, manage errors (e.g., no flights under â‚¬500, relax constraints). Implementing reliable agents requires care: the model must be provided with a kind of _loop_ where it can generate thoughts and actions in cycles until it reaches a termination condition. Furthermore, it is necessary to ensure that it does not take unwanted steps. Frameworks like **LangChain**, **Microsoft Semantic Kernel**, or **Hugging Face transformers agent** provide abstractions for building agents with LLMs, defining available tools and managing the prompt cycle with action results. This is a frontier field, but it promises more **autonomous and proactive** AI systems in solving complex problems by breaking them down into sub-problems (much like humans would). A core principle also highlighted in OpenAI/AWS guides is: _"an LLM alone is not enough for intelligent and reliable behavior; it needs to be embedded in a structured workflow with planning, memory, toolsâ€¦"_[\[34\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=decision,and%20coordinate%20with%20other%20agents)[\[36\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,building%20robust%2C%20extensible%2C%20and%20intelligent).

- **Quality, Security, and Observability Control:** By integrating LLMs into larger systems, we also have the opportunity to introduce **verification** and **monitoring** modules. For example, after the LLM generates a response, we could have a **validation** step (another model or a set of rules checks if the response meets certain criteria: no prohibited content, no missing info, correct format, etc.). Or implement a **self-reflection** loop: the model rereads its response and evaluates whether it seems consistent and correct (additional _chain-of-thought_ or _vote/verify_ techniques). In production, it is crucial to have **telemetry**: measuring LLM call latency, number of tokens used, parsing error rates, etc. _LLM observability_ tools are used to track not only classic metrics but also indicators such as: frequency of detected hallucinations, cost trends (tokens per request), types of user requests, and user feedback. All of this falls under **ML-Ops for LLMs** (sometimes called LLMOps). You can't really put a conversational model into the hands of millions of users without adequate logging and monitoring: "things can get weird in production" - latencies spiking, out-of-policy outputs after an update, runaway costs, etc.[\[37\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=experiences,after%20a%20minor%20prompt%20change)[\[38\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1.%20Latency%2C%20cost%2C%20and%20third,dependencies). An AI Engineer must implement **guardrails and alarms**: for example, if the usage rate of a tool (external API) called by the agent suddenly rises abnormally, there might be a prompt injection underway; or if the average response time increases, perhaps the model is "reasoning" too long on certain queries (maybe malicious users are providing input to stress it). LLM observability means being able to _see inside_ these dynamics and react[\[39\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=Engineering%20teams%20need%20more%20than,built%20to%20handle%20AI%20workloads)[\[40\]](https://www.honeycomb.io/resources/getting-started/what%20is%20llm%20observability#:~:text=1,and%20scoring%20of%20LLM%20responses).

In summary, **today an LLM in production is rarely barebones**. It is wrapped in a layer of **engineered prompts**, with optional **retrieval** for updated knowledge, the ability to call external **tools**, and **control** modules. All of this for reasons of:

- **Latency & cost:** minimizing unnecessary tokens (optimized short prompts[\[41\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,the%20entire%20history%20every%20time), caching frequent responses[\[42\]](https://www.finout.io/blog/finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,logic%20and%20cache%20invalidation%20strategy), making the model do only what's necessary and delegating the rest). For example, if I know that 90% of user queries are simple, I could use a smaller/cheaper model for those and call the large model only for the difficult 10%[\[43\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=%2A%20Model%20right,com). Or _summarize_ the conversation in the background to avoid passing the entire context every time (reducing tokens, thus cost, and latency).
- **Reliability:** using tools for tasks where the LLM is weak (calculation, real-time data), retrieval to improve factual accuracy, validation to prevent incorrect outputs. This increases confidence that the system will respond correctly and reduces risks (e.g., if the LLM hallucinates financial data, we could mitigate this by requiring it to _always_ cite a knowledge document: no document = the model must say "I don't know").
- **Maintainability:** by keeping the pieces separate, I can update the knowledge base without having to retrain the model; I can modify prompts or add new tools if requirements change. It's a more modular and engineerable approach compared to viewing the LLM as a monolith.
- **Observability:** a component-based design allows logging interactions between them. I can see which document was fetched in RAG, which tool was called and with what input, and naturally the user-model conversation. These logs help diagnose problems: if the LLM gives strange answers, perhaps the knowledge document passed was wrong or the prompt has degraded. Without this visibility, an LLM is a black box that "sometimes goes haywire" and you don't know why.

In conclusion, the _system view_ of an LLM is like **a linguistic brain embedded in a body with sensors and actuators**: the LLM provides general cognitive abilities (understanding, reasoning, language), but it needs "eyes and ears" (search modules, databases) and "hands" (APIs to act) to be truly useful and reliable in the real world.

## 7. Structural Limitations of LLMs

Despite the miracles they seem to perform, current LLMs have significant **intrinsic limitations**. Understanding them is crucial because many usage challenges arise from these limitations, which cannot be solved simply by "training a bit better" but require architectural or system interventions (as we saw above). The main ones are:

- **Hallucinations:** An LLM "hallucinates" when it invents information that does not correspond to factual reality, even if it expresses it convincingly. Example: you ask a model to list an author's works, and it inserts 2 non-existent books among the correct titles, without batting an eye. Why does this happen? Fundamentally, an LLM's training pushes it to _predict the most probable next word_, not to verify truth. If a name frequently appears associated with certain facts in the training data, the model will repeat them even if they are false in that specific case. Furthermore, when pushed out of distribution (asked about something it has never read), the model **still tends to give an answer_, because that's how it's trained (penalized if it doesn't produce output). It doesn't have a "verified" knowledge base; it only has statistical correlations. So if we ask_ "Who won the Nobel Prize in Physics in 2025?" _(future, it doesn't know), it's likely to hallucinate a plausible name, perhaps combining names of existing scientists. This tendency to_ "extrapolate when facts are not available" _is intrinsic to how language models work_**[**_\[33\]_**](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues)**_." There's no simple solution within the model (even huge and advanced models like GPT-4 hallucinate occasionally). Mitigation comes from RAG (so it must rely on sources) or from _post-checking\* techniques. But as long as an LLM generates text based on learned probabilities,** it will not have the concept of "truth" _except as other probable text. Some researchers say that truthfulness is an externality for these models: it must be introduced with rules or retrieval, because by default the model doesn't know when it doesn't know_.
- **Lack of grounding (anchoring to reality):** This is related to hallucinations but more general. An LLM reasons only on **text**. It has no direct perception of the physical world, nor real "experiences". By "grounding" we mean the connection between symbols and real things. LLMs have been called _stochastic parrots_ (Bender et al.) precisely because they generate plausible sentences without "understanding" the world as we do. For example, an LLM might say that an elephant weighs 5 kg if the prompt leads it down the wrong path, because it has never seen a real elephant. This lack of grounding also makes them **fragile to contradictions**: it might say in one sentence that _"Rome is the capital of Italy"_ and a few lines later that _"the capital of Italy is Milan"_, if prompted by differently formulated questions/options. For us, this is absurd because we know there's a single external reality; for the model, it might have memorized both phrases in different contexts and repeats them without meta-consistency. Another aspect: without temporal grounding, LLMs do not _"know"_ about the passage of time. If training stopped in 2021, in 2023 the model feels no _cognitive discomfort_ in saying "the current prime minister is X" when X is no longer in office - the model does not perceive the changed reality, it lives within the data it was fed[\[44\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=,called%20hallucination). This static nature (unless the model is updated with fine-tuning, which is expensive and not continuous) is an intrinsic problem.

- **Difficulty of Incremental Updates:** Related to the previous point, LLMs **do not have separable memory** that can be easily updated. If a country's capital changes, a classic DB-based system modifies a row in the table, and all future queries reflect the change. An LLM, however, has "knowledge" imprinted in the synaptic weights of an enormous network: to update it, one would need to retrain (very costly) or attempt neural editing/local fine-tuning techniques. But these interventions are risky: _catastrophic forgetting_ (you change one piece of information and unknowingly ruin other related ones), _overfitting_ (the model starts repeating the updated training phrase and loses fluidity), etc. In short, LLMs **are not designed as updatable knowledge bases**, but as static statistical models. Here too, RAG is a patch: you keep the knowledge in an external DB and ensure the model uses it, so you update the DB and the model "knows" things again. But the model itself remains static; if you ask it without providing updated context, it will give you the old information. In critical applications, this is a major limitation (think of medical assistants who must keep up with guidelines, or news chatbotsâ€¦ you can't retrain GPT-4 every day with the news).
- **Bias and Toxicity:** LLMs learn from training data, which includes large portions of the Internet, social media, booksâ€¦ Unfortunately, this data contains **cultural biases, stereotypes, hate speech, misinformation**, and so on. As a result, the model internalizes them and, if not filtered, can reproduce or even amplify them. There are documented cases of models generating racist or sexist output when provoked. Companies have introduced **fine-tuning techniques with Human Feedback (RLHF)** and filters to mitigate these problems (e.g., ChatGPT has a moderate "Default Personality" and rejects certain content). But this is a _post hoc_ mitigation. Intrinsically, if you ask an LLM to impersonate a certain toxic role or explore in an unfiltered manner, the original biases can emerge. Furthermore, even on non-toxic matters, LLMs can have subtle biases: e.g., a tendency to name more male inventors than female, or assume Western contexts as default in stories, etc. These reflect the datasets (more content on historical men, etc.). **Correcting bias after training** is difficult: you either filter incoming data (censorship/proactive, but risks reducing diversity) or apply penalties via RL (risks ruining knowledge). It is an active field of research. As an engineer, you must be aware of this: _never_ assume that the LLM is neutral or unbiased. It must be tested and monitored, especially for sensitive outputs (e.g., advice in medico-legal contexts).
- **Adversarial Fragility and Inconsistency:** LLMs can be surprisingly **sensitive to small changes** in the prompt. For example, reversing the order of two sentences in a question can sometimes lead to different answers. Or adding a superfluous detail can confuse the model. There are also _prompt injection_ attacks: if the user inserts something like "Forget previous instructions. [Malicious instruction]" into their input, some models might obey and violate the original constraints. This vulnerability arises from the fact that the model does not have a strong concept of _higher-level truth_ or _permissions_: every input is text to be continued, so if the input contained "The following is a malicious prompt: ..." the model might incorporate it into its internal narrative. In short, they **lack formal robustness**. Internal logical consistency is also not guaranteed: they can contradict themselves, or provide two different answers to paraphrased questions. For an engineer, this means that **safety nets** must be put in place: e.g., validate responses through separate models/verifiers, do not blindly trust on matters of correctness. A/B testing of prompts and behavioral _unit tests_ for the model are also desirable to understand how it responds to various phrasing, and to choose the least unstable ones.

**Why are these intrinsic limitations?** Ultimately, because they derive from the very nature of language models. An LLM is trained to compress the statistics of an enormous text corpus into its parameters. It has no direct perception of the world nor a causal model of the world (only linguistic correlations). Therefore, it cannot know if a statement is _true_; it can only judge if it is _probable_. Bias and toxicity are present because they are present in human data, and the model has no inherent ethical value â€“ unless we instill it through additional objectives. Inconsistency and fragility stem from not having reliable symbolic reasoning: even if advanced models show traces of logic, at their core they do not perform symbolic inference, so they can fall into contradiction or be misled.

These limitations _are not bugs fixable with a patch_, but foundational aspects. This means that when designing systems with LLMs, we must **build around them** to mitigate them. For example, for up-to-date knowledge (grounding) we use RAG; for hallucinations, we can have a search engine re-check the answer or provide sources; for bias/toxicity, we implement moderation filters and style definitions in the prompt; for inconsistency, we use agents that re-check answers or segment problems into easier sub-problems.

In the future, new architectures (e.g., deeper integration with knowledge bases, or multi-modal models that _see_ and _act_ in the environment) may reduce these limitations. But as of 2026, anyone using LLMs must do so with awareness of these _intrinsic uncertainties_, adopting an "AI safety" mindset: never let an LLM make irreversible decisions without supervision, and structure products in a way that allows intervention if (when) something goes wrong.

## 8\. Connection with your GEO & Disaster Response path

Let's now turn to the use case that interests you: applying these technologies in geospatial and disaster response (earthquakes, natural calamities, etc.) contexts. This sector combines _multi-modal_ data (texts, maps, satellite images, sensors) and requires both **precise quantitative analysis** (e.g., detecting damage from images) and **synthesis and reasoning capabilities** (e.g., drafting a situation report, making inferences about risks). LLMs can play a valuable role, but they **must be correctly integrated** with existing geospatial workflows. Let's look at some scenarios:

- **LLM + RAG for post-earthquake reports:** Imagine after a strong earthquake needing to quickly create a report summarizing damage, most affected areas, infrastructure status, and possible actions. Various sources are available: fire department reports, geolocated social media posts, satellite images with collapse analysis, GIS databases with buildings and population. An LLM alone doesn't _know_ anything about the earthquake (unless trained on past events, but not on the new one). However, we could use it as a **language generation engine** by feeding it event-specific data. With a RAG approach, the system can retrieve, for example: _"fire department text reports from the last 12h"_, _"results of automatic image analysis (X buildings collapsed in area Y)"_, _"lists of blocked roads from live maps"_. This information is inserted (perhaps already summarized) into the prompt, and the LLM is tasked with drafting a **coherent and readable report** for, for example, the authorities. The LLM excels at **connecting the dots**: it can take the list of facts and transform it into a narrative: "In the northern part of the city (XX District), approximately 30 buildings have collapsed, with the highest concentrations of damage along Alpha Street and Beta Street. Rescue teams have saved 12 people from the rubble and report at least 5 missing. The bridge over the river is impassable, temporarily isolating Gamma hamlet...". Without an LLM, a human operator would have to manually write this summary integrating many sources; with an LLM, the operator can focus on verifying and correcting, instead of writing from scratch. **Important:** as seen, here the LLM must be _grounded_ in real data: we don't want it to invent numbers of missing people! Therefore, we provide precise figures and details via RAG, and perhaps ask the model to cite sources (if the output is for internal use only). In this way, the LLM does well what it is good at - language and textual reasoning - but does not act "in the absence of information".

- **LLM + Agents for Emergency Decision Support:** In crisis situations, a decision-maker might query an AI system with complex questions, such as _"Where should we concentrate USAR teams based on reports and damage data?"_. Answering requires: understanding the question (linguistic task), having data (geospatial and textual), reasoning by combining _criteria_ (for example: USAR teams = urban search and rescue, so they are needed where collapsed buildings and potentially trapped population are greatest). A single static model would struggle. But we can build an **LLM agent** equipped with tools: one that queries a GIS database for the number of collapsed buildings per zone, one that reads the latest SOS messages received, one that consults the register of already deployed teams. The agent can make a plan like: 1) obtain a collapse density map; 2) obtain a list of reports of trapped people; 3) cross-reference by zone; 4) propose priorities. Steps 1) and 2) are performed via tools (for example, by calling a geospatial API that returns data, or by executing a query on an emergency knowledge graph). Then the LLM itself can generate a response like: _"The zones most in need of USAR appear to be A and B. In district A (20 collapsed buildings, ~50 people reported under rubble) there is currently only one operational team; I suggest sending at least two more. In district B (15 collapses, 30 people reported) the situation is similar. Zones C and D have fewer collapses or already sufficient coverage."_. This is **decision support**: the LLM does not make the decision, but provides a reasoned and quickly readable analysis, integrating disparate data (GIS + reports + resource status). This allows the person in charge to confirm and act much faster. Again, the LLM here acts as an **intelligent collector**: it manipulates data with reasoning and presents it effectively.
- **LLM as a "Cognitive Interface" over Remote Sensing (RS) Models:** In satellite image analysis or remote sensing, we often obtain technical results: classification maps, confusion matrices, damage percentages per cell, etc. An LLM can help translate these raw outputs into **humanly usable insights**. For example, a computer vision model processes post-disaster images and produces shapefiles with polygons of flooded areas as output, along with a severity indicator per area. An LLM could take these results (converted into structured text) and generate a **briefing**: _"Satellite analyses indicate extensive flooding along the Delta River: approximately 45 kmÂ² of territory are flooded. The municipalities of X and Y are particularly affected, where water has covered ~30% and ~45% of the urban area, respectively. The industrial area of Y is entirely submerged with possible release of substances into the water. Key affected infrastructures include SP123 and railway Z, both interrupted."_. Observe how many deductions and aggregations are included: the LLM can describe the total area (by summing polygons), convert that information into an impact statement, identify municipalities within the polygons (by cross-referencing coordinates with names via a GIS tool in the backend), mention affected infrastructures (if it has vector data on roads and railways, it can cross-reference them). In short, we use it as an _intelligent report generator_ that sits on top of numerical models. **What should the LLM not do?** It should not perform the segmentation on the image _itself_! To recognize flooded pixels, there is a specialized vision model that works on rasters and perhaps uses convolutional networks or other architectures. The LLM does not have direct visual perception (unless a multimodal model is used, but currently for precision tasks, dedicated models are better). So the rule: leave the "pixel-wise" quantitative work to RS models (they are trained for high accuracy on that), and use the LLM to **connect those results with knowledge and present them**. An LLM can, for example, explain why a certain flooding pattern is dangerous ("this area was already prone to landslides; the flood makes it unstable"), which a pure RS model does not do.

**Multimodality (text â†” images â†” geospatial):** It's worth noting that the current trend is towards models capable of ingesting multiple forms of data. For example, _GPT-4_ has visual capabilities: you can give it an image and it produces text about it. There are models like **CLIP** and **BLIP** that connect vision and language. For geospatial data, works are emerging that integrate georeferenced graphs with LLMs (e.g., GraphRAG in geospatial). So in the not-too-distant future, you might have a _multimodal LLM_ that directly takes both maps and texts. Already today, services like **Google's PaLM-E** aim to combine vision, language, and robotics. In a disaster context, imagine giving the model both the damage map and localized tweets: a multimodal model could directly combine them and explain the situation to you. We are at the beginning of this - for now, the modular approach (visual model + LLM) is more practical. But keep an eye on research, because tools like **Imagen (Google)** or **Kosmos-1 (Microsoft)** are building bridges between visual data and LLMs.

**What an LLM _should not do_ in geospatial/DR:** as already mentioned, do not entrust an LLM with the **technical precision** that requires dedicated algorithms. If you need to get the latitude/longitude of an address, use a geocoding API, don't ask the model to invent it! If you need to calculate the magnitude of an earthquake from seismographic data, physical formulas are needed, not the "opinion" of an LLM. LLMs have no guarantees of numerical accuracy or scientific rigor. Therefore, the _core_ parts of analysis (detecting damage, calculating extensions, exact counting) should be done with deterministic methods or specialized ML models. LLMs, however, excel in: **synthesis, high-level correlation, communication, Q&A**. Furthermore, they are excellent at filling general knowledge gaps: if in a report you also need to explain concepts (e.g., what is a seismic fault, or what are the effects of soil liquefaction) the LLM can generate those paragraphs by drawing on its trained knowledge.

**Integration with existing GEO pipelines:** You could imagine your system as: data ingestion pipeline (satellites, sensors, open data) â†’ analytical models (CV for images, GIS computations, etc.) â†’ **LLM layer** for user output. During the design phase, clearly define the API between the analytical layer and the LLM. It's often advisable to structure data in a textual format understandable to the model (e.g., bullet points or JSON), also including explanations. For example, instead of just throwing raw numbers, you could say: "Road X: interrupted (collapsed bridge)". This way, the LLM already knows that Road X is interrupted and why, and can easily include it in its narrative, perhaps reasoning "collapsed bridge â†’ isolated that northern municipality". If you only gave "road X status: 0", it would have to infer the meaning of 0, which is much more difficult. Therefore, some **data preprocessing for LLMs** is useful: converting technical results into simple phrases or statements.

**Cross-validation:** in safety-critical domains (disasters are one), an LLM should not be the sole voice. You can use _ensemble_ approaches: have the LLM generate the report, then have another LLM reread it, asking it to highlight contradictions or possible errors, and finally have a human-in-the-loop (an operator) verify key points. Or generate two versions (perhaps with different temperatures or different prompts) and compare them. In short, use the LLM as an assistant, not as an oracle.

In conclusion on GEO & Disaster Response: an LLM can act as an **intelligent collector and communicator** over geospatial data. Think of it as a **virtual analyst** who knows a bit of everything (thanks to general training) and can be instructed to use your specific data to produce analyses and reports. It frees you from having to manually interpret every map and every table, offering you an integrated picture. But as an engineer, you set up the ecosystem: specialized models to extract info from raw data, well-organized databases, and then the LLM appropriately harnessed (targeted prompts, RAG, tools) to stitch everything together. This way you leverage the best of both worlds - quantitative accuracy of geo models and _linguistic intelligence_ of LLMs.

## 9\. Conclusions and evolutionary conceptual map

To recap what has been discussed, we present a **conceptual map** of the NLP â†’ LLM evolution, and some guidelines for an AI Engineer on what is fundamental to master and what can be (relatively) overlooked:

### 9.1 Summary Conceptual Map (NLP â†’ LLM)

- **Statistical Era (90s - early 00s):** Approaches based on simple probability models and manual features. Examples: _n-grams_ for language modeling[\[2\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=N,words%20to%20guess%20the%20third), Markov models (HMM) for sequential tagging, _bag-of-words + TF-IDF_ for IR and classification. **Limitations:** no understanding of meaning, context limited to a few words, require many observations to cover rare cases[\[3\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,handle%20long%20dependencies%20or%20variations). The engineer had to design features (keyword lists, regex patterns, etc.). Obsolete today except for quick baselines.
- **Early Neural Networks for NLP (00s - early 10s):** Introduction of **feed-forward** networks for language models (Bengio et al. 2003) and especially **word embeddings** (Mikolov 2013)[\[45\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Researchers%3A%20Tomas%20Mikolov%20et%20al,%28GloVe%2C%202014). Here the focus is on representing words in dense vectors that capture semantic similarity (the famous _king-man+woman=queen_[\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D)). Neural models began to outperform count-based models, partly solving sparsity. **However**, these models still didn't model entire sentences well: embeddings were static, and feed-forward networks had limited window context (e.g., 5 words). The _"pre-training + fine-tuning"_ paradigm emerged in a primitive version: general embeddings were pre-trained, then used in models for specific tasks.
- **Sequence modeling with RNNs (2014-2016):** The need for broader context led to the massive adoption of **RNNs, LSTMs, and GRUs**. _Sequence-to-sequence_ with attention (Bahdanau et al. 2014) revolutionized machine translation: an LSTM encoder encoded the source sentence, an LSTM decoder generated the target sentence, with **attention** acting as a flexible bridge (at the time, attention was a specific mechanism, not the entire architecture). LSTMs dominated many applications â€“ e.g., speech synthesis, image captioning (image captioning combined CNN+LSTM). **Problems solved:** short-term memory, word order, variable lengths. **Remaining problems:** difficulty with very long dependencies (LSTMs improve but don't work miracles with entire paragraphs)[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to), non-parallel and slow training[\[46\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,The%20fundamental), many tricks needed to prevent divergence (clipping, orthogonal initializations, etc.). At this stage, models began to have tens of millions of parameters, and GPU training became standard in NLP.
- **The Transformer (2017):** _Game changer_. It introduced multi-head self-attention and abandoned recursion[\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including). Result: models faster to train, which scale to enormous data and capture global context better than LSTMs. In a few months, it replaced LSTMs in translation, then in practically every sequential task. Libraries like Tensor2Tensor and subsequently Hugging Face accelerated its spread. Transformer encoder-only models (BERT, 2018) and decoder-only models (GPT, 2018) inaugurated the era of **pre-trained language models**.
- **Large-scale Pre-training (2018-2019):** With BERT and GPT, the potential of training models on massive amounts of generic text and then reusing them became apparent. BERT achieved SOTA on 11 NLP tasks with minimal fine-tuning â€“ an "ImageNet moment" for NLP. GPT-2 demonstrated fluid and coherent text generation like never before (to the point that OpenAI was initially reluctant to release it entirely, fearing misuse). The open community easily replicated BERT (see RoBERTa), while GPT-2 remained somewhat exclusive due to training costs. **ULMFiT** (Howard & Ruder) also appeared, demonstrating universal fine-tuning. The base models BERT-base (110M param) and GPT-2 (1.5B param) already seemed largeâ€¦ but it was just the beginning.

- **Large Language Models Emerge (2020-2021):** OpenAI releases **GPT-3 (175 billion)**[\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on), demonstrating that _scaling up_ by an order of magnitude yields impressive zero-shot/few-shot capabilities. The concept of **prompting** spreads as an alternative to fine-tuning: GPT-3 solves tasks described in the prompt without changing weights. Other big labs follow suit: Google Brain with **PaLM (540B)**, NVIDIA/Microsoft with Megatron-Turing (530B). Sparse architectures are also explored (Switch Transformers with gating to reach trillions of effective parameters, Mixture-of-Experts), but dense models like GPT-3 dominate. **Emergent abilities** become a research topic â€“ large models show language understanding, basic arithmetic and logical reasoning, and programming, which smaller models did not exhibit[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up). In parallel, DeepMind publishes **Gopher (280B)** and an analysis on the risks/biases of LLMs. Empirically, "more data, more parameters" continues to improve performance on almost every metric, albeit with enormous costs.
- **Alignment and Utility (2022-2023):** A challenge emerges to make these models "useful" and not just large parrot models. OpenAI develops **InstructGPT** then ChatGPT: it applies RLHF (reinforcement learning with human feedback) to align the output with user intents (less rambling, more following instructions). ChatGPT (based on GPT-3.5) explodes in popularity, demonstrating the effectiveness of _aligned_ LLMs in a conversational interface. Meanwhile, research on **scaling laws** leads to Chinchilla (70B) which beats Gopher (280B) using 4x data, and **UL2** (Google) explores alternative training architectures (mix of seq2seq objectives). In 2023, OpenAI releases **GPT-4**, a multimodal model (accepts images) with even more advanced reasoning capabilities (aims for a level almost like a _"small AGI"_ in certain benchmarks). Notable open-source models are also released: **T5** (Google 2019, text-to-text), **BLOOM** (2022, open multi-language model 176B), **OPT** (Meta 175B), and especially **LLaMA** (Meta 2023) which, although not fully open-source, leaks and is fine-tuned into a thousand variants (Alpaca, Vicuna, etc.), somewhat democratizing quality LLMs. In the meantime, LLMs are integrated into products and industrial workflows, with a strong focus on **incorporating customer knowledge** (hence the boom in RAG) and **tools** (ChatGPT plugins, LangChain for developers). **Multimodality** emerges: GPT-4 Vision, Google Gemini on the horizon, ideas of agents with perception (see Google's PaLM-E which connects robotics). The trend is to have LLMs as _general brains_ with connected eyes, ears, and hands.
- **Towards Small & Specialized Models (2024+):** In reaction to the enormous cost of giant LLMs, there is a research trend on smaller and more efficient models. Techniques like **distillation** (compression of a large model into a smaller one), **quantization** (reduction of numerical precision of weights), and alternative architectures (mixture of experts, RETRO with retrieval in training, etc.) promise performance similar to GPT-3/4 with a smaller footprint. The term **Small Language Models** is used when a model is trained on a specific domain with much fewer dimensions but maintaining quality in that domain. For example, you could train a 6B param model entirely on medical literature: you would get a _MedLM_ that, with 6B params, does things that GPT-3 175B would struggle with because it's a generalist. There's no free lunch â€“ small models will hardly have the robustness of enormous ones on "wild" inputs. But for practical implementation, sometimes _medium-sized LLM pieces_ integrated with retrieval are more than enough, at a fraction of the cost (both computational and hallucination risk). In 2026, it is likely that we will see hybrid architectures: generative neural part + symbolic or retrieval modules, rather than just pushing for 1 trillion parameters. Above all, the emphasis is on **control and interpretability**: how to make LLMs explain their answers (e.g., source citations), how to have _"constitutional AI"_ (models that follow a constitution of ethical principles in generation).

### 9.2 What an AI Engineer Must Know, What They Can Ignore

**Must-know:**

- The **architectural foundations**: how a Transformer works (self-attention, multi-head, etc.)[\[47\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Core%20ideas%3A), differences between RNN and Transformer[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental), what encoders vs decoders are. Not necessarily being able to derive equations by hand, but understanding the _data flow_ and why it's efficient. This helps in debugging tensor dimensions, understanding shape errors, and reasoning about limitations (e.g., why a 2k token context model cannot accept 10k tokens without modifications).
- The concept of **pre-training vs fine-tuning vs prompting**: knowing that models like GPT/BERT are pre-trained on enormous corpora with a general objective, then can be _fine-tuned_ (by updating weights) on specific tasks or _prompted_ with appropriate instructions. This influences design choices: if you have little specific data, perhaps prompt engineering is more suitable than fine-tuning weights, etc.
- Knowing the **main model families** and what they offer: BERT (encoder, bidirectional, excellent for comprehension), GPT (decoder, generative), T5 (encoder-decoder "unified" text-to-text), and some open models like GPT-neo/LLama, Bloom. Not so much the implementation details, but the conceptual differences: an AI eng must know how to choose "for this task I need a generative model (e.g., completion/assistant) vs a classification model (e.g., information extraction)".
- **Limitations and failure modes** of LLMs: _hallucination_, _bias_, _context length limit_, etc., and their mitigations[\[33\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues)[\[38\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1.%20Latency%2C%20cost%2C%20and%20third,dependencies). This is essential for designing robust systems: if you know an LLM can invent, set up controls; if you know it cannot process input > 4096 tokens, you must consider chunking or special long-form models.
- The principles of **Retrieval-Augmented Generation**: even if you won't implement the vectorization algorithm yourself, you need to understand how a vector database can integrate, how to embed queries and documents, and what cosine similarity is. And especially _when RAG is needed_: knowledge-intensive situations with up-to-date information[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static). Practical examples: chatbot on documentation (the model alone doesn't know precise answers, RAG is needed). Also know the limitations: if documents are long, the model might not use them well if there are too many; if query embedding fails, the model responds empty. Therefore, test end-to-end pipelines.
- **Tool/Agents pattern**: Familiarity with at least one library (LangChain, etc.) for orchestrating LLMs with tools. It's not necessary to know the detailed algorithm of an agent like ReAct, but _yes_ to know how the LLM can execute iterative steps and call functions[\[48\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,and%20coordinate%20with%20other%20agents)[\[35\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=Overview%20of%20LLM,to%20the%20LLM%20prompt%20through). It's also very useful to be able to read an agent's "chain-of-thought" logs for debugging.
- **Practical prompt engineering:** Knowing how to formulate prompts for various scenarios (e.g., role prompt, few-shot with examples, delimiting context with special tokens, etc.). Knowing tricks like: _"Think step by step"_ to encourage reasoning, or providing structured instructions ("Respond with JSON containing fields X, Y, Z"). This has almost become a programming skill. An AI eng must iterate on the prompt to improve output, and be careful about _injections_ from user input. In short, consider the prompt as part of the application code.
- **MLOps for LLMs:** Even if not in detail, basic concepts: logging, monitoring cost and latency[\[49\]](https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025#:~:text=Each%20integration%20serves%20production%20AI,evaluation%20metrics%2C%20production%20alerting%2C), regression testing (if I change the model or prompt, I have test cases to compare responses), version management (model v1 vs v2, how to roll out). And knowing how to use tools like OpenAI Evaluation framework or prompt testing suites.

- **Ethics and Policy:** Do not ignore aspects of AI Ethics. An AI Engineer must at least know the model usage guidelines (avoid discriminatory outputs, protect sensitive user data, etc.). And know how to implement moderation filters (e.g., use moderation APIs on outputs, or dedicated models that classify generated text). This is both for social responsibility and to avoid legal or reputational issues.

**What can be (relatively) ignored / delegated:**

- **Mathematical details of backpropagation and derivation:** How BPTT works exactly, formally proving the vanishing gradient, or manually deriving the attention equation with pen and paper - as an engineer, you can consider this background. Qualitative intuition is sufficient in most cases. In practice, libraries and papers already implement everything; you need to understand the effect (e.g., _"the gradient vanishes if the sequence is too long"_[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to), _"attention weighs relevant terms"_), but you don't need to be able to prove why from scratch.
- **Outdated models:** Don't spend too much time mastering Naive Bayes, HMMs, Markov n-grams, or even classic algorithms like CRFs, SVMs applied to text - unless you're working on a very low-computational-resource case where a simple model might suffice. Today, almost always a small fine-tuned Transformer or an LLM via API outperforms them, so knowing HMM theory is more historical than practical knowledge. (It's still useful to be aware of them for general knowledge and to understand terms found in older systems, but you'll rarely implement them in new projects).
- **Implementing models from scratch:** It's not efficient to recreate a Transformer layer by layer if proven libraries exist (Hugging Face Transformers, PyTorch Lightning, etc.). Unless doing architectural research, an engineer can use pre-trained models and APIs. So you can "ignore" low-level code (like manually writing multi-head attention, with all the dimensioning). Better to focus on _how to integrate_ the model into the broader pipeline.
- **All models released on the market:** There are dozens of variants (ALBERT, XLNet, ELECTRA, DeBERTa, GPT-NeoX, etc.). You don't need to know them all in detail. It's useful to know macro-categories and perhaps 1-2 names per category as an example. When you need a specific one, you can research it at that moment. Focus on general ideas: e.g., "ELECTRA pre-trains as a discriminator rather than a masked generator, for efficiency" - the concept is fine, but you don't need to remember every detail. In practice today, you will either use mainstream models (BERT, GPT-3, etc.) or models trained ad hoc on your data (in which case you follow a known architecture). Minor differences between architectures matter little for practical use.
- **In-depth linguistic theory:** Knowing what POS tagging is, what a syntactic dependency is, is useful. But you don't need a PhD in computational linguistics. Many classic linguistic concepts (formal grammars, etc.) have been implicitly incorporated into neural models. Once upon a time, grammar and semantics had to be hand-coded; today, the model learns them. So, for example, you might never have to manually implement a syntactic parser if you use LLMs for text analysis. Instead, focus on how to evaluate outputs (BLEU, Rouge metrics, etc.) and on practical notions (tokenization, etc.).
- **Pushing the SOTA to the limit:** If your goal is to build functional systems, it's not necessary to achieve absolute top accuracy on a benchmark with elaborate fine-tuning. Often, an out-of-the-box pre-trained model + a bit of prompt engineering already yields excellent results for products. Sometimes "good enough" wins over "perfect but complicated". So, you can ignore micro-optimizations like "should I use Adafactor with linear decay vs AdamW with cosine schedule?" unless you are training models yourself. If you use APIs like OpenAI, these choices are abstracted away. (Of course, if you _train_ models, then yes, you need to manage hyperparameters - but that's more the job of an ML researcher than a system implementer).

In summary, an AI Engineer must be **T-shaped**: broad knowledge of the landscape (from bag-of-words to LLMs, to understand existing solutions) but depth in the technologies essential today (Transformer and its evolutions, and how to put them into production). They can safely do without historical details and rigorous theoretical demonstrations, as long as they understand the _why_ and _when_ of each technique.

### 9.3 Essential Sources to Truly Study

We conclude with some recommended sources (papers and blogs) that I consider fundamental for consolidating the discussed knowledge and staying up-to-date:

- **Tomas Mikolov et al. (2013), "Efficient Estimation of Word Representations in Vector Space"**[\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D)[\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20mattered%3A) - _(Paper)_ Introduces **word2vec**. A milestone explaining the concept of distributed embeddings and two algorithms (CBOW, Skip-gram). Relevant because it lays the foundation for the idea of dense representations that is still at the heart of language models today.
- **Sepp Hochreiter & JÃ¼rgen Schmidhuber (1997), "Long Short-Term Memory"**[\[50\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=functions,deal%20with%20vanishing%20gradients%20and) - _(Paper)_ Proposes the **LSTM** architecture to overcome the vanishing gradient problem in RNNs. It's a technical paper, but reading at least the introduction and understanding the components (input/forget/output gate) helps to grasp how the concept of _memory over time_ originated. Useful for historical perspective and because LSTMs are still used in some specific contexts.
- **Vaswani et al. (2017), "Attention Is All You Need"**[\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including)[\[17\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=mechanism,small%20fraction%20of%20the%20training) - _(Paper)_ An absolute must-read. Introduces the **Transformer**. Explains self-attention, multi-head, positional encoding, and shows results on translation. It is the foundation of everything that came after. After reading it, the concept of attention will be much clearer, and you will appreciate the reason for the breakthrough. (It also contains some implementation details like _scaled dot-product_, useful to know).
- **Brown et al. (2020), "Language Models are Few-Shot Learners" (GPT-3 paper)**[\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on)[\[25\]](https://arxiv.org/abs/2005.14165#:~:text=approaches.%20Specifically%2C%20we%20train%20GPT,At%20the%20same%20time%2C%20we) - _(Paper)_ The abstract and some key sections show what happens when a model is scaled to 175 billion parameters. It introduced the phenomenon of **few-shot learning** within the prompt. Reading this paper helps to understand the emergent capabilities of LLMs and also their limitations (they have an honest section on where GPT-3 fails)[\[51\]](https://arxiv.org/abs/2005.14165#:~:text=demonstrations%20specified%20purely%20via%20text,evaluators%20have%20difficulty%20distinguishing%20from). It's long, but I recommend focusing on the descriptive parts and example tables.
- **Jared Kaplan et al. (2020), "Scaling Laws for Neural Language Models"**[\[29\]](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves) - _(Paper)_ An OpenAI work that quantified how increasing model/data/compute predictably decreases error. It's useful for gaining the intuition that bigger = better (up to certain limits) and concepts like _compute-optimal_. Even if you don't follow all the formulas, the message is clear: there's an efficient way to choose model size vs. data. This informed choices like those of Chinchilla.
- **Hoffmann et al. (2022), "Training Compute-Optimal Large Language Models" (Chinchilla)**[\[30\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,Transformers%20to%2022%20Billion%20Parameters) - _(Paper)_ Important because it rectifies the scaling laws by considering the parameter vs. token trade-off. It shows that a 70B model trained with 4x tokens outperforms an under-trained 175B model. A must-read to understand that it's not enough to accumulate parameters; you also need to _feed them_ sufficiently. There are very instructive graphs on perplexity as quantities vary. The concept of "compute-optimal" LLM.
- **Wei et al. (2022), "Emergent Abilities of Large Language Models"**[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up) - _(Paper)_ An essay (also in the form of a Google Research blog post) that catalogs various _emergent skills_ appearing beyond a certain scale, e.g., compositionality, multimodality, etc. It's useful for understanding the differences between medium and giant models. Also useful at a conceptual level: it discusses what an emergent ability is and what hypotheses explain the phenomenon. For an AI eng, it helps to motivate _why_ large models are valuable (they do qualitatively different things, not just slightly better versions of the usual).

- **Lewis et al. (2020), "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"**[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static) - _(Paper)_ Proposes the **RAG** framework. It is the theoretical basis behind many modern QA systems. It shows how to combine a document index with a neural generator. By reading it, you will understand the RAG architecture (encoders for queries and documents, top-k selection, conditional generation) and see results on QA where the model with retrieval significantly outperforms one without. Fundamental for anyone wanting to implement or improve _LLM + knowledge base_ systems.
- **Akanksha Sinha (2025), "From N-grams to Transformers: Tracing the Evolution of Language Models"**[\[52\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=5,Broader%2C%20Multimodal)[\[53\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20scale%20matters%3A) - _(Blog)_ A Medium article that summarizes a journey similar to ours, also including historical context. It is useful because it is written in a discursive style, covering N-grams, Word2Vec, RNNs, Transformers, Scaling, in ~6 minutes of reading. It can serve as a quick review or to explain the evolution to non-specialist colleagues (it also has images and analogies). A light read that nevertheless reinforces chronological understanding.
- **Jay Alammar (2018), "The Illustrated Transformer"**[\[54\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,language%20models%20and%20their%20implications) - _(Blog/Tutorial)_ A visual and step-by-step explanation of the Transformer. Alammar is known for his blogs with excellent diagrams and illustrations (in this post, he represents query-key-value with colored diagrams, etc.). It is highly recommended if you want to _intuit_ what happens inside attention without getting lost in algebra. After reading it, concepts like multi-head and residual connection become much more concrete. It is also a perfect resource to recommend to students or colleagues in training.

_(These sources cover theory and practice. Of course, the literature is vast; other honorable mentions:_ _Chris Olah's blog_ _- e.g., "Understanding LSTM Networks" - for intuitive explanations of LSTMs; the OpenAI blog "Better Language Models and Their Implications" (2019) which discusses GPT-2 and risks; the_ _GPT-4 technical report (2023)_ _to understand the capabilities and limitations of the most advanced model; and the_ _Papers with Code_ _website to stay updated on new SOTA. But the 10 above offer an excellent foundation.)_

**Conclusion:** The revolution from classic NLP models to LLMs has combined solid theoretical foundations (neural networks, attention, probability) with a large-scale engineering vision (immense datasets, GPU infrastructures, system integration). As an AI Engineer, understanding this evolution allows you to make informed choices about _which model to use, how to train or integrate it, what limitations to consider,_ and ultimately how to build **effective and reliable AI systems**. We are only at the beginning of this new era: models will continue to evolve (perhaps becoming more multimodal, more efficient, more specialized), but the principles you have learned here will help you navigate the rapidly changing landscape of linguistic AI. Good luck on your GEO & Disaster Response journey - with this knowledge, you will be able to leverage LLMs to truly make a difference in critical and socially impactful applications!

[\[1\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20fell%20short%3A) [\[2\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=N,words%20to%20guess%20the%20third) [\[3\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the%20evolution%20of%20language%20models-101f10e86eba#:~:text=,handle%20long%20dependencies%20or%20variations) [\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=Why%20it%20mattered%3A) [\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D) [\[18\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=,%E2%80%9D) [\[19\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order) [\[20\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=,are%20added%20to%20preserve%20order) [\[21\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=Original%20architecture%3A) [\[22\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order) [\[30\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=,Transformers%20to%2022%20Billion%20Parameters) [\[45\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=Researchers%3A%20Tomas%20Mikolov%20et%20al,%28GloVe%2C%202014) [\[47\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=Core%20ideas%3A) [\[52\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=5,Broader%2C%20Multimodal) [\[53\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=Why%20scale%20matters%3A) [\[54\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing%20the%20evolution%20of%20language%20models-101f10e86eba#:~:text=,language%20models%20and%20their%20implications) From N-Grams to Transformers: Tracing the Evolution of Language Models | by Akanksha Sinha | Medium

<https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba>

[\[6\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=Word2Vec%3A) [\[7\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=%2A%20,%28River%20edge) [\[27\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=BERT%20is%20trained%20on%20two,clever%20tasks) [\[28\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=1,it%20learn%20relationships%20between%20sentences) Beyond "One-Word, One-Meaning": Contextual Embeddings - DEV Community

<https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16>

[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to) [\[9\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=In%20previous%20parts%20of%20the,between%20words%20that%20are%20several) [\[10\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=It%20is%20easy%20to%20imagine,it%E2%80%99s%20not%20obvious%20when%20they) [\[11\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=parameters%2C%20we%20could%20get%20exploding,Your) [\[12\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=use%20Long%20Short,deal%20with%20vanishing%20gradients%20and) [\[13\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=perhaps%20most%20widely%20used%20models,deal%20with%20vanishing%20gradients%20and) [\[50\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=functions,deal%20with%20vanishing%20gradients%20and) Recurrent Neural Networks Tutorial, Part 3 - Backpropagation Through Time and Vanishing Gradients Â· Denny's Blog

[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental) [\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including) [\[17\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=mechanism,small%20fraction%20of%20the%20training) [\[23\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,of%20sequential%20computation%2C%20however%2C%20remains) [\[24\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=hidden%20representations%20in%20parallel%20for,as%20described%20in%20section%C2%A0%2016) [\[46\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,The%20fundamental) \[1706.03762\] Attention Is All You Need

<https://ar5iv.labs.arxiv.org/html/1706.03762>

[\[15\]](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#:~:text=Alternatively%2C%20we%20can%20truncate%20the,simpler%20and%20more%20stable%20models) 9.7. Backpropagation Through Time - Dive into Deep Learning 1.0.3 documentation

<https://d2l.ai/chapter_recurrent-neural-networks/bptt.html>

[\[25\]](https://arxiv.org/abs/2005.14165#:~:text=approaches.%20Specifically%2C%20we%20train%20GPT,At%20the%20same%20time%2C%20we) [\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on) [\[51\]](https://arxiv.org/abs/2005.14165#:~:text=demonstrations%20specified%20purely%20via%20text,evaluators%20have%20difficulty%20distinguishing%20from) \[2005.14165\] Language Models are Few-Shot Learners

<https://arxiv.org/abs/2005.14165>

[\[29\]](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves) \[2001.08361\] Scaling Laws for Neural Language Models

<https://arxiv.org/abs/2001.08361>

[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up) Emergent Abilities in Large Language Models: An Explainer

<https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/>

[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static) [\[33\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues) [\[44\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=,called%20hallucination) Retrieval augmented generation: Keeping LLMs relevant and current - Stack Overflow

<https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/>

[\[34\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=decision,and%20coordinate%20with%20other%20agents) [\[35\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=Overview%20of%20LLM,to%20the%20LLM%20prompt%20through) [\[36\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,building%20robust%2C%20extensible%2C%20and%20intelligent) [\[48\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,and%20coordinate%20with%20other%20agents) AWS Prescriptive Guidance - Agentic AI patterns and workflows on AWS

<https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf>

[\[37\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=experiences,after%20a%20minor%20prompt%20change) [\[38\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1.%20Latency%2C%20cost%2C%20and%20third,dependencies) [\[39\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=Engineering%20teams%20need%20more%20than,built%20to%20handle%20AI%20workloads) [\[40\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1,and%20scoring%20of%20LLM%20responses) What Is LLM Observability and Monitoring? | Honeycomb

<https://www.honeycomb.io/resources/getting-started/what-is-llm-observability>

[\[41\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,the%20entire%20history%20every%20time) [\[42\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,logic%20and%20cache%20invalidation%20strategy) [\[43\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=%2A%20Model%20right,com) FinOps in the Age of AI: A CPO's Guide to LLM Workflows, RAG, AI Agents, and Agentic Systems

<https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems>

[\[49\]](https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025#:~:text=Each%20integration%20serves%20production%20AI,evaluation%20metrics%2C%20production%20alerting%2C) Top 10 LLM observability tools: Complete guide for 2025 - Braintrust

<https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025>