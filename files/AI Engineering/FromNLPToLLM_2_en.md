# From classic NLP models to LLMs (Part 2)

<details class="post-warning">
<summary><strong>Article under review</strong> (click to open)</summary>

This article is still a work in progress and under editorial review. Some paragraphs may be incomplete or change significantly in the coming weeks.

</details>

Welcome to the third installment of the journey to becoming a GeoAI engineer. In this article, we continue talking about history and will focus on how we came to talk about GPT, Gemini & co. and where we started. Not only that, don't worry!

Enjoy the read!

## Introduction

If in part 1 we saw the foundations (statistical models, classic embeddings, RNN/LSTM/GRU), and which problems remained even with the first neural networks, here we analyze what allowed the real leap: we enter the **Transformer** era and see why it completely changed the world of text understanding.

From this point on, we are no longer just talking about "better models", but about **scalability**, **large-scale pre-training**, and the birth of modern **LLMs**. In practice: how we moved from slow recurrent networks with limited memory to systems capable of handling broad contexts, generalizing human language better, and becoming the basis for tools like ChatGPT, Gemini, and the like.

The goal of this part is twofold: to understand the key technical concepts (self-attention, scaling, differences between model families) and, above all, to read LLMs with a technical mindset, i.e., as **system components** to be integrated with retrieval, tools, guardrails, and observability.

We then close with the more practical part: real structural limits (hallucinations, grounding, costs, fragility) and the connection to the context of this path, namely the Geoinformatics field. Enough talk, let's begin!

## The Transformer revolution

In 2017, Vaswani et al. published [_"Attention Is All You Need"_](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental), introducing the **Transformer**, an architecture that completely eliminates recurrence in favor of a generalized **self-attention** mechanism[\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including). This conceptually simple change triggered a revolution: the Transformer proved it could scale much better, be trained in parallel, and achieve superior performance on tasks like translation in a fraction of the training time of recurrent models[\[17\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=mechanism,small%20fraction%20of%20the%20training).

Let's look at the key concepts of the Transformer:

- **Self-Attention:** it is the heart of everything. In a self-attention layer, each position/token in the sequence **"pays attention"** to all others to decide which words are most relevant for interpreting that position. Technically, for each token _i_, a **query vector** \$q_i\$ is calculated and, for each potential reference _j_, a **key vector** \$k_j\$ (both obtained by projecting the initial embeddings). An affinity score \$s_{ij} = q_i \cdot k_j\$ is calculated (higher if word _j_ is relevant for interpreting _i_). These scores activate a weighted sum of the **values** \$v_j\$ (another projection of each token) to produce the output for position _i_. In formula:

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

where \$Q\$ is the query matrix of all tokens, \$K\$ of the keys, \$V\$ of the values, and \$\\sqrt{d_k}\$ is a normalization factor (dimension of the key vectors). The softmax produces weights that highlight the positions most similar to the query. The result is that the output for token _i_ is a combination of the representations of _all_ other tokens, **weighted** based on relevance. For example, in _"She put down the glass because it was_ _fragile."_, the word _"fragile"_ as a query will assign high attention to _"glass"_ and much less to _"she"_, successfully resolving the reference of _"it was fragile"_[\[18\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,%E2%80%9D).

- **Multi-Head Attention:** instead of a single Q,K,V projection, the Transformer performs several in parallel (_multi-head_). Each _head_ is like an attention channel that can focus on a different type of relationship (e.g., one head might focus on subject-verb syntactic relationships, another on pronominal coreferences, etc.). The results from different heads (operating on reduced dimensional subspaces) are concatenated and linearly combined. This enriches the expressive capacity: the model can **simultaneously** consider multiple dependency aspects for each token[\[19\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order).
- **Positional Encoding:** a "disadvantage" of pure self-attention is that it treats inputs as an unordered set - if we swap two tokens, the attention scores change, but the architecture has no intrinsic information about sequential position. For this reason, a **positional encoding** (fixed sinusoidal or learned) is added to each initial embedding. Thus, the input vectors contain both the meaning of the word and its absolute/relative position. In this way, the network can distinguish "the cat ate the mouse" from "the mouse ate the cat" based on positional offsets[\[20\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,are%20added%20to%20preserve%20order).
- **Encoder-Decoder vs variants:** The original model is Encoder-Decoder: an **Encoder** reads the input sequence (e.g., sentence in the original language) producing contextual representations; a **Decoder** generates the output sequence (e.g., translated sentence) one token at a time, "looking" at both the internal states of the encoder (via _cross-attention_ between decoder and encoder) and the tokens already generated (via autoregressive self-attn with a future mask). Today, simplified configurations exist: **BERT**-type models use only the encoder (bidirectional self-attention on the entire input, for comprehension tasks), while **GPT**-type models use only the decoder (unidirectional, autoregressive self-attention, for text generation)[\[21\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Original%20architecture%3A).

**Why the Transformer scales (and changed everything):**

- _Total parallelization:_ Unlike an RNN, the Transformer has no sequential dependencies _within_ each layer. Each self-attention layer can process **all tokens in parallel** through optimized matrix operations on GPUs/TensorCores. Sequential dependency remains only in the decoder's autoregression during **inference** (step-by-step generation), but during _training_, even the decoder can be trained in parallel using masks (the entire output sequence "shifted" as input). In fact, whereas before analyzing 10 words required 10 steps one after another, now I can perform 1 step that covers 10 positions with attention. The efficiency gain is enormous, especially on long sequences[\[22\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order)[\[23\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,of%20sequential%20computation%2C%20however%2C%20remains). Transformers trained on gigantic datasets became feasible _only thanks to_ this feature.
- _Effortless long-range:_ In a single attention layer, each token can interact _directly_ with any other, even at a distance of 50 positions, _with a single step_. In an RNN, connecting distant tokens requires many steps and gradients might vanish along the way. The Transformer, instead, calculates global dependencies **in one go**. This leads to naturally capturing long-term relationships (e.g., who is the subject of a distant verb, long-distance agreements, etc.) much better than RNNs. Formally, in Transformers, the number of operations required to connect any two positions is constant (1 per layer, or a few layers for higher-order relationships)[\[24\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=hidden%20representations%20in%20parallel%20for,as%20described%20in%20section%C2%A0%2016), while in an RNN it is proportional to their distance in the sequence.
- _Greater expressivity:_ The combination of multi-head attention and position-wise feed-forward (each layer also has an FFN network that individually processes each position after attention) provides the model with an enormous representation capacity. A Transformer with sufficient heads and layers can effectively simulate even sequential operations, but it has the freedom to learn very different structures (e.g., topological orderings of words to represent the syntax tree, etc.). The community has discovered that Transformers tend to spontaneously learn things like _grammatical patterns_, _semantic relationships_, and even perform _latent forms of reasoning_ when scaled. In short, their generalization capacity as size increases is superior to that of previous models.

**RNN vs Transformer - summary comparison:**

- _Parallelism:_ RNNs process 1 token at a time (difficult to parallelize), Transformers process N tokens in parallel (much more efficient on modern hardware)[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental).
- _Long-term memory:_ RNNs/LSTMs store in the hidden vector with potential attenuation, Transformers with self-attention look directly at every relevant part of the sequence (theoretically infinite context, limited only by the manageable input length).
- _Complexity:_ The cost of self-attention grows \$O(N^2)\$ with length (because it compares every pair of tokens). This is more expensive than an RNN (\$O(N)\$ per step), but the trade-off is widely offset by parallel execution. On short/medium sequences, the Transformer is significantly faster for the same resources; on _very_ long sequences (e.g., thousands of tokens), it can become heavy in memory and time, but many _sparse attention_ algorithms have been introduced to mitigate this.
- _Data needed:_ In general, Transformers have more parameters and flexibility, so they tend to require a lot of data to express their potential. Fortunately, the era of big data has provided immense corpora; furthermore, self-supervised training (language modeling) has made it possible to use practically the entire internet as data.
- _Modular architecture:_ The Transformer is easier to distribute across GPU clusters (each layer is a series of standard matrix operations). Moreover, it is modular: parts can be replaced (e.g., different attention schemes, different pos. encoding) without overhauling everything. This has led to an ecosystem of rapid variants and improvements.

**Conceptual schema (step-by-step) of a Transformer encoder layer:**  
1\. **Input Embedding + Positional Encoding:** a position vector is added to each token.  
2\. **Self-Attention (multi-head):** $Q,K,V$ are calculated for each token and attention is computed. Each token "collects" relevant information from other tokens. (In the autoregressive decoder, a mask would be applied to prevent peeking at the future).  
3\. **Add & Norm:** there is a residual skip connection that adds the original input of the attention to its output, followed by layer normalization. This helps gradient flow and stability (the model basically learns a sort of identity + corrections).  
4\. **Feed-Forward Network (FFN):** an MLP applied separately to each position (same network for all tokens). Usually 2 layers with ReLU/GELU, expands and contracts dimension (e.g., from d_model=512 to 2048 and back). It serves to introduce non-linearity and mix the information synthesized by the attention.  
5\. **Add & Norm:** another residual connection adding the FFN input (which was the output of the attn sublayer) to the FFN output, then normalization.

A stack of N such layers forms the Encoder. In the Decoder, each layer additionally has a *cross-attention* block after the self-attn, where the *queries* come from the previous decoder layer and the *keys/values* from the final output of the Encoder

- **Auto-encoder models (BERT-type):** use only the encoder part of the Transformer and are trained with *bidirectional* tasks such as **Masked Language Modeling (MLM)**. In BERT (Devlin et al., 2018), 15% of the input words are randomly masked and the model must predict them by looking *both to the left and to the right* (thus fully exploiting bidirectional attention)[\[27\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=BERT%20is%20trained%20on%20two,clever%20tasks). Furthermore, BERT was also trained with an auxiliary task of *Next Sentence Prediction* (deciding if two sentences were in sequence in the original text), encouraging discourse understanding[\[28\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=1,it%20learn%20relationships%20between%20sentences). BERT provided powerful contextual embeddings that, with light fine-tuning, radically improved dozens of NLP tasks (from classification to QA to NER). These models are not designed for arbitrary generation (they lack an autoregressive decoder), but they excel in **comprehension** and in producing representations to be used as input for simple classifiers. BERT was a watershed: within a few months, practically every NLP benchmark was dominated by BERT variants. Among these: *RoBERTa* (2019, better trained and without NSP), *ALBERT* (2019, smaller thanks to factorization and sharing), *DistilBERT* (2019, compressed), etc.

Subsequently, attention shifted increasingly toward **large generative models**, particularly with the release of GPT-3. The community discovered that *scaling model size and data leads to substantial and sometimes qualitatively new improvements*. This was formalized in the **Scaling Laws** by Kaplan et al. (OpenAI 2020): *perplexity* (a measure of language model goodness) decreases following approximately a power law as parameters, data, and compute used increase[\[29\]](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves). No sign of saturation appeared on the horizon: *bigger is better*. In other words, if you double parameters (and proportionally data and computation), the error drops in a predictable way (albeit with slightly diminishing returns). This encouraged a **"gigantism"** in models: GPT-3 with 175B was followed by even larger models like **Megatron-Turing NLG** (Microsoft-Nvidia, 530B, 2021) and various Chinese models with 200+ billion parameters.

However, in 2022, a study by DeepMind (**Chinchilla** by Hoffmann et al.) recalibrated the perspective: it was discovered that many LLMs were *under-trained* relative to their size. Chinchilla (70B parameters) was trained with 4 times more data than that used for GPT-3, showing superior performance to GPT-3 despite being < half the parameters, because it had better utilized the compute budget[\[30\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,Transformers%20to%2022%20Billion%20Parameters). In practice, for a fixed *compute* budget, there is an optimal balance between model size and the number of training tokens: models that are too large trained on too little data do not reach their potential; it is better to reduce parameters and extend training. This led to a shift: not just increasing parameters, but ensuring there is *much more data* (which is a problem, because after scraping the entire textual internet, synthetic or multimodal data are needed to continue).

**Emergent Abilities:** A fascinating phenomenon observed with LLMs is the appearance of capabilities _not present in smaller models_. Wei et al. (2022) have cataloged various **emergent abilities** that arise beyond certain parameter/data thresholds[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up). For example, models under 10B parameters fail to perform simple arithmetic calculations or multi-step logical reasoning, while models like GPT-3 (175B) manage to do 3-digit additions, zero-shot translations, explain jokes, write simple code, etc. These abilities _do not scale linearly_ but seem to activate suddenly when the model surpasses a "critical point" of knowledge and generalization. There is debate as to whyâ€”some say they actually emerge gradually but become measurable beyond a certain noise level; others that models begin to perform _meta-learning_. The fact remains that very large LLMs show qualitatively different behaviors: they can follow instructions (**instruction following**), plan reasoning via _chain-of-thought_, interface with tools if properly instructed, etc., while small models tend to return disconnected sentences or fail to grasp more complex tasks.

**"Bigger is better" - until when?** The push to scale has brought enormous progress, but it does not solve _all_ problems. Beyond the computational cost (training GPT-3 cost on the order of millions of \$, GPT-4 even more), there are **practical limits**: huge models are difficult to update, deploy, and run with low latency. Furthermore, some weaknesses (e.g., tendency to _hallucinate_ facts, bias) persist even as the model growsâ€”simply put, large models hallucinate more convincingly ðŸ˜…. Studies like Chinchilla suggest that there is no need to increase parameters infinitely if they cannot be fed with adequate data. Today, much research focuses on _efficient scaling_: better data selection, specialized architectures for long contexts (Transformers with sparse or recurrent attention), **smaller but specialized** models (the emerging so-called **Small Language Models**). An example is **Alpaca** (Stanford, 2023): a model of only 7 billion parameters (based on LLaMA) that, with instruction fine-tuning, manages to behave similarly to ChatGPT on many common requestsâ€”indicating that with the right specialization, a 100B monster is not always needed to deliver value.

In conclusion, from 2018 to today we have moved from Transformers with ~110M parameters (BERT-base) to LLMs with hundreds of billions. This **scaling** has unlocked latent capabilities and opened up new applications. But it has also highlighted problems of **alignment** (avoiding toxic outputs, ensuring truthfulness) and **efficiency**. This brings us to the current context, where an LLM is rarely used "alone": it is integrated into broader systems to be made reliable, updatable, and useful in real-world application contexts.

## LLM as _system components_, not just models

A modern AI engineer knows that using a powerful **"raw" and isolated** LLM is often not enough. Today, LLMs are typically encapsulated in broader architectures where other components mitigate their limits and enhance their capabilities. Here are the main roles and integrations:

- **Prompt Engineering and formatting:** The _prompt_ is the immediate interface with an LLM. Since these models are _task-agnostic_ (they do not have a predefined goal beyond generating plausible text), the user must define the **instruction** or question clearly and often include _additional context and examples_. Designing good prompts is an art: e.g., providing the desired format in the request, or concatenating a short example conversation that shows the model how it should respond. For complex systems, prompts are sometimes constructed automatically by combining various pieces (instructions, retrieved knowledge, conversation memory, etc.) - this is referred to as prompt **orchestration**. Good prompt engineering can improve reliability and precision without touching the underlying model. In production, prompts must also be **version-managed**: small changes (an extra sentence, a different example) can significantly alter the output. Therefore, logging and testing on prompts are needed to ensure that behavior remains stable as prompts vary and potentially across different model versions.
- **Retrieval-Augmented Generation (RAG):** One of the major problems with LLMs is that their _knowledge_ is static (limited to training data) and sometimes inaccurate. The RAG technique seeks to **ground** the LLM in up-to-date and factual information by integrating a _retrieval_ component into the loop. In practice, when faced with a question or task, the system first performs a search in an external knowledge base (documents, databases, web) and then constructs a prompt that includes the found content as **context** for the LLM. The LLM is then guided to formulate the response based on that context instead of internal knowledge alone[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static). For example, if we ask: _"What is the inflation rate in Italy this year?"_, a base LLM (trained until 2021) can only guess and risks hallucinating; with RAG, the system will search reliable sources for the latest data and provide them to the model, which will summarize them correctly. **Benefits:** RAG addresses both the problem of stale knowledge (because it inserts updated info) and reduces hallucinations (the model is "grounded" in explicit sources)[\[33\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues). Furthermore, it allows for smaller LLMs with limited general knowledge but integrated with extensive **external memories**. Many "LLM-enabled" applications (such as chatbots on company documentation, customer support assistants, Q&A engines on specific data) use RAG: they "break down" the user prompt into queries for a search engine (often on a **vector database** with semantic embeddings of documents) and package the results into a final prompt for the LLM.

- **Tool use and API calling:** Next-generation LLMs can be seen as linguistic "brains" that reason but have no direct interaction with the outside world (other than text). To extend their capabilities, they are equipped with the faculty to **call external tools**. For example, an LLM integrated into an assistant could, upon request, invoke: calculators, weather services, SQL databases, Python functions, search engines, etc. This requires an architecture that intercepts when the model "decides" to use a tool. Various approaches exist: one is the **ReAct** (Reason+Act) pattern, in which the model explicitly produces a _chain-of-thought_ and action commands (e.g., Search("latest inflation news Italy")), which the system executes, then returns the result to the model to allow it to continue generation[\[34\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=decision,and%20coordinate%20with%20other%20agents)[\[35\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=Overview%20of%20LLM,to%20the%20LLM%20prompt%20through). Another approach is to provide the LLM with a list of available functions (with relative documentation in the prompt) and have it emit a special syntax when it wants to invoke them (see e.g., _OpenAI function calling_ or _LangChain tools_). The key idea is that the LLM acts as an intelligent **controller**: it understands which tool is needed and with which parameters, delegates the sub-task, and then incorporates the response into its flow. This enormously increases reliability on tasks where the pure LLM would be weak: precise mathematical calculations, data lookup, real-time web interactions, image manipulation, etc. In practice, the LLM transitions from a _soloist_ to an _orchestrator_ of a network of services.
- **Agents (LLM-driven agents):** An _agent_ is a more complex system that combines the mechanisms above (memory, search, tools) to pursue higher-level goals autonomously. An LLM agent typically: receives a goal (e.g., _"Book a flight from Milan to New York for next Friday under â‚¬500"_), then plans a series of actions (searches for flights, compares prices, perhaps asks the user for confirmation, and finally calls the booking API). During this process, the LLM might need to **iterate**: reflect on partial results, update the plan, handle errors (e.g., no flight under â‚¬500, relax constraints). Implementing reliable agents requires care: the model must be provided with a sort of _loop_ where it can generate thoughts and actions in cycles until it reaches a termination condition. Furthermore, it is necessary to ensure it does not take unwanted steps. Frameworks like **LangChain**, **Microsoft Semantic Kernel**, or **Hugging Face transformers agent** provide abstractions for building agents with LLMs, defining available tools and managing the prompt cycle with action results. This is a frontier field, but it promises AI systems that are more **autonomous and proactive** in solving complex problems by breaking them down into sub-problems (much like we humans would). A core principle that has also emerged in OpenAI/AWS guides is: _"an LLM alone is not enough for intelligent and reliable behavior; it needs to be embedded in a structured workflow with planning, memory, tools..."_[\[34\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=decision,and%20coordinate%20with%20other%20agents)[\[36\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,building%20robust%2C%20extensible%2C%20and%20intelligent).

- **Quality control, safety, and observability:** By integrating LLMs into larger systems, we also have the opportunity to introduce **verification** and **monitoring** modules. For example, after the LLM generates a response, we could have a **validation** step (another model or a set of rules checks if the response meets certain criteria: no prohibited content, no missing info, correct format, etc.). Or implement a **self-reflection** loop: the model re-reads its response and evaluates whether it seems coherent and correct (additional _chain-of-thought_ or _vote/verify_ techniques). In production, it is crucial to have **telemetry**: measuring latency of LLM calls, number of tokens used, parsing error rates, etc. **LLM observability** tools are used to track not only classic metrics but also indicators such as: frequency of detected hallucinations, cost trends (tokens per request), types of requests made by the user, and user feedback. All of this falls under **ML-Ops for LLMs** (sometimes called LLMOps). You can't really put a conversational model in the hands of millions of users without adequate logging and monitoring: "things can get weird in production" - spiking latencies, out-of-policy outputs after an update, runaway costs, etc[\[37\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=experiences,after%20a%20minor%20prompt%20change)[\[38\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1.%20Latency%2C%20cost%2C%20and%20third,dependencies). An AI Engineer must implement **guardrails and alerts**: for example, if the usage rate of a tool (external API) called by the agent suddenly rises abnormally, there might be a prompt injection in progress; or if the average response time increases, perhaps the model is "reasoning" too long on certain queries (maybe malicious users are providing inputs to stress it). LLM observability means being able to _see inside_ these dynamics and react[\[39\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=Engineering%20teams%20need%20more%20than,built%20to%20handle%20AI%20workloads)[\[40\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1,and%20scoring%20of%20LLM%20responses).

In summary, **today an LLM in production is rarely bare-bones**. It is wrapped in a layer of **engineered prompts**, with possible **retrieval** for updated knowledge, the ability to call external **tools**, and **control** modules. All this for reasons of:

- **Latency & cost:** minimizing useless tokens (optimized short prompts[\[41\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,the%20entire%20history%20every%20time), caching frequent responses[\[42\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,logic%20and%20cache%20invalidation%20strategy), making the model do only what is necessary and delegating the rest). For example, if I know that 90% of user queries are simple, I could use a smaller/cheaper model for those and call the large model only for the difficult 10%[\[43\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=%2A%20Model%20right,com). Or _summarize_ the conversation in the background to avoid passing the entire context every time (reducing tokens, thus cost, and latency).
- **Reliability:** using tools for tasks where the LLM is weak (calculation, real-time data), retrieval to improve factual accuracy, validation to prevent incorrect outputs. This increases confidence that the system responds correctly and decreases risks (e.g., if the LLM hallucinates a financial figure, we could mitigate this by requiring it to _always_ cite a knowledge document: no document = the model must say "I don't know").
- **Maintainability:** by keeping the pieces separate, I can update the knowledge base without having to re-train the model; I can modify prompts or add new tools if requirements change. It is a more modular and engineerable approach compared to seeing the LLM as a monolith.
- **Observability:** a component-based design allows logging the interactions between them. I can see which document was fetched in RAG, which tool was called and with what input, and of course the user-model conversation. These logs help diagnose problems: if the LLM gives strange answers, perhaps the knowledge document passed was wrong or the prompt has degraded. Without this visibility, an LLM is a black box that "occasionally goes haywire" and you don't know why.

```markdown
In conclusion, the _system view_ of an LLM is like **a linguistic brain inserted into a body with sensors and actuators**: the LLM provides general cognitive abilities (understanding, reasoning, language), but it needs "eyes and ears" (search modules, databases) and "hands" (APIs to act) to be truly useful and reliable in the real world.

## Limiti strutturali dei LLM

Despite the miracles they seem to perform, current LLMs have important **intrinsic limits**. Understanding them is crucial because many usage challenges arise from these limits, which are not solved simply by "training a bit better" but require architectural or system interventions (as seen above). The main ones are:

- **Hallucinations:** An LLM "hallucinates" when it invents information that does not correspond to factual reality, even while expressing it convincingly. Example: you ask a model to list an author's works and it inserts 2 non-existent books among the correct titles, without blinking. Why does this happen? At a fundamental level, the training of an LLM pushes it to _predict the next most probable word_, not to verify truth. If a name often appears associated with certain facts in the training data, the model will repeat them even if they are false in that specific case. Furthermore, when pushed out of distribution (a question about something it has never read about), the model **still tends to give an answer_, because that is how it is trained (penalized if it does not produce output). It does not have a "verified" knowledge base

- **Difficulty of incremental updates:** Linked to the previous point, LLMs **do not have a separable memory** that is easily updatable. If a country's capital changes, a classic DB-based system modifies a row in a table and all future queries reflect the change. An LLM, on the other hand, has "knowledge" imprinted in the synaptic weights of a massive network: to update one piece of information, you would need to retrain (extremely expensive) or attempt neuronal editing/local fine-tuning techniques. But these interventions are risky: _catastrophic forgetting_ (you change one piece of information and unknowingly ruin others connected to it), _overfitting_ (the model starts repeating the updated training phrase and loses fluidity), etc. In short, LLMs **are not designed as updatable knowledge bases**, but as static statistical models. Here too, RAG is a patch: you keep the knowledge in an external DB and make the model use it, so you update the DB and the model "knows" things again. But the model itself remains static; if you ask it without providing updated context, it will give you the old information. In critical applications, this is a major limitation (think of medical assistants that must keep up with guidelines, or news chatbotsâ€¦ you can't retrain GPT-4 every day with the news).
- **Bias and toxicity:** LLMs learn from training data, which includes large portions of the Internet, social media, booksâ€¦ Unfortunately, this data contains **cultural biases, stereotypes, hate speech, disinformation**, and so on. As a result, the model internalizes them and, if not filtered, can reproduce or even amplify them. There are documented cases of models generating racist or sexist outputs when provoked. Companies have introduced **fine-tuning with Human Feedback (RLHF)** techniques and filters to mitigate these problems (for example, ChatGPT has a moderate "default personality" and refuses certain content). But it is a _post hoc_ mitigation. Intrinsically, if you ask an LLM to impersonate a certain toxic role or explore in an unfiltered manner, the original biases can emerge. Furthermore, even on non-toxic things, LLMs can have subtle biases: e.g., a tendency to name more male inventors than female, or assuming Western contexts as default in stories, etc. These reflect the datasets (more content on historical men, etc.). **Correcting bias after training** is difficult: you must either filter incoming data (proactive censorship, but risks reducing diversity) or apply penalties via RL (risks ruining knowledge). It is an active field of research. As an engineer, you must be aware of this: _never_ assume that the LLM is neutral or free of prejudice. It must be tested and monitored, especially on sensitive outputs (e.g., advice in the medical-legal field).
- **Adversarial fragility and inconsistency:** LLMs can be surprisingly **sensitive to small changes** in the prompt. For example, reversing the order of two sentences in the question can sometimes lead to different answers. Or adding a superfluous detail can confuse the model. There are also _prompt injection_ attacks: if the user inserts something like "Forget previous instructions. \[Malicious instruction\]" into their input, some models might obey and violate the original constraints. This vulnerability arises from the fact that the model does not have a strong concept of _higher-level truth_ or _permissions_: every input is text to be continued, so if the input contained "The following is a malicious prompt: ..." the model might incorporate it into its internal narrative. In short, **they lack formal robustness**. Even internal logical consistency is not guaranteed: they can contradict themselves, or provide two different answers to paraphrased questions. For an engineer, this means you need to put up **safety nets**: e.g., validate answers through separate models/verifiers, do not trust blindly on matters of correctness. A/B testing of prompts and behavioral _unit tests_ for the model are also desirable to understand how it responds to various phrasings, and to choose the least unstable ones.

**Why are these intrinsic limits?** Ultimately, because they derive from the very nature of language models. An LLM is trained to compress the statistics of a massive corpus of text into its parameters. It has no direct perception of the world nor a causal model of the world (only linguistic correlations). Therefore, it cannot know if a statement is _true_, it can only judge if it is _probable_. Bias and toxicity are present because they are present in human data and the model has no ethical values of its ownâ€”unless we insert them through additional objectives. Inconsistency and fragility derive from not having reliable symbolic reasoning: even if advanced models show traces of logic, at their core they do not perform symbolic inference, so they can fall into contradiction or be misled.

These limits _are not bugs that can be fixed with a patch_, but foundational aspects. This means that when we design systems with LLMs, we must **build around** them to mitigate them. For example, for up-to-date knowledge (grounding) we use RAG; for hallucinations, we can have a search engine double-check the response or provide sources; for bias/toxicity, we put moderation filters and style definitions in the prompt; for inconsistency, we use agents that double-check responses or segment problems into easier sub-problems.

In the future, new architectures (e.g., deeper integration with knowledge bases, or multi-modal models that _see_ and _act_ in the environment) may reduce these limits. But as of 2026, those who use LLMs must do so with awareness of these _intrinsic uncertainties_, adopting an "AI safety" mindset: never let an LLM make irreversible decisions without supervision, and structure products so that it is possible to intervene if (when) something goes wrong.

## Connection with your GEO & Disaster Response path

Let's now turn to the use case that interests you: applying these technologies in the geospatial and disaster response field (earthquakes, natural disasters, etc.). This sector combines _multi-modal_ data (texts, maps, satellite images, sensors) and requires both **precise quantitative analysis** (e.g., detecting damage from images) and **synthesis and reasoning capabilities** (e.g., drafting a situation report, making inferences about risks). LLMs can play a valuable role, but **they must be correctly integrated** with existing geospatial workflows. Let's look at some scenarios:

- **LLM + RAG for post-earthquake reports:** Imagine that after a strong earthquake you need to quickly create a report summarizing the damage, the most affected areas, the state of infrastructure, and possible actions. Various sources are available: fire department reports, geolocated social media posts, satellite images with collapse analysis, GIS databases with buildings and population. An LLM alone _knows_ nothing about the earthquake (unless trained on past events, but not on the new one). However, we could use it as a **language generation engine** by feeding it specific event data. With a RAG approach, the system can retrieve, for example: _"textual reports from the fire department in the last 12h"_, _"results of automatic analysis from images (X buildings collapsed in area Y)"_, _"lists of blocked roads from live maps"_. This information is inserted (perhaps in an already summarized form) into the prompt, and the LLM is tasked with drafting a **coherent and readable report** for, for example, the authorities. The LLM excels at **connecting the dots**: it can take the list of facts and transform it into a narrative: "In the northern part of the city (District XX), approximately 30 buildings have collapsed, with the highest concentrations of damage along Via Alfa and Via Beta. Rescue teams have saved 12 people from the rubble and report at least 5 missing. The bridge over the river is impassable, temporarily isolating the Gamma hamlet...". Without an LLM, a human operator would have to manually write this summary by integrating many sources; with the LLM, the operator can focus on verifying and correcting, instead of writing from scratch. **Important:** as seen, here the LLM must be _grounded_ to real data: we don't want it to invent numbers of missing persons! Therefore, we provide precise figures and details via RAG, and perhaps ask the model to cite sources (if the output is for internal use only). In this way, the LLM does well what it knows how to doâ€”language and textual reasoningâ€”but does not act "in the absence of information."

- **LLM + agents for emergency decision support:** In crisis situations, a decision-maker might query an AI system with complex questions, such as _"Where should we concentrate USAR teams based on reports and damage data?"_. Responding requires: understanding the question (linguistic task), having data (geospatial and textual), and reasoning by combining _criteria_ (for example: USAR teams = urban search and rescue, so they are needed where collapsed buildings and potential trapped population are highest). A single static model would struggle. But we can build an **LLM agent** equipped with tools: one that queries a GIS database for the number of collapsed buildings per zone, one that reads the latest incoming SOS messages, and one that consults the registry of already deployed teams. The agent can create a plan like: 1) obtain a collapse density map; 2) obtain a list of reports of trapped people; 3) cross-reference by zone; 4) propose priorities. Steps 1) and 2) are done via tools (for example, by calling a geospatial API that returns data, or executing a query on an emergency knowledge graph). Then the LLM itself can generate a response like: _"The areas with the greatest need for USAR seem to be A and B. In neighborhood A (20 collapsed buildings, ~50 people reported under rubble) there is currently only one operational team; I suggest sending at least two more. In neighborhood B (15 collapses, 30 people reported) the situation is similar. Areas C and D have fewer collapses or already have sufficient coverage."_. This is **decision support**: the LLM does not make the decision but provides a reasoned and quickly readable analysis, integrating disparate data (GIS + reports + resource status). This allows the person in charge to confirm and act much faster. Again, the LLM here acts as an **intelligent collector**: it manipulates data with reasoning and presents it effectively.
- **LLM as a "cognitive interface" over Remote Sensing (RS) models:** In satellite imagery analysis or remote sensing, we often obtain technical results: classification maps, confusion matrices, damage percentages per cell, etc. An LLM can help translate these raw outputs into **human-usable insights**. For example, a computer vision model processes post-disaster images and produces shapefiles with polygons of flooded areas and a severity indicator per area as output. An LLM could take these results (converted into structured text) and generate a **briefing**: _"Satellite analyses indicate extensive flooding along the Delta River: approximately 45 kmÂ² of territory are flooded. The municipalities of X and Y are particularly affected, where water has covered ~30% and ~45% of the urban area, respectively. The industrial area of Y is entirely submerged with possible release of substances into the water. The main infrastructures affected include the SP123 and the Z railway, both of which are interrupted."_. Note how many deductions and aggregations are included: the LLM can describe the total area (summing polygons), convert that information into an impactful sentence, identify municipalities within the polygons (cross-referencing coordinates with names via a GIS tool in the backend), and mention affected infrastructure (if it has vector data on roads and railways, it can cross-reference them). In short, we use it as an _intelligent report generator_ that sits on top of numerical models. **What should the LLM not do?** It should not perform the image _segmentation_ itself! To recognize flooded pixels, there is a specialized vision model that works on rasters and perhaps uses convolutional networks or other architectures. The LLM does not have direct visual perception (unless using a multimodal model, but currently, for precision tasks, dedicated models are better). Therefore, the rule is: leave the quantitative "pixel-wise" work to RS models (they are trained for high accuracy on that), and use the LLM to **connect those results with knowledge and present them**. An LLM can, for example, explain why a certain flooding pattern is dangerous ("this area was already prone to landslides; the flood makes it unstable"), something a pure RS model does not do.

**Multimodality (text â†” images â†” geospatial):** It is worth noting that the current trend is towards models capable of ingesting multiple forms of data. For example, _GPT-4_ has vision capabilities: you can give it an image and it produces text about it. There are models like **CLIP** and **BLIP** that connect vision and language. For geospatial data, works are emerging that integrate georeferenced graphs with LLMs (e.g., GraphRAG in geospatial). So in the not-too-distant future, you could have a _multimodal LLM_ that directly takes both maps and texts. Already today, services like **Google's PaLM-E** aim to unite vision, language, and robotics. In the context of disasters, imagine giving the model both the damage map and localized tweets: a multimodal model could directly combine them and explain the situation to you. We are at the beginning of thisâ€”for now, the modular approach (vision model + LLM) is more practical. But keep an eye on research, because tools like **Imagen (Google)** or **Kosmos-1 (Microsoft)** are building bridges between visual data and LLMs.

**What an LLM _must not do_ in geospatial/DR:** as already mentioned, do not entrust an LLM with the **technical precision** that requires dedicated algorithms. If you need to get the latitude/longitude of an address, use a geocoding API, don't ask the model to invent it! If you need to calculate the magnitude of an earthquake from seismographic data, you need physical formulas, not an LLM's "opinion". LLMs have no guarantees of numerical accuracy or scientific rigor. Therefore, the _core_ parts of analysis (detecting damage, calculating extents, exact counting) must be done with deterministic methods or specialized ML models. LLMs instead excel in: **synthesis, high-level correlation, communication, Q&A**. Furthermore, they are excellent at filling general knowledge gaps: if in a report you also need to explain concepts (e.g., what a seismic fault is, or what the effects of soil liquefaction are), the LLM can generate those paragraphs by drawing on its trained knowledge.

**Integration with existing GEO pipelines:** You could imagine your system as: data ingestion pipeline (satellites, sensors, open data) â†’ analytical models (CV for images, GIS computations, etc.) â†’ **LLM layer** for output to the user. During the design phase, define the API between the analytical layer and the LLM well. It is often convenient to structure the data in a textual format understandable to the model (e.g., bullet points or JSON), also including explanations. For example, instead of throwing in raw numbers, you could say: "Road X: interrupted (collapsed bridge)". This way, the LLM already knows that road X is interrupted and why, and can easily include it in its narrative, perhaps reasoning "collapsed bridge â†’ that municipality to the north is isolated". If you only gave "road X status: 0", it would have to infer the meaning of 0, which is much harder. Therefore, doing some **data preprocessing for LLMs** is useful: converting technical results into simple sentences or statements.

**Cross-validation:** in safety-critical areas (disasters are), an LLM must not be the only voice. _Ensemble_ approaches can be used: have the LLM generate the report, then have another LLM proofread it, asking to highlight contradictions or possible errors, and finally have a human-in-the-loop (an operator) who verifies key points. Or generate two versions (perhaps with different temperatures or different prompts) and compare. In short, use the LLM as an assistant, not as an oracle.

In conclusion on GEO & Disaster Response: an LLM can act as an **intelligent collector and communicator** on top of geospatial data. Think of it as a **virtual analyst** who knows a bit of everything (thanks to general training) and who can be instructed to use your specific data to produce analysis and reports. It frees you from having to manually interpret every map and every table, proposing an integrated picture. But you, as an engineer, set up the ecosystem: specialized models to extract info from raw data, well-organized databases, and then the appropriately harnessed LLM (targeted prompts, RAG, tools) to stitch everything together. This way, you exploit the best of both worldsâ€”the quantitative accuracy of geo models and the _linguistic intelligence_ of LLMs.

## Conclusions and evolutionary conceptual map

To recap what we have seen, we present a **conceptual map** of the NLP â†’ LLM evolution, and some guidelines for an AI Engineer on what is fundamental to master and what can be (relatively) neglected:

### Summary conceptual map (NLP â†’ LLM)

- **Statistical era (1990s - early 2000s):** Approaches based on simple probability models and manual features. Examples: _n-grams_ for language modeling[\[2\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=N,words%20to%20guess%20the%20third), Markov models (HMMs) for sequential tagging, _bag-of-words + TF-IDF_ for IR and classification. **Limitations:** no understanding of meaning, context limited to a few words, require many observations to cover rare cases[\[3\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,handle%20long%20dependencies%20or%20variations). Engineers had to design features (keyword lists, regex patterns, etc.). Obsolete today except for fast baselines.
- **Early neural networks for NLP (2000s - early 2010s):** Introduction of **feed-forward** networks for language models (Bengio et al. 2003) and especially **word embeddings** (Mikolov 2013)[\[45\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Researchers%3A%20Tomas%20Mikolov%20et%20al,%28GloVe%2C%202014). Here the focus is on representing words in dense vectors that capture semantic similarity (famous _king-man+woman=queen_[\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D)). Neural models begin to outperform word counters, partially solving sparsity. **However**, these models do not yet model entire sentences well: embeddings are static, and feed-forward networks had a limited window context (e.g., 5 words). The _"pre-training + fine-tuning"_ paradigm emerges in a primitive version: general embeddings are pre-trained, then used in models for specific tasks.
- **Sequence modeling with RNNs (2014-2016):** The need for broader context leads to the massive adoption of **RNNs, LSTMs, and GRUs**. _Sequence-to-sequence_ with attention (Bahdanau et al. 2014) revolutionizes machine translation: an LSTM encoder encodes the source sentence, an LSTM decoder generates the target sentence, with **attention** acting as a flexible bridge (at the time, attention was a specific mechanism, not the entire architecture). LSTMs dominate many applications - e.g., speech synthesis, image captioning (image captioning combined CNN+LSTM). **Problems solved:** short-term memory, word order, variable lengths. **Remaining problems:** difficulty with very long dependencies (LSTM improves but doesn't work miracles for entire paragraphs)[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to), non-parallel and slow training[\[46\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,The%20fundamental), many tricks needed to avoid divergence (clipping, orthogonal initializations, etc.). In this phase, models began to have a few tens of millions of parameters and GPU training became standard in NLP.
- **The Transformer (2017):** _Game changer_. Introduces multi-head self-attention and abandons recursion[\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including). Result: models that are faster to train, scale to massive data, and capture global context better than LSTMs. In a few months, it replaces LSTMs in translation, then in practically every sequential task. Libraries like Tensor2Tensor and later Hugging Face accelerate adoption. Encoder-only Transformer models (BERT, 2018) and decoder-only (GPT, 2018) usher in the era of **pre-trained language models**.
- **Large-scale pre-training (2018-2019):** With BERT and GPT, the potential of training models on massive amounts of generic text and then reusing them is seen. BERT achieves SOTA on 11 NLP tasks with minimal fine-tuning - the "ImageNet moment" for NLP. GPT-2 shows fluid and coherent text generation like never before (to the point that OpenAI was initially reluctant to release it entirely, fearing abuse). The open community replicates BERT easily (see RoBERTa), while GPT-2 remains somewhat exclusive due to training costs. **ULMFiT** (Howard & Ruder) also appears, showing universal fine-tuning. The base models BERT-base (110M parameters) and GPT-2 (1.5B parameters) already seemed largeâ€¦ but it was only the beginning.

- **Large Language Models emerge (2020-2021):** OpenAI releases **GPT-3 (175 billion)**[\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on), demonstrating that *scaling* by an order of magnitude yields impressive zero-shot/few-shot capabilities. The concept of **prompting** spreads as an alternative to fine-tuning: GPT-3 solves tasks described in the prompt without changing weights. Other big labs follow: Google Brain with **PaLM (540B)**, NVIDIA/Microsoft with Megatron-Turing (530B). Sparse architectures are also explored (Switch Transformers with gating to reach trillions of effective parameters, Mixture-of-Experts), but dense ones like GPT-3 dominate. **Emergent abilities** become a research topic - large models show language understanding, basic arithmetic and logical reasoning, and programming, which small models did not show[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up). In parallel, DeepMind publishes **Gopher (280B)** and an analysis

- The **architectural foundations**: how a Transformer works (self-attention, multi-head, etc.)[\[47\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Core%20ideas%3A), differences between RNNs and Transformers[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental), what encoders vs decoders are. Not necessarily knowing how to derive the equations by hand, but understanding the _data flow_ and why it is efficient. This helps in debugging tensor dimensions, understanding shape errors, and reasoning about limitations (e.g., why a 2k token context model cannot accept 10k tokens without modifications).
- The concept of **pre-training vs fine-tuning vs prompting**: knowing that models like GPT/BERT are pre-trained on massive corpora with a generic objective, then can be _fine-tuned_ (updating weights) on specific tasks or _prompted_ with appropriate instructions. This influences design choices: if you have little specific data, prompt engineering might be better than weight fine-tuning, etc.
- Knowing the **main model families** and what they offer: BERT (encoder, bidirectional, excellent for understanding), GPT (decoder, generative), T5 ("unified" text-to-text encoder-decoder), and some open models like GPT-neo/Llama, Bloom. Not so much the implementation details, but the conceptual differences: an AI engineer must know how to choose "for this task I need a generative model (e.g., completion/assistant) vs a classification model (e.g., info extraction)."
- **Limitations and failure modes** of LLMs: _hallucination_, _bias_, _context length limit_, etc., and relative mitigations[\[33\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues)[\[38\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1.%20Latency%2C%20cost%2C%20and%20third,dependencies). This is essential for designing robust systems: if you know an LLM can invent things, set up checks; if you know it cannot process inputs > 4096 tokens, you must think about chunking or special long-form models.
- The principles of **Retrieval-Augmented Generation**: even if you don't implement the vectorization algorithm yourself, you must understand how a vector database can integrate, how to embed queries and documents, and what cosine similarity is. And above all, _when RAG is needed_: knowledge-intensive situations with up-to-date info[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static). Practical examples: chatbots for documentation (the model alone doesn't know precise answers, RAG is needed). Also know the limits: if documents are long, the model might not use them well if there are too many; if the query embedding fails, the model responds blankly. Therefore, test end-to-end pipelines.
- **Tool/Agents patterns**: Familiarity with at least one library (LangChain, etc.) to orchestrate LLMs with tools. It is not necessary to know the details of an agent algorithm like ReAct, but _yes_, you should know how the LLM can execute iterative steps and call functions[\[48\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,and%20coordinate%20with%20other%20agents)[\[35\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=Overview%20of%20LLM,to%20the%20LLM%20prompt%20through). It is also very useful to know how to read an agent's "chain-of-thought" logs for debugging.
- **Practical prompt engineering:** Knowing how to formulate prompts for various scenarios (e.g., role prompt, few-shot with examples, delimiting context with special tokens, etc.). Knowing tricks like: _"Think step by step"_ to encourage reasoning, or providing structured instructions ("Respond with JSON containing fields X, Y, Z"). This has become almost a programming skill. An AI engineer must iterate on the prompt to improve output and watch out for _injections_ from user input. In short, consider the prompt as part of the application code.
- **MLOps for LLMs:** Even if not in detail, basic concepts: logging, monitoring cost and latency[\[49\]](https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025#:~:text=Each%20integration%20serves%20production%20AI,evaluation%20metrics%2C%20production%20alerting%2C), regression tests (if I change the model or prompt, do I have test cases to compare responses?), version management (model v1 vs v2, how to rollout). And knowing how to use tools like the OpenAI Evaluation framework or prompt testing suites.

- **Ethics and policy:** Do not ignore AI Ethics aspects. An AI Engineer must at least know the model usage guidelines (avoiding discriminatory outputs, protecting sensitive user data, etc.). And know how to implement moderation filters (e.g., using moderation APIs on outputs, or dedicated models that classify generated text). This is both for social responsibility and to avoid legal or reputational trouble.

**What can be (relatively) ignored / delegated:**

- **Mathematical details of backpropagation and derivation:** How BPTT works exactly, formally proving the vanishing gradient, or deriving the attention equation by hand with pen and paperâ€”as an engineer, you can consider this background. Qualitative intuition is enough in most cases. In practice, libraries and papers already implement everything; you need to understand the effect (e.g., _"the gradient vanishes if the sequence is too long"_[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to), _"attention weighs relevant terms"_), but you don't need to know how to prove the why from scratch.
- **Outdated models:** Don't spend too much time mastering Naive Bayes, HMM, Markov n-grams, or even classic algorithms like CRF, SVM applied to textâ€”unless you are working on a very low-resource case where a simple model might suffice. Today, a small fine-tuned Transformer or an LLM via API almost always outperforms them, so knowing HMM theory is more historical than practical knowledge. (It is still useful to be aware of them for general culture and to understand terms found in old systems, but you will rarely implement them in new projects).
- **Implementing models from scratch:** It is not efficient to recreate a Transformer layer by layer if proven libraries exist (Hugging Face Transformers, PyTorch Lightning, etc.). Unless you are doing architectural research, an engineer can use pre-trained models and APIs. Therefore, you can "ignore" low-level code (like writing multi-head attention manually, with all the dimensioning). Better to focus on _how to integrate_ the model into the broader pipeline.
- **All models released on the market:** There are dozens of variants (ALBERT, XLNet, ELECTRA, DeBERTa, GPT-NeoX, etc.). You don't need to know them all in detail. It is useful to know macro-categories and maybe 1-2 names per category as examples. When you need a specific one, you can research it at the time. Focus on general ideas: e.g., "ELECTRA pre-trains as a discriminator instead of a masked generator, for efficiency"â€”concept ok, but you don't have to remember every detail. In practice, today you will use either mainstream models (BERT, GPT-3, etc.) or models trained specifically on your data (in which case you follow a known architecture). Minor differences between architectures matter little for usage.
- **In-depth linguistic theory:** Knowing what POS tagging is, what a syntactic dependency is, is useful. But you don't need a PhD in computational linguistics. Many classic linguistic concepts (formal grammars, etc.) have been implicitly incorporated into neural models. Once, grammar and semantics had to be hand-coded; today the model learns it. So, for example, you might never have to manually implement a syntactic parser if you use LLMs for textual analysis. Focus instead on how to evaluate outputs (BLEU metric, Rouge, etc.) and on practical notions (tokenization, etc.).
- **Pushing the SOTA to the limit:** If your goal is to build functional systems, it is not necessary to achieve top absolute accuracy on a benchmark with elaborate fine-tuning. Often a pre-trained model out-of-the-box + some prompt engineering already gives excellent results for products. Sometimes "good enough" wins over "perfect but complicated". Therefore, you can ignore micro-optimizations like "should I use Adafactor with linear decay vs AdamW with cosine schedule?" unless you are training models yourself. If you use APIs like OpenAI, these choices are abstracted away. (Of course, if you _train_ models, then yes, you must take care of hyperparametersâ€”but that is more the work of an ML researcher than a system implementer).

In summary, an AI Engineer must be **T-shaped**: broad knowledge of the landscape (from bag-of-words to LLMs, to understand existing solutions) but depth in those technologies that are essential today (Transformer and its evolutions, and how to put them into production). They can easily do without historical details and rigorous theoretical proofs, as long as they understand the _why_ and _when_ of each technique.

### Fundamental sources to really study

We close with some recommended sources (papers and blogs) that I consider fundamental for consolidating the knowledge discussed and staying updated:

- **Tomas Mikolov et al. (2013), "Efficient Estimation of Word Representations in Vector Space"**[\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D)[\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20mattered%3A) - _(Paper)_ Introduces **word2vec**. A milestone that explains the concept of distributed embedding and two algorithms (CBOW, Skip-gram). Relevant because it lays the foundation for the idea of dense representations that is still at the heart of language models today.
- **Sepp Hochreiter & JÃ¼rgen Schmidhuber (1997), "Long Short-Term Memory"**[\[50\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=functions,deal%20with%20vanishing%20gradients%20and) - _(Paper)_ Proposes the **LSTM** architecture to overcome the vanishing gradient in RNNs. It is a technical paper, but reading at least the introduction and understanding the components (input/forget/output gates) helps to understand how the concept of _memory over time_ was born. Useful for historical retrospection and because LSTMs are still used in some specific contexts.
- **Vaswani et al. (2017), "Attention Is All You Need"**[\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including)[\[17\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=mechanism,small%20fraction%20of%20the%20training) - _(Paper)_ An absolute must-read. Introduces the **Transformer**. Explains self-attention, multi-head, positional encoding, and shows results on translation. It is the foundation of everything that came after. After reading it, the concept of attention will become much clearer and the reason for the breakthrough will be appreciated. (It also contains some implementation details like _scaled dot-product_, useful to know).
- **Brown et al. (2020), "Language Models are Few-Shot Learners" (GPT-3 paper)**[\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on)[\[25\]](https://arxiv.org/abs/2005.14165#:~:text=approaches.%20Specifically%2C%20we%20train%20GPT,At%20the%20same%20time%2C%20we) - _(Paper)_ The abstract and some key sections show what happens when a model is scaled to 175 billion parameters. Introduced the phenomenon of **few-shot learning** within the prompt. Reading this paper helps to understand the emergent capabilities of LLMs and also the limits (they have an honest section on where GPT-3 fails)[\[51\]](https://arxiv.org/abs/2005.14165#:~:text=demonstrations%20specified%20purely%20via%20text,evaluators%20have%20difficulty%20distinguishing%20from). It is long, but I recommend focusing on the descriptive parts and the tables of examples.
- **Jared Kaplan et al. (2020), "Scaling Laws for Neural Language Models"**[\[29\]](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves) - _(Paper)_ A work by OpenAI that quantified how increasing model/data/compute causes the error to decrease in a predictable way. It is useful for gaining the intuition that bigger = better (up to certain limits) and concepts like _compute-optimal_. Even if you don't follow all the formulas, the message is clear: there is an efficient way to choose model size vs. data. This informed choices such as those of Chinchilla.
- **Hoffmann et al. (2022), "Training Compute-Optimal Large Language Models" (Chinchilla)**[\[30\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,Transformers%20to%2022%20Billion%20Parameters) - _(Paper)_ Important because it rectifies the scaling laws by considering the trade-off between parameters and tokens. Shows that a 70B model trained with 4x tokens beats an under-trained 175B model. A must-read to understand that it's not enough to accumulate parameters; they also need to be _fed_ sufficiently. There are very instructive graphs on perplexity as quantities vary. Concept of "compute-optimal" LLM.
- **Wei et al. (2022), "Emergent Abilities of Large Language Models"**[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up) - _(Paper)_ Essay (also in the form of a blog post on Google Research) that catalogs various _emergent skills_ that appeared beyond a certain scale, e.g., compositionality, multimodality, etc. It is useful for being aware of the differences between medium models and giant models. Also useful at a conceptual level: it discusses what an emergent ability is and which hypotheses explain the phenomenon. For an AI eng, it helps motivate _why_ large models have value (they do things qualitatively differently, not just a bit better than usual).

- **Lewis et al. (2020), "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"**[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static) - _(Paper)_ Proposes the **RAG** framework. It is the theoretical foundation behind many modern QA systems. It shows how to combine a document index with a neural generator. By reading it, you will understand the RAG architecture (encoders for queries and documents, top-k selection, conditioned generation) and see results on QA where the model with retrieval significantly outperforms one without. Fundamental for those who want to implement or improve _LLM + knowledge base_ systems.
- **Akanksha Sinha (2025), "From N-grams to Transformers: Tracing the Evolution of Language Models"**[\[52\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=5,Broader%2C%20Multimodal)[\[53\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20scale%20matters%3A) - _(Blog)_ A Medium article that summarizes a path similar to ours, including historical context. It is useful because it is written in a conversational style, touching on N-grams, Word2Vec, RNNs, Transformers, and Scaling, in a ~6-minute read. It can serve as a quick review or to explain the evolution to non-specialist colleagues (it also features images and analogies). A light read that nonetheless reinforces chronological understanding.
- **Jay Alammar (2018), "The Illustrated Transformer"**[\[54\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,language%20models%20and%20their%20implications) - _(Blog/Tutorial)_ A visual, step-by-step explanation of the Transformer. Alammar is known for his blogs with excellent diagrams and illustrations (in this post, he represents query-key-value with colored diagrams, etc.). It is highly recommended if you want to gain an _intuition_ for what happens inside attention without getting lost in algebra. After reading it, concepts like multi-head and residual connections become much more concrete. It is also a perfect resource to recommend to students or colleagues in training.

_(These sources cover theory and practice. Obviously, the literature is vast; other honorable mentions: Chris Olah's blog - e.g., "Understanding LSTM Networks" - for intuitive explanations of LSTMs; the OpenAI blog "Better Language Models and Their Implications" (2019) discussing GPT-2 and risks; the GPT-4 technical report (2023) to understand the capabilities and limitations of the most advanced model; and the Papers with Code website to stay updated on new SOTAs. But the 10 above offer an excellent foundation.)_

**Closing:** The revolution from classic NLP models to LLMs has combined solid theoretical foundations (neural networks, attention, probability) with a large-scale engineering vision (immense datasets, GPU infrastructures, system integration). As an AI Engineer, understanding this evolution allows you to make informed choices about _which model to use, how to train or integrate it, what limitations to consider,_ and ultimately how to build **effective and reliable AI systems**. We are only at the beginning of this new era: models will continue to evolve (perhaps becoming more multimodal, more efficient, more specialized), but the principles you have learned here will help you navigate the rapidly changing landscape of linguistic AI. Good luck on your GEO & Disaster Response pathâ€”with this knowledge, you will be able to make the most of LLMs to truly make a difference in critical and socially impactful applications!

## Bibliography

[\[1\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20fell%20short%3A) [\[2\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=N,words%20to%20guess%20the%20third) [\[3\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,handle%20long%20dependencies%20or%20variations) [\[4\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20it%20mattered%3A) [\[5\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,France%20%2B%20Italy%20%3D%20Rome%E2%80%9D) [\[18\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,%E2%80%9D) [\[19\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order) [\[20\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,are%20added%20to%20preserve%20order) [\[21\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Original%20architecture%3A) [\[22\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=%2A%20Self,are%20added%20to%20preserve%20order) [\[30\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,Transformers%20to%2022%20Billion%20Parameters) [\[45\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Researchers%3A%20Tomas%20Mikolov%20et%20al,%28GloVe%2C%202014) [\[47\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Core%20ideas%3A) [\[52\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=5,Broader%2C%20Multimodal) [\[53\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=Why%20scale%20matters%3A) [\[54\]](https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba#:~:text=,language%20models%20and%20their%20implications) From N-Grams to Transformers: Tracing the Evolution of Language Models | by Akanksha Sinha | Medium

<https://medium.com/@akankshasinha247/from-n-grams-to-transformers-tracing-the-evolution-of-language-models-101f10e86eba>

[\[6\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=Word2Vec%3A) [\[7\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=%2A%20,%28River%20edge) [\[27\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=BERT%20is%20trained%20on%20two,clever%20tasks) [\[28\]](https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16#:~:text=1,it%20learn%20relationships%20between%20sentences) Beyond "One-Word, One-Meaning": Contextual Embeddings - DEV Community

<https://dev.to/mshojaei77/beyond-one-word-one-meaning-contextual-embeddings-4g16>

[\[8\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=You%20can%20see%C2%A0that%20the%20,Vanishing%20gradients%20aren%E2%80%99t%20exclusive%20to) [\[9\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=In%20previous%20parts%20of%20the,between%20words%20that%20are%20several) [\[10\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=It%20is%20easy%20to%20imagine,it%E2%80%99s%20not%20obvious%20when%20they) [\[11\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=parameters%2C%20we%20could%20get%20exploding,Your) [\[12\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=use%20Long%20Short,deal%20with%20vanishing%20gradients%20and) [\[13\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=perhaps%20most%20widely%20used%20models,deal%20with%20vanishing%20gradients%20and) [\[50\]](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/#:~:text=functions,deal%20with%20vanishing%20gradients%20and) Recurrent Neural Networks Tutorial, Part 3 - Backpropagation Through Time and Vanishing Gradients Â· Denny's Blog

<https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/>

[\[14\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=Recurrent%20models%20typically%20factor%20computation,The%20fundamental) [\[16\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=The%20dominant%20sequence%20transduction%20models,the%20existing%20best%20results%2C%20including) [\[17\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=mechanism,small%20fraction%20of%20the%20training) [\[23\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,of%20sequential%20computation%2C%20however%2C%20remains) [\[24\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=hidden%20representations%20in%20parallel%20for,as%20described%20in%20section%C2%A0%2016) [\[46\]](https://ar5iv.labs.arxiv.org/html/1706.03762#:~:text=input%20and%20output%20sequences,The%20fundamental) \[1706.03762\] Attention Is All You Need

<https://ar5iv.labs.arxiv.org/html/1706.03762>

[\[15\]](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#:~:text=Alternatively%2C%20we%20can%20truncate%20the,simpler%20and%20more%20stable%20models) 9.7. Backpropagation Through Time - Dive into Deep Learning 1.0.3 documentation

<https://d2l.ai/chapter_recurrent-neural-networks/bptt.html>

[\[25\]](https://arxiv.org/abs/2005.14165#:~:text=approaches.%20Specifically%2C%20we%20train%20GPT,At%20the%20same%20time%2C%20we) [\[26\]](https://arxiv.org/abs/2005.14165#:~:text=language%20models%20greatly%20improves%20task,several%20tasks%20that%20require%20on) [\[51\]](https://arxiv.org/abs/2005.14165#:~:text=demonstrations%20specified%20purely%20via%20text,evaluators%20have%20difficulty%20distinguishing%20from) \[2005.14165\] Language Models are Few-Shot Learners

<https://arxiv.org/abs/2005.14165>

[\[29\]](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves) \[2001.08361\] Scaling Laws for Neural Language Models

<https://arxiv.org/abs/2001.08361>

[\[31\]](https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/#:~:text=Explainer%20cset,and%20training%20data%20scale%20up) Emergent Abilities in Large Language Models: An Explainer

<https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/>

[\[32\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%20,LLM%20training%20data%20remaining%20static) [\[33\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=LLMs%27%20knowledge,to%20two%20key%20issues) [\[44\]](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=,called%20hallucination) Retrieval augmented generation: Keeping LLMs relevant and current - Stack Overflow

<https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/>

[\[34\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=decision,and%20coordinate%20with%20other%20agents) [\[35\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=Overview%20of%20LLM,to%20the%20LLM%20prompt%20through) [\[36\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,building%20robust%2C%20extensible%2C%20and%20intelligent) [\[48\]](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf#:~:text=To%20perform%20complex%20tasks%20reliably%2C,and%20coordinate%20with%20other%20agents) AWS Prescriptive Guidance - Agentic AI patterns and workflows on AWS

<https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-patterns/agentic-ai-patterns.pdf>

[\[37\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=experiences,after%20a%20minor%20prompt%20change) [\[38\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1.%20Latency%2C%20cost%2C%20and%20third,dependencies) [\[39\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=Engineering%20teams%20need%20more%20than,built%20to%20handle%20AI%20workloads) [\[40\]](https://www.honeycomb.io/resources/getting-started/what-is-llm-observability#:~:text=1,and%20scoring%20of%20LLM%20responses) What Is LLM Observability and Monitoring? | Honeycomb

<https://www.honeycomb.io/resources/getting-started/what-is-llm-observability>

[\[41\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,the%20entire%20history%20every%20time) [\[42\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=,logic%20and%20cache%20invalidation%20strategy) [\[43\]](https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems#:~:text=%2A%20Model%20right,com) FinOps in the Age of AI: A CPO's Guide to LLM Workflows, RAG, AI Agents, and Agentic Systems

<https://www.finout.io/blog/finops-in-the-age-of-ai-a-cpos-guide-to-llm-workflows-rag-ai-agents-and-agentic-systems>

[\[49\]](https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025#:~:text=Each%20integration%20serves%20production%20AI,evaluation%20metrics%2C%20production%20alerting%2C) Top 10 LLM observability tools: Complete guide for 2025 - Braintrust

<https://www.braintrust.dev/articles/top-10-llm-observability-tools-2025>