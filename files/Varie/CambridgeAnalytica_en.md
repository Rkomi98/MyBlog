# From Cambridge Analytica to chatbots: how much is our privacy at risk?

## Abstract

The **Cambridge Analytica (CA)** scandal revealed how psychometric profiling can transform social media data into powerful tools for political micro-targeting. Since 2014, [CA collected Facebook data ([1])](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf) from tens of millions of people through a quiz ("thisisyourdigitallife"), obtaining [**OCEAN profiles** ([2])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the) (the "Big Five" personality traits) and matching them with demographic and consumer information. These **psychometric profiles** were used to segment the electorate and experiment with targeted political messages: e.g., by modulating [pro-gun ads based on the "neuroticism" trait ([3])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). CA propagated content on **Facebook** (via **Custom Audiences** and similar tools), testing ad variants with **A/B testing** techniques to maximize impact.

Okay, but what was the impact of all this?

Evidence on real effectiveness is mixed: [CA itself ([4])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups) reported increases of **39%** in awareness on certain issues and a **+30%** in turnout for targeted groups in 2014 US campaigns.
However, other independent analyses [\[5\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=In%20an%20email%20to%20me%2C,like%20race%2C%20age%2C%20and%20gender)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed) noted that CA's predictive model **did not significantly outperform normal demographic criteria**.

Today, similar logics of **intensive user data collection and use** are found in major **chatbot and LLM** (Large Language Model) services. Consumer platforms like **ChatGPT** (OpenAI, [\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out)) and **Gemini** (Google) by default **record user prompts, conversations, and feedback** and use them to continuously improve models [\[8\]](https://support.google.com/gemini/answer/13594961?hl=en). Private users can "opt-out" (i.e., refuse to share their data) by limiting the sharing of their conversation history, but in the absence of such a choice, chat data can be stored for extended periods (e.g., **OpenAI** keeps general user chats indefinitely to train models, unless opted out, while **Anthropic** from 2025 offers the choice: no training and 30-day retention or active training with **5-year** retention [\[9\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention)). In contrast, **business/enterprise** services offer **isolation** guarantees: for example, **OpenAI API/Enterprise** and **Microsoft 365 Copilot** ensure that inputs and outputs **do not feed into the training data** of public models [\[10\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform)[\[11\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important), remaining confined to the client's environment. On the **transparency and governance** front, companies have introduced **controls for users and administrators** (such as privacy dashboards, retention settings, opt-out forms for the EU [\[12\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Beginning%20this%20week%2C%20people%20based,well%20as%20newly%20submitted%20ones)[\[13\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes)) and adopted contractual commitments (e.g., **SOC 2**, **DPA** on data processing) to reassure businesses and regulators [OpenAI \[14\]](https://openai.com/enterprise-privacy/#:~:text=Comprehensive%20compliance)[Google \[15\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). However, concerns remain: for example, the **DeepSeek** chatbot, a popular Chinese app, collects **every input, file, and chat history**, sending everything to servers in China [\[16\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D)[\[17\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D); this raises security doubts and has attracted the attention of experts for potential risks of government access [\[18\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so).

> To access this article, a Wired premium account is required.

In summary, from the Cambridge Analytica era to today's AI chatbot boom, user **personal and behavioral data** have become the "**fuel for predictive and generative models**". While this enables "smarter" services and tailored campaigns, it also poses new challenges for **privacy, control, and accountability**. Platforms are responding with greater transparency (dedicated policies) and control options, but it is also up to users, and especially regulators, to demand **clarity on the use of their data**, exercise opt-out/off rights, and carefully evaluate what to share with these AIs.

## Cambridge Analytica: what happened?
Now let's look in detail at what happened with the Cambridge Analytica case. To do this, I will use the first two sources we saw in the article [1](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf),[2](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the)

### Data collected
Cambridge Analytica (an affiliate of **SCL Elections**) acquired a massive dataset of Facebook users in 2014-2015 through the company **GSR** (Global Science Research) of researcher Aleksandr Kogan. Kogan developed a personality quiz app ("thisisyourdigitallife") leveraging Facebook's Graph API, which at the time allowed extracting not only the data of the consenting user but also those of their **friends** (_friends permissions_ functionality valid until 2014). Approximately **320,000** Facebook users, mainly Americans, completed the OCEAN test by logging in via Facebook; in exchange for a few dollars, they gave the app permission to read a wide range of information: public profile (name, gender), date of birth, current city, **"Liked" pages**, wall posts, friends list, even private messages and tagged photos.

Since the app inherited the access rights of Kogan's previous academic app (developed before Facebook's 2015 restrictions), it could also collect data from participants' friends, if the latter had not set their privacy differently. **In total, ~87 million people** (including over **1 million** in the UK) underwent this massive data collection without their knowledge. Facebook confirmed the estimate and published the list of involved countries in 2018. Approximately **30 million** individuals had both Facebook data and psychometric quiz results matched, forming the core for predictive analyses.

### Psychometric characteristics (OCEAN model)

The Big Five personality model, called **OCEAN** (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), was at the heart of CA's profiling [\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the). Kogan and colleagues applied methodologies from the Cambridge Psychometrics Centre (known for the "MyPersonality" project) which demonstrated how **Facebook Likes** could predict OCEAN traits and other personal attributes with surprising accuracy [\[19\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=OCEAN%20model%20and%20pioneer%20the,as%20ethnicity%20and%20political%20affiliation)[\[20\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that).

In the contract stipulated with SCL on June 4, 2014, Kogan declared that his techniques allowed achieving predictivity "**close to test-retest**" in personality scores, with correlations such that an algorithm based on likes was _more accurate_ in describing a person than knowing their friends or even family [\[21\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Nix%20told%20us%3A%20%E2%80%9CWe%20do,41)[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the).

> This statement echoed a 2015 academic study [[22](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that)], where Michal Kosinski, Kogan's colleague, showed that 70 Facebook likes outperformed friends in outlining an individual's psychological profile.

In practice, CA had estimated OCEAN scores for millions of US voters, obtained directly from the quiz or inferred through models trained on Kogan's data.

Since it might seem complicated now, let's take [an example](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). A user with a high "Neuroticism" score and low "Openness" was identified as **sensitive to messages of fear and order**, while a highly "Extroverted" person might respond better to optimistic and social content. Cambridge Analytica **clustered** the public into psychographic groups and identified key themes for each segment: according to former CEO Alexander Nix, "presenting a fact supported by an emotion" was the strategy, adapting the argument to the audience's emotional profile [\[23\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=We%20are%20trying%20to%20make,109).

### Predictive models and ML employed

While not publishing technical details of its algorithms, CA combined **machine learning** approaches with traditional analytics. Kogan in [an email](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/) explained that his model for CA operated similarly to Netflix's recommendation system, i.e., via **SVD/factor analysis**: reducing a user-like matrix to latent components, which incorporated personality, demographics, and social networks together. In essence, the algorithm did not "openly" isolate the 5 traits but mixed them with dozens of other variables (age, gender, political orientation, etc.) into correlated factors useful for predicting electoral behavior.

As we did in the previous section, let's take an example. CA used _regressions_ and _decision trees_ to estimate the probability that an individual would support certain causes or candidates, given their known psychographic and demographic characteristics.

CA's data scientists built models to identify so-called **persuadables**, i.e., undecided voters strongly influenced by specific emotional levers. The volume of data (likes, tests, online and offline behaviors) also allowed the use of **shallow neural networks** or multivariate classification models to associate profiles with _outcomes_ of interest (vote, donation, abstention). **Facebook's look-alike models** played an important role: CA could upload lists of known users (e.g., individuals with high "openness" scores identified by the quiz) and use Facebook's algorithm to find other similar users, expanding the targeting reach.

**Dataâ†’profileâ†’targeting pipeline**

![Cambridge Analytica data pipeline diagram](../Assets/ca-data-pipeline.svg)

_Figure:_ the numbering follows the phases by which CA transforms social and commercial data into micro-targeted messages and continuously refines models.

Firstly, CA (via GSR) [**extracts raw data**](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to) from social media (Facebook) and combines it with other sources (e.g., public **electoral rolls**, data from commercial brokers such as purchases and subscriptions).
This data feeds into **psychometric analysis**: individual OCEAN traits are calculated from the quiz and likes, which are then **aggregated and inserted** into a unique profile for each voter (including age, gender, location, political leanings, etc.). On this basis, CA's data analysts developed **predictive models** to segment the population into key groups (e.g., "insecure neurotics", "open progressives") and predict for each the susceptibility to specific messages.

In parallel, another team develops **message variants** (memes, videos, slogans) tailored to the psychological insights of the segments: for example, the same theme (such as gun sales) is packaged in a _fear and protection_ version for individuals with high neuroticism, and in a _sporting hobby_ version for open/calm subjects.

Messages are then **sent via micro-targeting**: CA uploaded target user lists to Facebook (identified by name/ID, email, or phone) and used tools like **Custom Audience** and **Dark Posts** to show different ads to different groups, without them being publicly visible to others. This phase included **A/B tests** and effectiveness checks: clicks, shares, viewing time, and conversion rates (e.g., event registration, donation) were monitored for each variant, then iterating on the winning creative. Finally, campaign results (actual engagement, changes in internal polls) were **fed back** into the process: user reactions served to further refine persuasion models, in a continuous optimization cycle.

### Delivery channels and A/B experimentation

The main vehicle for CA's messages was **Facebook**. The company created targeted ads using Facebook's advertising system, which allowed directing ads to very specific demographic and psychographic clusters (by geographic area, interests, similar to a provided list). CA also leveraged Canadian partner **AggregateIQ** for campaigns on other platforms and the **display network** (e.g., targeted web banners): AIQ managed advertising spending for [pro-Brexit](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=sharing%20of%20data%20in%20the,overseas%20elections%20in%20Chapter%206) and [pro-Trump](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter) groups, using data and segments provided by CA. Furthermore, CA did not disdain traditional methods: in some cases, it provided _scripts_ for **telemarketing** or for political volunteers, calibrated to the profile of the voter to be contacted (e.g., emphasizing immigration when speaking with a "closed" and fearful subject, vs. economy with an "open" cosmopolitan).

**A/B experiments** were central: in practice, **hundreds of ad variants** were tested simultaneously, changing, for example, color, emotional tone, call-to-action, and measuring which version had the highest click-through or conversion rate in each segment.

A famous example reported by Wylie is the ["Defeat Crooked Hillary"](https://edition.cnn.com/2018/03/21/opinions/trump-cambridge-analytica-clinton-slogan-opinion-psaki) campaign in which CA allegedly tested dozens of anti-Clinton messages (from the most moderate to the most conspiratorial) to understand which resonated with doubtful voter groups, then bombarding them with the optimized message. Facebook at that time **did not effectively track** these _dark ads_ nor limit their extreme personalization, which allowed CA to conduct a kind of propaganda laboratory invisible to the public and the victims themselves.

### Integration with external data
In addition to Facebook data, CA had a mosaic of other information. In the USA, it leveraged state **electoral registers** (containing voting history, party affiliation, etc.) and [cross-referenced them with commercial datasets](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to) (from brokers like Acxiom, Experian) on purchasing habits, type of car owned, magazines read, charitable donations...

This **hybrid strategy** (social media + offline data) allowed identifying unprecedented correlations. Another tool was **Lookalike Audiences**: starting from an audience whose orientation CA knew (e.g., people profiled as _pro-Trump_), Facebook's algorithm found other users with similar characteristics to extend the campaign to.

### Outcomes and implications
In 2018, the Cambridge Analytica scandal, which emerged thanks to investigations (_The Guardian_, _NY Times_) and the action of the UK Parliament, led to the closure of CA/SCL and a symbolic fine for Facebook ([Â£500k from the ICO UK](https://www.bbc.com/news/technology-45976300)) for failing to protect data.

The main legacy has been a global awareness of the risks of personal data abuse for **mass manipulation**. The sophisticated tools developed by CA (psychological profiling + social media targeting) were not entirely new in themselves, but CA pushed their use beyond ethical limits, operating without transparency or informed consent from those affected.

The implications? Several!

The episode accelerated several reforms: Facebook in 2018-19 further restricted APIs and made political ads more controllable, while in the regulatory sphere, cases like this contributed to the drafting of guidelines (e.g., GDPR in the EU already provides for the right to object to profiling and automated decisions since 2018, although it was not invoked at the time).

### Do today's chatbots collect data?

The question naturally arises.

With the advent of ChatGPT and similar (2022+), tech companies have applied a similar paradigm: collect as many **user interactions** as possible to train and refine generative artificial intelligence models. In this second part, we analyze the policies of the main LLM providers (OpenAI, Google, Anthropic, Microsoft, Meta, DeepSeek), focusing on how they manage user data (types collected, purposes, retention periods, training, controls, legal basis) and what **transparency and governance** mechanisms they offer.

## Chatbots and LLMs: data collected, uses, and comparison between platforms (2023-2025)

**Conversational generative AI** platforms have different business and user models (consumer vs. enterprise), but they show common trends: **recording user requests (prompts)**, monitoring conversations (logs), and leveraging such data to **continuously improve models through "continuous training"** (we will clarify this later).

Below, we outline for each key vendor the current status (as of 2025) on data collection, use for training, retention, human review, controls offered, and legal basis, with a summary comparative overview.

### OpenAI - ChatGPT
When we talk about OpenAI, we are obviously talking about GPT models and its flagship product and chatbot: ChatGPT.
#### Data collected
OpenAI [collects every input](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance) provided by users to its models, both through the **ChatGPT interface** (chat conversations, uploaded attachments) and via **API**. In detail, **chats** on chat.openai.com include user prompts and AI responses; if the user provides files (e.g., when using plugins or through the image/PDF upload function), that content is also acquired. **Explicit feedback** (ðŸ‘/ðŸ‘Ž votes on responses, reports) and **technical metadata** (timestamp, IP address, device/browser type) are also recorded for security and performance monitoring purposes.

OpenAI does not collect biometric or sensor data because the service is textual/visual/voice. Regarding the latter, using ChatGPT's voice function implies that user audio is transcribed and processed (using models like Whisper).

In addition to data directly provided by the user, OpenAI may collect **usage telemetry** (e.g., session frequency, prompt length, error rates) and applies cookies/tracking on the ChatGPT website as per its general Privacy Policy.

#### Purposes and legal basis

OpenAI **does not use user data for marketing/personalized advertising purposes**, a difference from classic social networks: prompts are not used to profile the user to sell ads, but to _profile the model_ itself, so to speak.

From a GDPR perspective, OpenAI (after the events with the Italian Garante in 2023) introduced an optional consent form for the use of data for training purposes, but continues to rely on a combination of legal bases: the performance of a contract for response services, and legitimate interest for model improvement, while still offering **opt-out** (right to object) to data subjects.

#### Use for model training purposes
By _default_, for **consumer users** (ChatGPT free and Plus), chat content **is used to train and improve OpenAI models**, **unless explicitly opted out**.

OpenAI explains that ChatGPT "improves by training on the conversations people have with it." This form of _training_ includes both use in supervised fine-tuning/RLHF sets (e.g., OpenAI specialists review chat samples to create training data with human feedback, or use user ðŸ‘/ðŸ‘Ž ratings as a reward signal in Reinforcement Learning) and direct use in _datasets_ for future model versions. In **March 2023**, OpenAI changed its API policies, announcing that by default it would _no longer_ use customer API call data to train models, unless opted in. This was to reassure businesses and developers. Thus, for **business/API services**, the setting is opposite: **no use of user data for training**, unless the organization voluntarily chooses to share it (e.g., by specifically sending prompt examples via _Playground_ with an opt-in flag). In September 2023, OpenAI launched **ChatGPT Enterprise**, clarifying that all enterprise user conversations "will never enter the training of our models" by default.

#### Data retention
OpenAI's retention policies distinguish by service and settings:
- For ChatGPT conversations _with_ active history (consumer default): data is stored in OpenAI's systems indefinitely, unless otherwise communicated. OpenAI does not explicitly state an expiration date â€“ in fact, the goal is to accumulate a long history for training future models.

- For ChatGPT conversations _with_ history _disabled_ (opt-out training): OpenAI states that such data "will not be used to train models" and will be **stored only for 30 days** for abuse monitoring purposes, after which it will be deleted. This ~30-day window serves to potentially inspect content if problems arise (e.g., a user generating many illegal requests).

- For [**standard API**](https://openai.com/index/response-to-nyt-data-demands/) uses: similarly, request and response _logs_ are kept for a maximum of **30 days** for security/abuse reasons, then deleted. This was a change introduced in late March 2023: previously, API data could be stored longer. In addition, OpenAI offers enterprise customers a "**Zero Data Retention**" option, meaning no content from requests is stored at all (if the customer opts for this mode, requests are processed and immediately discarded, retaining only aggregated metrics).

- For **ChatGPT Enterprise**: OpenAI provides administrators with control over the retention period of their corporate users' conversations (up to being able to choose [_zero retention_](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law)). In the Enterprise documentation (updated June 2025), it states: "You control how long your data is retained (ChatGPT Enterprise)". This implies that a company can set, for example, auto-deletion of chats after X days. If not configured otherwise, enterprise data should follow the contractual default retention, which is 30 days in API endpoints and potentially more extended for the enterprise interface if the user allows it (but always excluded from training).

- **Legal exceptions**: as for all, any data subject to legal obligations (e.g., preservation order from authorities, or data necessary for legal disputes) may be kept beyond the terms above.

In summary, the average ChatGPT user who does not touch the settings accepts indefinite retention for training purposes. The aware user can reduce the impact by disabling history (reducing retention to 30 days and no training). Businesses and developers, on the other hand, have short retention by design and no training, with the option to further tighten to zero logs.

#### Human review of data

OpenAI employs [manual review](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance) on a _small percentage_ of conversations, both to improve the model (e.g., labeling difficult conversations to teach better responses) and for moderation (analysts checking if the model has violated policies). The documentation explains that **OpenAI staff can access and view** a sample of user content, with tools to obfuscate personal information where possible.

Data intended for training is **pseudonymized**: OpenAI [states](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance) that it _removes or aggregates as much personal information as possible_ before using the data in training datasets, to reduce the risk of identity reconstruction. For example, names, emails, numbers could be automatically masked. However, this is not an absolute guarantee of anonymity, as various entities have noted (the Italian Garante has asked for better implementation of these measures). OpenAI's human review also applies to **feedback**: if a user reports an output as inappropriate, a member of the security team may read that entire conversation.

> Important: for **API/enterprise users**, OpenAI states that it _does not_ involve humans in the contained requests, unless the client itself shares them via feedback. Thus, corporate data remains confidential (also because it often contains trade secrets); in ChatGPT Enterprise, conversation logs are not visible to OpenAI personnel or used for training, except in exceptional abuse situations.

### Gemini - Google

#### Data collected

Google collects a wide range of data through its conversational AI services, which in 2023 evolved from _Google Bard_ to a unified suite under the name **Gemini**. Google's **Gemini Privacy Hub** details the [categories](https://support.google.com/gemini/answer/13594961?hl=en#):

- **User-provided content:** everything the user _says or enters_ in interaction with the AI. This includes **textual** or voice **prompts**, any uploaded files (images, documents for model analysis), data shared through "_connected apps_" functions (e.g., if the user asks Gemini to read a web page or an email, the content of that page/email is acquired). If the user uses the AI in vocal/video mode (e.g., _Gemini Live_), the **transcriptions and audio/video recordings** of that interaction are collected.

- **AI-generated content:** Google also records the **responses generated** by the model (text, code, created images, audio, video). So the entire conversation (prompt + output) is stored. Even _chat summaries_ or _public links_ created with Gemini are considered generated content and tracked.

- **App and device data:** every interaction with Gemini is accompanied by **telemetry** data. Google collects information about **connected apps** and integrated Google services (e.g., if the user has enabled the use of data from YouTube, the system records which services have been consulted). It then collects **device** details (model, OS, app version), browser and local settings, as well as unique identifiers and the **IP address**. In addition, interaction metrics are logged: response times, any crashes.

- **Permissions and device context (mobile):** if the user uses the Gemini mobile app (or Google app with assistant), Google may collect additional data: address book/contacts (if "help you keep in touch" functionality is active), call and message logs (to respond to prompts like "when did X call me?"), list of installed apps, screen content (if "overlay" is enabled to ask questions about the current screen). This is potentially very sensitive information, managed with Android/iOS permissions, but if granted, it becomes input to the model