# AI nel banking: tra DPIA e AI Act

## Abstract

Il settore bancario sta adottando l'IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarendo quando scattano DPIA e FRIA e quali obblighi operativi ne derivano. L'obiettivo è offrire una mappa pratica per valutare i rischi, distinguere i casi ad alto rischio e impostare una governance coerente.

## Introduzione

L'adozione dell'IA in banca non è piu' sperimentale: è già parte di processi che toccano diritti, accesso al credito e tutela dei dati. Prima di addentrarci nelle regole, serve un quadro introduttivo che colleghi i principali casi d'uso ai due pilastri regolatori (GDPR e AI Act) e alle valutazioni di impatto richieste. Questo articolo fa da ponte tra tecnologia e compliance, per orientare fin da subito le scelte di progetto.

## Cosa dicono le regole e come muoversi

### Principali evidenze

Le nuove regole europee sull'AI (Reg. UE 2024/1689, "AI Act") classificano come _"sistemi ad alto rischio"_ diversi impieghi dell'Intelligenza Artificiale (IA) nel settore bancario, imponendo requisiti stringenti[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). 

In particolare, l'uso dell'AI per valutare l'affidabilità creditizia (credit scoring) di persone fisiche rientra nell'Allegato III ed è quindi considerato _high-risk_[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Analogamente, i sistemi IA impiegati nel processo di **reclutamento e gestione del personale** (es. selezione candidati, decisioni di assunzione o promozione) sono esplicitamente elencati tra gli usi ad alto rischio, dato il potenziale impatto sui diritti dei lavoratori e il rischio di discriminazioni[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati). 

Altri casi d'uso tipici in banca - come sistemi di **chatbot o assistenti virtuali per clienti**, strumenti di **anti-riciclaggio (AML) e rilevazione frodi**, sistemi di **verifica identità (KYC)** con riconoscimento biometrico, ecc. - pur non tutti ricadendo in categorie "alto rischio" per l'AI Act, comportano comunque significativi obblighi di conformità. In particolare, l'AI Act esenta dal novero _high-risk_ i sistemi IA usati per individuare frodi finanziarie o per calcoli patrimoniali prudenziali[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20previsti,ad%20alto%20rischio%20ai%20sensi); ciò significa che, paradossalmente, algoritmi di **fraud detection** o di **monitoraggio transazioni sospette AML** non sono soggetti ai requisiti di alto rischio dell'AI Act, pur dovendo rispettare la normativa di settore (es. antiriciclaggio) e privacy. 

> Resta fermo, in ogni caso, l'obbligo per le banche di effettuare una valutazione caso-per-caso: ad esempio, un sistema di IA bancario che, pur non rientrando espressamente nell'Allegato III, presenti rischi significativi per diritti, sicurezza o accesso a servizi essenziali potrebbe essere prudentemente trattato come _alto rischio_ ai fini interni.

Le fonti regolamentari primarie delineano una serie di **checklist obbligatorie** e buone pratiche applicabili. Sul versante privacy, il GDPR impone la **Data Protection Impact Assessment (DPIA)** per trattamenti con rischio elevato, e le autorità (EDPB e Garante Privacy) hanno elencato criteri e casi tipici: ad esempio, profilazione o scoring su larga scala, decisioni automatizzate con effetti significativi (come concessione di un prestito), monitoraggio sistematico di utenti, uso di dati sensibili o di tecnologie innovative come l'IA[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). 

Molti use case di IA bancari soddisfano tali criteri come per esempio:
- credit scoring = profilazione finanziaria con impatto su diritti;
- monitoraggio transazioni = controllo sistematico;
- riconoscimento facciale = dato biometrico;

> _Questi richiedono quindi una valutazione preventiva d'impatto sulla privacy (DPIA)_ 

In parallelo, l'AI Act introduce per gli utilizzatori (_deployers_) di sistemi AI ad alto rischio l'obbligo di condurre una **Valutazione d'Impatto sui Diritti Fondamentali (FRIA)** prima della messa in uso[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista). 

In ambito bancario ciò riguarda in particolare gli enti che impiegano sistemi di AI Act Annex III, ad esempio le banche che usano IA per credit scoring (Allegato III §5(b)) o per decisioni su polizze vita/sanitarie. Tale FRIA deve considerare contesto d'uso, categorie di interessati coinvolti, rischi per diritti (es. bias, esclusione), misure di controllo umano previste e azioni di rimedio[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta). Va notificata all'autorità di vigilanza di mercato competente con i relativi risultati[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=obbligo%20di%20notifica), inserendosi quindi come adempimento formale prima dell'uso del sistema.

Un tema chiave emerso è la **sovrapposizione e distinzione** tra DPIA (focalizzata sui rischi privacy GDPR) e FRIA (focalizzata su impatti _etici e diritti fondamentali_ più ampi). In caso di sistemi IA che trattano dati personali, il legislatore prevede che la FRIA _si integri_ con la DPIA già svolta, evitando duplicazioni: in pratica, la **DPIA copre privacy e sicurezza dati**, mentre la **FRIA estende l'analisi a discriminazione, accesso equo a servizi, trasparenza verso gli interessati**, ecc. 

Ad esempio, per un algoritmo di concessione prestiti, la DPIA valuterà liceità e proporzionalità del trattamento dati, minimizzazione, misure di sicurezza e anonimizzazione; la FRIA aggiungerà la valutazione dei rischi di _bias algoritmico_, impatto socio-economico (es. esclusione finanziaria di gruppi vulnerabili) e l'adeguatezza delle misure di **human oversight** adottate. Quest'ultimo punto, la supervisione umana, ricorre come principio basilare: tanto il GDPR (art.22) quanto l'AI Act e le linee guida di settore insistono sulla necessità che l'IA _non prenda decisioni completamente autonome senza possibilità di intervento umano_. 

Le **Linee Guida EBA 2020 sul governo del credito** hanno chiarito che l'uso di modelli automatizzati è consentito solo entro confini precisi, includendo _trasparenza, tracciabilità, supervisione costante ed evitando ogni forma di discriminazione_[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,supervisione%20costante). 

Allo stesso modo, Banca d'Italia ha ribadito che l'IA può supportare la valutazione del rischio creditizio _ma non sostituire il giudizio umano_: la banca rimane responsabile ultima di correttezza e legittimità delle decisioni[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=La%20Banca%20d'Italia,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). 

**L'imprescindibilità del controllo umano** è sancita anche a livello di principi internazionali: vi è ampia convergenza sull'importanza di mantenere un intervento umano significativo ("human-in-the-loop") nei processi decisionali automatizzati, per garantire accountability e tutela dei diritti[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in). Nella pratica bancaria ciò si traduce, ad esempio, nel prevedere che le decisioni più impattanti (es. rifiuto di credito, segnalazione di operazione sospetta) siano riesaminate o avallate da personale competente, e che gli operatori addetti abbiano _formazione adeguata sull'IA_ e potere di fermare o correggere il sistema[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la). 

I grandi istituti si stanno infatti orientando verso modelli "ibridi" in cui l'IA elabora raccomandazioni ma l'uomo ha l'ultima parola, investendo in programmi di **upskilling** del personale per colmare il gap di competenze tecniche[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent). Ciò risponde anche a esigenze di gestione del rischio: come osservato dall'EBA, molte banche adottano un approccio graduale e prudente all'AI, introducendo solide **misure di controllo e "guardrails"** prima di estendere l'uso di modelli avanzati, specie di tipo generativo, e testando i casi d'uso più rischiosi solo dopo aver maturato sufficiente esperienza e confidenza nella tecnologia[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences).

Dal punto di vista strettamente normativo, emergono _nuovi obblighi operativi_: ad esempio requisiti di **trasparenza verso gli utenti**. Il regolamento AI Act impone (anche per sistemi non high-risk) che chi interagisce con un'IA sia informato del fatto che sta interagendo con una macchina e non un essere umano[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Ciò significa che in un contact center automatizzato o chatbot bancario dev'essere chiaramente segnalato al cliente che il servizio è erogato da un sistema AI, e occorre prevedere canali di _escalation a un operatore umano_ su richiesta.

Analogamente, se lo strumento di GenAI (Generative Artificial Intelligence) genera contenuti (es. una risposta testuale in linguaggio naturale al cliente), vanno forniti eventuali disclaimer sull'origine automatica e accuratezza delle informazioni. Sul fronte **non-discriminazione e fairness**, sebbene la normativa bancaria italiana contenga solo generiche clausole di equità nell'erogazione del credito, il nuovo quadro richiede una grande attenzione: l'AI Act inserisce esplicitamente il divieto di algoritmi che introducano classificazioni basate su caratteristiche sensibili (pratiche assimilabili al _social scoring_ sono vietate) e, come detto, include il credito tra i casi ad alto rischio proprio per i possibili _bias algoritmici_[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). I garanti privacy e le autorità di consumo potranno sindacare pratiche scorrette se un modello nega sistematicamente servizi a categorie protette. 

Pertanto, le banche devono implementare tecniche di **AI governance**: test proattivi dei modelli per rilevare disparità di trattamento (bias audit), documentazione trasparente delle variabili usate (feature importance), e meccanismi di reclamo efficaci per gli interessati. Un cliente ha diritto di sapere se una decisione sul suo finanziamento è stata influenzata da un algoritmo e su quali parametri[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori), nonché di ottenere intervento umano e contestare la decisione automatizzata - come previsto dal [GDPR art.22](https://www.privacy-regulation.eu/it/22.htm).

### Principali ambiguità normative
Nonostante i progressi regolamentari, permangono aree grigie e nodi interpretativi. Un primo elemento di incertezza riguarda la _portata esatta delle categorie di alto rischio_: alcuni use case bancari non rientrano in modo netto nell'Allegato III. 

Ad esempio, l'utilizzo di IA per **Anti-Money Laundering** (rilevazione di operazioni sospette) non è elencato tra gli high-risk a meno che avvenga direttamente da autorità di contrasto. Infatti il regolamento europeo chiarisce che i sistemi di IA usati in procedimenti amministrativi da autorità fiscali e doganali, e dalle Unita’ di Informazione Finanziaria (FIU) per analisi antiriciclaggio, non vanno trattati come sistemi di IA ad alto rischio impiegati dalle forze dell’ordine. Resta pero’ fermo che l’uso dell’IA non deve creare disuguaglianze o ostacolare il diritto alla difesa, soprattutto quando le decisioni sono difficili da contestare per mancanza di trasparenza.[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo). 

Questo solleva dubbi: un algoritmo che _blocca preventivamente_ transazioni o conti correnti potrebbe incidere su diritti fondamentali (es. libertà economica) quasi quanto un sistema di credit scoring, ma formalmente l'AI Act non lo copre come high-risk. La scelta sembra motivata dal voler favorire l'innovazione anti-frode, ma resta ambigua la linea di confine: le banche dovranno decidere se trattare questi sistemi "non classificati" comunque con un approccio conservativo (applicando volontariamente requisiti affini a quelli high-risk) per prudenza e accountability.

Ulteriore ambiguità concerne la **definizione di "rischio significativo"** nell'AI Act. 

La norma prevede infatti che i sistemi elencati in Allegato III _non_ siano considerati ad alto rischio se, "in deroga", _non presentano un rischio significativo_ per salute, sicurezza o diritti[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,%20risultato%20del%20processo%20decisionale). Questa clausola di esenzione è di non facile applicazione pratica: ad esempio, una piccola soluzione AI usata in via sperimentale su pochi clienti, pur tecnicamente rientrando in una categoria (mettiamo credit scoring), potrebbe essere sostenuta come a rischio trascurabile dal fornitore; tuttavia i criteri per stabilirlo non sono esplicitati e c'è il rischio di interpretazioni difformi. Le aziende potrebbero essere restie a "declassare" un sistema da high-risk a non, temendo contestazioni a posteriori. Si profila quindi un atteggiamento prudenziale, ma la mancanza di linee guida attuative al riguardo è un gap che richiederà chiarimenti (la Commissione è delegata a emettere atti per modificare l'Allegato III e fornire criteri, ma occorrerà vedere come verrà gestito).

Un terzo profilo di incertezza riguarda la **metodologia e governance della FRIA**. Trattandosi di un adempimento nuovo, non esistono ancora standard consolidati su _come condurre una valutazione di impatto etico/fondamentale_. L'AI Act prevede che l'_AI Office_ (nuovo organismo europeo) fornisca un modello di questionario anche via tool automatizzato per facilitare i deployer[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=L'ufficio%20per%20l'IA%20elabora%20un%20modello%20di%20questionario,%20a%20norma%20del%20presente%20articolo%20in%20modo%20semplificato.), ma fino a che ciò non sarà sviluppato, le imprese dovranno arrangiarsi ispirandosi a linee guida analoghe (es. quelle del Garante per valutazioni etiche, o framework come l'Assessment List for Trustworthy AI dell'UE). 

>Quali competenze dovranno coinvolgere? Chi sarà l'"autorità di notifica" della FRIA in Italia per il settore bancario - il Ministero dello Sviluppo Economico, Banca d'Italia o un nuovo organo?

Tutto questo non è ancora definito con precisione, e ciò crea ambiguità operative. 

Ambiguità anche nella **distinzione di ruoli e responsabilità** lungo la filiera AI. In molti casi le banche utilizzano soluzioni di IA fornite da vendor terzi o basate su modelli generativi pre-addestrati (es. un foundation model linguistico integrato nel chatbot). Il regolamento distingue **"fornitore"** (chi immette sul mercato il sistema AI) e **"utilizzatore (deployer)"** finale; nel caso bancario, però, una banca che sviluppi internamente un algoritmo per uso proprio potrebbe essere considerata sia fornitore che deployer, con obblighi cumulativi (inclusa la conformità tecnica e marcatura CE del sistema high-risk). Se invece acquista un servizio AI esterno, dovrà comunque garantire gli obblighi dei deployer (FRIA, registrazione nel database UE, sorveglianza d'uso) ma dipende dal fornitore per la documentazione tecnica conforme. È incerto come gestire contrattualmente questa condivisione di responsabilità: le banche dovranno pretendere dai vendor garanzie di conformità AI Act (es. **EU Declaration of Conformity** per sistemi high-risk) e accesso alle info sul modello per poter fare la FRIA.

Un'altra area grigia è la **gestione della spiegabilità ed esercizio dei diritti GDPR** in presenza di modelli AI opachi (es. deep learning). Il GDPR dà all'interessato diritto ad avere spiegazioni significative sulla logica di decisioni automatizzate; tuttavia, le tecniche di explainable AI sono ancora emergenti e potrebbe non essere possibile fornire spiegazioni semplici di modelli complessi. Le banche dovranno bilanciare questo obbligo con la tutela del segreto industriale sui propri algoritmi. Non esiste ancora un consenso su quale livello di trasparenza sia "sufficiente", ambito in cui sono attesi orientamenti dal [EDPB o dal futuro AI Office](https://www.edpb.europa.eu/our-work-tools/our-documents/letters/edpb-reply-letter-ai-office-edpb-statement-role-data_en).

Infine, permane incertezza su **come tradurre in prassi concrete i principi etici condivisi**. La letteratura e le autorità convergono su principi come _non-discriminazione, trasparenza, oversight umano_[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in), ma, come notato da Banca d'Italia, risulta meno agevole incorporarli in norme vincolanti e procedure operative efficaci[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20internazionale,%20prassi%20concrete%22). Ad esempio, tutti concordano sull'evitare bias, ma definire metriche quantitative di fairness e soglie accettabili di disparità è complesso e lasciato all'autonomia delle imprese per ora. Similmente, è pacifico che debba esserci un intervento umano, ma quanta discrezionalità e in quale fase del processo è adeguato? Sono aspetti su cui si naviga ancora a vista, con approcci conservativi (ad es. richiedere sempre un doppio controllo umano indipendente per certe decisioni critiche) in attesa di prassi consolidate.

### Implicazioni operative per un MVP
Le evidenze sopra delineate informano direttamente i requisiti del prototipo di un possibile tool (i.e. "AI Act Navigator" o "FRIA/DPIA Evidence Builder"). 

In sintesi, l'MVP dovrà: 

 1) incorporare un sistema di _triage dei casi d'uso_ basato su domande mirate che consentano di identificare se un use case ricade in categorie di _alto rischio AI Act_ o presenta trigger DPIA GDPR, guidando l'utente (es. un Compliance officer) nelle classificazioni corrette. 
 2) Dovrà implementare un insieme di **regole decisionali** (business rules) trasparenti: ad esempio, se l'utente indica che il sistema AI effettua valutazione del merito creditizio di clienti retail, il wizard dovrà automaticamente segnalarlo come _AI Act Allegato III - high-risk_ e predisporre gli step successivi (es. elenco requisiti da soddisfare, obbligo FRIA, ecc.)[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Se il caso d'uso comporta trattamento di categorie particolari di dati o profilazione estesa, il tool dovrà suggerire obbligo di DPIA. 
 3) L'output del wizard dovrà includere un _cruscotto di evidenze e obblighi_: ad es. una checklist personalizzata con "Documenti da predisporre" (es. _Scheda descrittiva sistema AI_, _DPIA_, _FRIA_, _registro trattamento_, _contratto con fornitore_…), "Requisiti applicabili" (es. _art. 10 AI Act - data governance, art. 14 - oversight umano_…), "Azioni consigliate" (es. _valutare bias su dataset_, _previsto intervento umano prima decisione definitiva_, _informativa agli interessati da aggiornare_). 
 4) L'MVP dovrà integrare **disclaimer e note esplicative** in ogni sezione critica, per gestire le ambiguità normative: ad esempio una nota che chiarisca "_Se il vostro caso non rientra esattamente nelle categorie AI Act ma presenta rischi potenziali, si consiglia l'approccio più prudente - vedere sezione 'Ambiguità'_". Oppure, in caso di dubbio sulla necessità di DPIA: "_In base alle informazioni fornite, non sussiste obbligo esplicito di DPIA ai sensi art.35 GDPR, ma si raccomanda comunque una valutazione documentata dato l'uso esteso di dati personali (opzione conservativa)_". 
 5) Fondamentale sarà la **tracciabilità e giustificazione** delle raccomandazioni: nella prossima sezione "Evidence table" verrà fornito il razionale (fonte normativa) dietro ogni regola del wizard, aumentando la confidenza dell'utente nelle indicazioni fornite. L'MVP quindi non solo guiderà step-by-step (wizard) ma fungerà anche da _knowledge base_ consultabile, con sezioni "Perché ti chiediamo questo?" o "Perché è richiesto questo documento?" che attingono alle fonti autorevoli (Garante, EBA, normativa) raccolte nella ricerca[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti).

### Evidence Table

| **Tema** | **Evidenza** | **Fonte** | **Implicazione per wizard** | **Confidenza** |
| --- | --- | --- | --- | --- |
| **Use case bancari ad alto rischio (AI Act)** | L'AI Act classifica come _alto rischio_ i sistemi IA usati per valutare affidabilità creditizia di persone (credit scoring) e per impieghi in ambito occupazionale (es. selezione del personale). I sistemi di questo tipo possono infatti incidere significativamente su diritti e tenore di vita degli individui, rischiando di perpetuare discriminazioni[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Sono esclusi invece dall'Allegato III i sistemi IA per rilevare frodi finanziarie o calcolo requisiti patrimoniali (non considerati ad alto rischio)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi). | _Reg. UE 2024/1689 (AI Act)_, consid. 58 e 59[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al); _Paradigma, 2025_[\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano). | Identificare subito se un use case rientra in categorie di Allegato III (es. credito, HR). In tal caso marcare come "High-Risk AI Act" e attivare i moduli di valutazione conformità (FRIA, requisiti art. 8-15 AI Act). Per ambiti esclusi (es. anti-frode) segnalare comunque obblighi settoriali ma con regime AI Act diverso. | **Alta** (testo normativo chiaro; confermato da dottrina) |
| **DPIA - trigger nel settore bancario** | Il GDPR richiede la DPIA per trattamenti ad alto rischio; linee guida WP29/EDPB elencano criteri: tra essi profilazione o scoring su larga scala su situazione economica, decisioni automatizzate con effetti giuridici (es. concessione prestiti), monitoraggio sistematico, uso di dati sensibili o tecnologie nuove (come IA)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). Il Garante ha specificato 12 tipologie obbligatorie, includendo: _"trattamenti valutativi o di scoring su larga scala"_ e _"decisioni automatizzate che incidono significativamente sull'interessato (es. screening clienti di una banca per concessione finanziamento)"_[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20valutazione%20d'impatto:)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). | _Linee Guida WP29 n.248/2017_ (EDPB); _Garante Privacy, Provv. 467/2018_[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). | Nel questionario wizard, domande per rilevare se il caso d'uso coinvolge profilazione finanziaria, decisioni automatizzate su clienti, monitoraggio transazioni, uso di biometria, ecc. - in caso affermativo, far scattare alert "DPIA obbligatoria" e aggiungere il task "Esegui DPIA" nell'output. | **Alta** (linee guida ufficiali EDPB recepite dal Garante) |
| **FRIA - obbligo e contenuti** | L'AI Act (art. 27) obbliga i _deployer_ di sistemi IA ad alto rischio (p.a. e privati che forniscono servizi pubblici, nonché chi usa sistemi di Allegato III punti 5(b) e (c)) a effettuare una **Valutazione d'Impatto sui Diritti Fondamentali** prima dell'uso. La FRIA deve includere: descrizione del contesto d'uso e scopo del sistema, durata e frequenza d'uso, categorie di persone impattate, rischi specifici per diritti (tenendo conto info fornite dal provider ai sensi art.13 AI Act), misure di sorveglianza umana previste, e misure di mitigazione/gestione in caso di problemi (incl. meccanismi di reclamo)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta). L'esito va notificato all'autorità di vigilanza di mercato, usando il modello che sarà predisposto (anche via tool automatizzato dall'AI Office). Se il deployer ha già svolto una DPIA GDPR che copre parte degli aspetti, la FRIA può integrare quella analisi senza duplicarla[\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d%27impatto%20sulla%20protezione%20dei%20dati). | _Reg. UE 2024/1689_, art. 27[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=che%20comprende%20gli%20elementi%20seguenti)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista); Consiglio UE, Comunicato 9/12/23[\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons). | Il wizard deve spiegare chiaramente quando è richiesta la FRIA (es. "Use case classificato High-Risk → obbligatoria FRIA prima della messa in esercizio"). Dovrà guidare l'utente nell'assemblare gli elementi per la FRIA: es. chiedere di descrivere finalità e contesto, elenco interessati impattati, ecc., e produrre uno schema di report. Inoltre, deve ricordare la necessità di _notifica all'autorità_ e fornire eventualmente un template di output conforme (es. modulistica standard AI Office). Deve anche segnalare che se è stata fatta una DPIA, questa va aggiornata/integrata piuttosto che duplicata. | **Alta** (disposizione di legge dettagliata) |
| **Human oversight - obbligo e modelli** | Le normative settoriali e l'AI Act convergono sull'obbligo di mantenere **supervisione umana efficace** sui sistemi IA ad alto rischio. Art. 14 AI Act impone che tali sistemi siano progettati per essere "sorvegliabili" da persone, e che le persone deputate al controllo abbiano competenze, formazione e autorità per intervenire, anche interrompendo il sistema se necessario[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=necessarie%20nonch%C3%A9%20del%20sostegno%20necessario). Le **Linee Guida EBA** sul credito prescrivono che l'IA non operi in autonomia piena: l'intermediario deve poter rivedere ed _eventualmente derogare_ alle decisioni del modello (principio del _"human-in-the-loop"_)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). Ad es., l'art. 172(3) CRR già richiede che nelle banche IRB vi sia possibilità di _override umano_ dei risultati dei modelli interni di rating[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving). Dalle analisi EBA emerge che le banche UE stanno adottando approcci graduati dove l'**intervento umano** è garantito specie nelle fasi iniziali di adozione di IA avanzata, per controllare i rischi[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants). | _AI Act_, art. 14[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in)[\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana); _EBA Risk Report 2024_[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences); _Paradigma 2025_ (cit. EBA GL)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). | Il wizard deve includere campi/domande per verificare la presenza di meccanismi di oversight: es. "È previsto un intervento umano prima che la decisione finale venga applicata al cliente?"; "Il personale addetto ha facoltà di bloccare o correggere l'output dell'AI?". In base alle risposte, fornire alert se l'oversight risulta inadeguata (trigger di non conformità art. 14). Inoltre, nelle _specifiche di output_ per ogni use case, aggiungere raccomandazioni su modelli di controllo sostenibili (es. doppia verifica umana per decisioni critiche, training specifico per operatori AI, audit periodici dei risultati del modello). | **Alta** (requisito legale + best practice EBA) |
| **Trasparenza verso individui** | L'AI Act impone obblighi di trasparenza anche per sistemi non high-risk: ad esempio, chi utilizza un **chatbot** o sistema che interagisce con persone deve informare chiaramente l'utente che si tratta di un sistema automatizzato[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Inoltre, se un contenuto (testo, immagine) è generato da IA, occorre dichiararlo all'utente finale (per prevenire inganni). In ambito bancario, il GDPR art.13-14 e 22 già richiedono di comunicare agli interessati l'esistenza di decisioni automatizzate e fornire informazioni significative sulla logica usata e sulle conseguenze previste[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). La normativa nazionale (es. TUB art. 124-bis) ribadisce che se la concessione di credito avviene con strumenti automatizzati, il cliente va informato in modo chiaro e ha diritto a spiegazioni adeguate. | _AI Act_, art. 52 (ora 50)[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system); _GDPR_, art. 13-14, 22; _Paradigma 2025_[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). | Il wizard deve chiedere se l'AI interagisce con clienti o determina output rivolti a persone. In caso affermativo, indicare tra gli output obbligatori: "Preparare un'informativa specifica per gli utenti", "Inserire disclaimer visibili nell'interfaccia (es. 'Assistente virtuale automatizzato')". Inoltre, fornire linee guida su come redigere spiegazioni delle decisioni in linguaggio comprensibile. Il tool potrebbe includere un modulo di **template di informativa** da riempire con i dettagli del caso d'uso (es. tipo di logica algoritmica, dati usati) in conformità a GDPR. | **Alta** (norme chiare GDPR + AI Act) |
| **Non discriminazione e fairness** | Il principio di non discriminazione non è dettagliatamente regolato nelle norme finanziarie, ma è un focus centrale dell'AI Act e delle autorità. La Banca d'Italia nota che nelle disposizioni di trasparenza bancaria vi sono scarsi riferimenti espliciti alla _parità di trattamento_, e l'uso di tecniche AI-ML sollecita nuove attenzioni su questo fronte[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove). L'AI Act vieta sistemi di scoring sociale e categorizzazione in base a dati sensibili, e nei considerando evidenzia il rischio che modelli di credito o di recruiting possano _perpetuare bias storici_ (ad es. sfavorire donne, minoranze)[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al). Casi reali confermano il pericolo: algoritmi di HR troppo opachi o di credito basati su dati correlati a etnia/zona possono produrre disparità. Il Garante privacy italiano ha richiamato come la profilazione creditizia debba evitare di trattare dati sensibili o proxy di quelli (es. cap di residenza) senza adeguate cautele[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi). | _Banca d'Italia QEF 721/2022_[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove); _AI Act_ consid. 58[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al); _Paradigma 2025_[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano). | Il wizard deve includere checkpoint dedicati alla fairness: es. chiedere se il dataset è stato controllato per bias (squilibri rappresentativi), se il modello utilizza attributi potenzialmente discriminanti (diretti o indiretti). In output, per casi d'uso sensibili (credito, HR), raccomandare di effettuare un _"bias audit"_ e documentare gli esiti nella FRIA. Inoltre, segnalare l'opzione conservativa di escludere dal modello variabili non pertinenti o potenzialmente proxy di categorie protette. Fornire riferimenti a linee guida (es. Appendice tecnica su metriche di fairness) nella bibliografia del wizard per approfondimento. | **Media** (principio generale chiaro, ma mancano metriche univoche) |
| **Overlap normativo e approcci conservativi** | L'EBA ha riscontrato che molte esigenze dell'AI Act (es. data governance, robustezza, oversight) sono in parte coperte dalle norme finanziarie esistenti, sebbene non vi siano _deroghe esplicite_ nell'AI Act per il settore bancario[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=includes%20a%20wide%20range%20of). Ciò significa che banche e intermediari dovranno rispettare entrambe le cornici: ad es., un modello di credito IRB deve seguire le regole CRR/EBA _e_ soddisfare i requisiti AI Act (documentazione tecnica, testing, ecc.). Questo doppio binario può creare oneri, ma anche opportunità di integrazione. Ad esempio, i controlli periodici sui modelli richiesti da Banca d'Italia/EBA (validazioni, backtesting) possono valere anche come misure di monitoraggio continuo ai sensi AI Act. Nel dubbio interpretativo su categorie borderline, le banche stanno adottando un approccio prudenziale, spesso applicando volontariamente le misure più rigorose. Una prassi raccomandata è considerare la **FRIA e DPIA combinate** come parte di un unico processo di _AI risk assessment_, coinvolgendo funzioni diverse (Compliance, DPO, Risk Management) per coprire tutti gli aspetti. | _EBA Chair Letter 2025_[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=includes%20a%20wide%20range%20of); _Banca d'Italia QEF_[\[37\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono)[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in). | Il wizard dovrebbe fornire all'utente un quadro di _"intersezione normativa"_ - ad esempio una sezione riassuntiva che dica: "Il tuo caso richiede: conformità AI Act (requisiti X, Y, Z) **e anche** rispetto delle regole bancarie ABC (es. Linee guida EBA x)". Suggerire un approccio integrato: output potrebbe consigliare di unificare DPIA+FRIA in un unico documento/procedura aziendale. Inoltre, nelle spiegazioni, il tool evidenzierà dove gli obblighi coincidono (es. qualità dati è sia requisito AI Act art.10 che buona pratica modelli interni) per evitare duplicazioni. In caso di incertezza (use case non esplicitamente normato), il wizard adotterà il flag "approccio conservativo suggerito" e includerà misure extra precauzionali. | **Alta** (ricognizione EBA autorevole; convergenza con prassi Banca d'Italia) |

## Questioni Residue Aperte
Rimangono aperti alcuni apunti, come: 

- **Criteri pratici per "rischio significativo" (Art.6(3) AI Act):** non è chiaro come un fornitore o utilizzatore potrà dimostrare che un sistema rientrante in Allegato III **non** presenta un _rischio significativo_ e quindi esentarlo dal regime high-risk[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,%20risultato%20del%20processo%20decisionale). Serviranno linee guida su metriche di rischio e chi avalla tale auto-valutazione, per evitare decisioni e under-classification.
- **Coordinamento tra Autorità (AI Act vs privacy vs settore):** chi sarà in pratica l'autorità di riferimento per vigilare sui sistemi AI bancari? Il _Market Surveillance Authority_ per high-risk AI potrebbe essere Banca d'Italia (vigilanza bancaria) o un nuovo soggetto; il Garante Privacy manterrà ruolo su DPIA e data protection; l'EBA/ECB avranno voce tramite l'_AI Board_. Occorrono protocolli chiari per prevenire conflitti o vuoti di competenza nelle valutazioni ex ante (FRIA) e nei controlli ex post.
- **Standard di riferimento e certificazioni:** dato il forte carattere tecnico dei requisiti AI Act, ci si chiede quali standard tecnici adotteranno le banche per dimostrare conformità (es. ISO 42001 AI Management? certificazioni di bias/fairness?). L'EBA nota che Commissione emanerà linee guida per la classificazione high-risk entro il 2 Feb 2026[\[65\]](https://www.eba.europa.eu/sites/default/files/2025-11/d8b999ce-a1d9-4964-9606-971bbc2aaf89/AI%20Act%20implications%20for%20the%20EU%20banking%20sector.pdf), ma sul piano operativo le banche vorrebbero un framework unificato. La domanda aperta: _conviene attendere standard ufficiali o partire con certificazioni volontarie (es. audit etici, attestazioni da terze parti) per stare avanti?_
- **Integrazione DPIA-FRIA:** come implementare praticamente un processo unico che copra entrambe? Si deve produrre due report separati (uno per Garante, uno per autorità AI Act) o un unico _"AI Risk Assessment Report"_ basterà per entrambi scopi? E in caso di valutazione d'impatto con esito dubbio (con un rischio residuo alto): per GDPR c'è un obbligo di consultazione del Garante, per AI Act non è prevista ma magari l'Autorità mercato può intervenire; bisognerebbe capire come allineare queste escalation.
- **Gestione dei fornitori terzi e liability:** se un vendor fornisce un sistema AI non conforme e la banca subisce una violazione (es. multa per discriminazione), su chi ricade la responsabilità? Il regime AI Act prevede responsabilità primaria del _provider_ per requisiti tecnici e dell' utente (_user_) per uso improprio. Ma nei contratti reali tra banca e vendor serviranno clausole robuste su garanzie, indennizzi e accesso alle informazioni (audit). La questione aperta: le banche potranno chiedere contrattualmente ai vendor di condurre e condividere una FRIA da _provider_? Oppure ogni banca dovrà farla in solitudine anche per pacchetti standard?
- **Metriche di fairness e soglie accettabili:** i regolatori richiederanno alle banche di quantificare e mantenere certi livelli di fairness nei modelli (es. _"disparate impact ratio"_ non peggiore di 80%)? O rimarrà tutto qualitativo? L'assenza di criteri quantitativi univoci è un problema: una banca potrebbe considerare accettabile un leggero scostamento, un'altra no. Ci si chiede se l'EBA o l'AI Office elaboreranno guidance in tal senso.
- **Uso di dati sensibili per finalità etiche (bias correction):** paradosso noto - per testare se un modello è discriminante servirebbe a volte considerare la variabile protetta (es. genere) nei dati; ma ciò è vietato per decisione. Il Garante permetterà di usare dati sensibili simulati o raccolti post-assunzione per validare fairness? Su questo c'è incertezza e le banche faticano a definire metodologie di bias audit rispettose del GDPR.
- **Interoperabilità con regolamenti futuri (ESG, AI liability):** come si combineranno i requisiti dell'AI Act con altri emergenti, ad esempio le iniziative sull'_AI liability_ (responsabilità civile per danni da AI) o la normativa ESG (che potrebbe includere uso etico di AI)? Le banche dovranno mappare anche questi aspetti. Inoltre rimane come domanda aperta se la documentazione predisposta (es. registro eventi AI, log decisioni) potrà essere usata contro la banca in cause civili (un tema di liability non risolto: troppa trasparenza potrebbe esporre a contenziosi).
- **Ruolo dell'AI Office UE vs Autorità nazionali:** l'AI Office avrà poteri di supervisione soprattutto sui foundation models e sui sistemi AI ad alto rischio con impatto. Ma potrà emanare linee guida vincolanti anche per settori regolati? Le banche dovranno tenere un occhio a possibili indicazioni sovranazionali aggiuntive. La questione: se l'AI Office (Commissione) identifica un modello bancario come _"alto impatto"_ GPAI, potrebbe intervenire direttamente? Va chiarito.
- **Aggiornamento continuo del tool vs evoluzione norme:** riconoscendo che le interpretazioni e prassi attorno ad AI Act e DPIA evolveranno (giurisprudenza, orientamenti EDPB, nuove modifiche regolamentari), come mantenere il Navigator sempre aggiornato? Il processo di desk research ha limiti, alcune questioni saranno risolte solo con _regulatory feedback loop_ (es. prime FRIA effettivamente svolte, sanzioni inflitte, ecc.). È aperto il tema di governance del tool: chi in azienda (o consorzio ABI Lab) lo aggiornerà con le _lessons learned_ e nuove fonti normative man mano che emergono? Questo determina la sostenibilità a lungo termine della soluzione che è stata proposta.

# Bibliografia

- **Regolamento (UE) 2024/1689 "AI Act"** - Testo normativo fondamentale adottato nel giugno 2024[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20previsti,ad%20alto%20rischio%20ai%20sensi)[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta)[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=obbligo%20di%20notifica)[\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d%27impatto%20sulla%20protezione%20dei%20dati)[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo)[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,%20risultato%20del%20processo%20decisionale)[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=L'ufficio%20per%20l'IA%20elabora%20un%20modello%20di%20questionario,%20a%20norma%20del%20presente%20articolo%20in%20modo%20semplificato.)[\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=necessarie%20nonch%C3%A9%20del%20sostegno%20necessario)[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in)[\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana)[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al). Definisce il quadro regolatorio UE per l'IA con approccio basato sul rischio. _Rilevanza:_ Allegati e articoli citati identificano i casi bancari high-risk (credito, HR) e impongono obblighi (es. art. 14 oversight umano, art. 27 FRIA). Base per molte compliance action del Navigator.
- **DPIA: Garante Privacy (Provv. n. 467/2018) e Linee Guida WP29/EDPB WP248** - Sintesi dei casi di trattamento obbligatoriamente soggetti a DPIA e dei criteri di rischio elevato[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20valutazione%20d'impatto:)[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). _Rilevanza:_ riferimento operativo per capire quando scatta la DPIA e quali elementi includere.
- **Paradigma.it - "AI e concessione del credito: tra innovazione e responsabilità" (Ott 2025)**[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,supervisione%20costante)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=La%20Banca%20d'Italia,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario)[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori)[\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano)[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi)[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano) - Approfondimento giuridico italiano su obblighi e responsabilità nelle decisioni di credito. _Rilevanza:_ utile per inquadrare prassi di controllo umano e accountability.
- **Banca d'Italia - Quaderno Economia e Finanza n.721 "Intelligenza artificiale nel credit scoring" (2022)**[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in)[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20internazionale,%20prassi%20concrete%22)[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove)[\[37\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono) - Studio empirico e regolatorio su benefici/rischi e governance AI nel credito. _Rilevanza:_ evidenzia la necessità di presidiare fairness e trasparenza con misure operative.
- **EBA Risk Assessment Report, "Special topic AI" (Nov 2024)**[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences)[\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants) - Rapporto EBA su diffusione AI e sfide operative nelle banche. _Rilevanza:_ supporta le sezioni su use case e skill gap.
- **Comunicato Consiglio UE 9 Dec 2023 (Accordo AI Act)** - Nota stampa del Consiglio che sintetizza obblighi chiave e requisiti di trasparenza[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system)[\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons).
- **Lettera EBA Chair José M. Campa (Nov 2025) - Mapping AI Act vs regolamentazione bancaria**[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving)[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=includes%20a%20wide%20range%20of) - Documento indirizzato alla Commissione che esamina sovrapposizioni tra obblighi AI Act e normativa bancaria. _Rilevanza:_ utile per capire dove la compliance esistente copre requisiti AI Act.
- **EBA Factsheet - "AI Act: Implications for the EU banking sector"**[\[65\]](https://www.eba.europa.eu/sites/default/files/2025-11/d8b999ce-a1d9-4964-9606-971bbc2aaf89/AI%20Act%20implications%20for%20the%20EU%20banking%20sector.pdf) - Scheda EBA con tempistiche e primi orientamenti operativi sulla classificazione high-risk.
