# AI nel banking: tra DPIA e AI Act

## Abstract

Il settore bancario sta adottando l'IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarendo quando scattano DPIA e FRIA e quali obblighi operativi ne derivano. L'obiettivo è offrire una mappa pratica per valutare i rischi, distinguere i casi ad alto rischio e impostare una governance coerente.

## Introduzione

L'adozione dell'IA in banca non è piu' sperimentale: è già parte di processi che toccano diritti, accesso al credito e tutela dei dati. Prima di addentrarci nelle regole, serve un quadro introduttivo che colleghi i principali casi d'uso ai due pilastri regolatori (GDPR e AI Act) e alle valutazioni di impatto richieste. Questo articolo fa da ponte tra tecnologia e compliance, per orientare fin da subito le scelte di progetto.

## Cosa dicono le regole e come muoversi

### Principali evidenze

Le nuove regole europee sull'AI (Reg. UE 2024/1689, "AI Act") classificano come _"sistemi ad alto rischio"_ diversi impieghi dell'Intelligenza Artificiale (IA) nel settore bancario, imponendo requisiti stringenti[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). 

In particolare, l'uso dell'AI per valutare l'affidabilità creditizia (credit scoring) di persone fisiche rientra nell'Allegato III ed è quindi considerato _high-risk_[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Analogamente, i sistemi IA impiegati nel processo di **reclutamento e gestione del personale** (es. selezione candidati, decisioni di assunzione o promozione) sono esplicitamente elencati tra gli usi ad alto rischio, dato il potenziale impatto sui diritti dei lavoratori e il rischio di discriminazioni[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati). 

Altri casi d'uso tipici in banca - come sistemi di **chatbot o assistenti virtuali per clienti**, strumenti di **anti-riciclaggio (AML) e rilevazione frodi**, sistemi di **verifica identità (KYC)** con riconoscimento biometrico, ecc. - pur non tutti ricadendo in categorie "alto rischio" per l'AI Act, comportano comunque significativi obblighi di conformità. In particolare, l'AI Act esenta dal novero _high-risk_ i sistemi IA usati per individuare frodi finanziarie o per calcoli patrimoniali prudenziali[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20previsti,ad%20alto%20rischio%20ai%20sensi); ciò significa che, paradossalmente, algoritmi di **fraud detection** o di **monitoraggio transazioni sospette AML** non sono soggetti ai requisiti di alto rischio dell'AI Act, pur dovendo rispettare la normativa di settore (es. antiriciclaggio) e privacy. 

> Resta fermo, in ogni caso, l'obbligo per le banche di effettuare una valutazione caso-per-caso: ad esempio, un sistema di IA bancario che, pur non rientrando espressamente nell'Allegato III, presenti rischi significativi per diritti, sicurezza o accesso a servizi essenziali potrebbe essere prudentemente trattato come _alto rischio_ ai fini interni.

Le fonti regolamentari primarie delineano una serie di **checklist obbligatorie** e buone pratiche applicabili. Sul versante privacy, il GDPR impone la **Data Protection Impact Assessment (DPIA)** per trattamenti con rischio elevato, e le autorità (EDPB e Garante Privacy) hanno elencato criteri e casi tipici: ad esempio, profilazione o scoring su larga scala, decisioni automatizzate con effetti significativi (come concessione di un prestito), monitoraggio sistematico di utenti, uso di dati sensibili o di tecnologie innovative come l'IA[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). 

Molti use case di IA bancari soddisfano tali criteri come per esempio:
- credit scoring = profilazione finanziaria con impatto su diritti;
- monitoraggio transazioni = controllo sistematico;
- riconoscimento facciale = dato biometrico;

> _Questi richiedono quindi una valutazione preventiva d'impatto sulla privacy (DPIA)_ 

In parallelo, l'AI Act introduce per gli utilizzatori (_deployers_) di sistemi AI ad alto rischio l'obbligo di condurre una **Valutazione d'Impatto sui Diritti Fondamentali (FRIA)** prima della messa in uso[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista). 

In ambito bancario ciò riguarda in particolare gli enti che impiegano sistemi di AI Act Annex III, ad esempio le banche che usano IA per credit scoring (Allegato III §5(b)) o per decisioni su polizze vita/sanitarie. Tale FRIA deve considerare contesto d'uso, categorie di interessati coinvolti, rischi per diritti (es. bias, esclusione), misure di controllo umano previste e azioni di rimedio[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta). Va notificata all'autorità di vigilanza di mercato competente con i relativi risultati[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=obbligo%20di%20notifica), inserendosi quindi come adempimento formale prima dell'uso del sistema.

Un tema chiave emerso è la **sovrapposizione e distinzione** tra DPIA (focalizzata sui rischi privacy GDPR) e FRIA (focalizzata su impatti _etici e diritti fondamentali_ più ampi). In caso di sistemi IA che trattano dati personali, il legislatore prevede che la FRIA _si integri_ con la DPIA già svolta, evitando duplicazioni: in pratica, la **DPIA copre privacy e sicurezza dati**, mentre la **FRIA estende l'analisi a discriminazione, accesso equo a servizi, trasparenza verso gli interessati**, ecc. 

Ad esempio, per un algoritmo di concessione prestiti, la DPIA valuterà liceità e proporzionalità del trattamento dati, minimizzazione, misure di sicurezza e anonimizzazione; la FRIA aggiungerà la valutazione dei rischi di _bias algoritmico_, impatto socio-economico (es. esclusione finanziaria di gruppi vulnerabili) e l'adeguatezza delle misure di **human oversight** adottate. Quest'ultimo punto, la supervisione umana, ricorre come principio basilare: tanto il GDPR (art.22) quanto l'AI Act e le linee guida di settore insistono sulla necessità che l'IA _non prenda decisioni completamente autonome senza possibilità di intervento umano_. 

Le **Linee Guida EBA 2020 sul governo del credito** hanno chiarito che l'uso di modelli automatizzati è consentito solo entro confini precisi, includendo _trasparenza, tracciabilità, supervisione costante ed evitando ogni forma di discriminazione_[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,supervisione%20costante). 

Allo stesso modo, Banca d'Italia ha ribadito che l'IA può supportare la valutazione del rischio creditizio _ma non sostituire il giudizio umano_: la banca rimane responsabile ultima di correttezza e legittimità delle decisioni[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=La%20Banca%20d'Italia,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). 

**L'imprescindibilità del controllo umano** è sancita anche a livello di principi internazionali: vi è ampia convergenza sull'importanza di mantenere un intervento umano significativo ("human-in-the-loop") nei processi decisionali automatizzati, per garantire accountability e tutela dei diritti[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in). Nella pratica bancaria ciò si traduce, ad esempio, nel prevedere che le decisioni più impattanti (es. rifiuto di credito, segnalazione di operazione sospetta) siano riesaminate o avallate da personale competente, e che gli operatori addetti abbiano _formazione adeguata sull'IA_ e potere di fermare o correggere il sistema[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la). 

I grandi istituti si stanno infatti orientando verso modelli "ibridi" in cui l'IA elabora raccomandazioni ma l'uomo ha l'ultima parola, investendo in programmi di **upskilling** del personale per colmare il gap di competenze tecniche[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent). Ciò risponde anche a esigenze di gestione del rischio: come osservato dall'EBA, molte banche adottano un approccio graduale e prudente all'AI, introducendo solide **misure di controllo e "guardrails"** prima di estendere l'uso di modelli avanzati, specie di tipo generativo, e testando i casi d'uso più rischiosi solo dopo aver maturato sufficiente esperienza e confidenza nella tecnologia[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences).

Dal punto di vista strettamente normativo, emergono _nuovi obblighi operativi_: ad esempio requisiti di **trasparenza verso gli utenti**. Il regolamento AI Act impone (anche per sistemi non high-risk) che chi interagisce con un'IA sia informato del fatto che sta interagendo con una macchina e non un essere umano[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Ciò significa che in un contact center automatizzato o chatbot bancario dev'essere chiaramente segnalato al cliente che il servizio è erogato da un sistema AI, e occorre prevedere canali di _escalation a un operatore umano_ su richiesta. Analogamente, se l'IA genera contenuti (es. una risposta testuale in linguaggio naturale al cliente), vanno forniti eventuali disclaimer sull'origine automatica e accuratezza delle informazioni. Sul fronte **non-discriminazione e fairness**, sebbene la normativa bancaria italiana contenga solo generiche clausole di equità nell'erogazione del credito, il nuovo quadro richiede attenzione rafforzata: l'AI Act inserisce esplicitamente il divieto di algoritmi che introducano classificazioni basate su caratteristiche sensibili (pratiche assimilabili al _social scoring_ sono vietate) e, come detto, include il credito tra i casi ad alto rischio proprio per i possibili _bias algoritmici_[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). I garanti privacy e le autorità di consumo potranno sindacare pratiche scorrette se un modello nega sistematicamente servizi a categorie protette. Pertanto, le banche devono implementare tecniche di **AI governance**: test proattivi dei modelli per rilevare disparità di trattamento (bias audit), documentazione trasparente delle variabili usate (feature importance), e meccanismi di reclamo efficaci per gli interessati. Un cliente ha diritto di sapere se una decisione sul suo finanziamento è stata influenzata da un algoritmo e su quali parametri[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori), nonché di ottenere intervento umano e contestare la decisione automatizzata - come previsto dal GDPR art.22.

**Principali ambiguità normative:** Nonostante i progressi regolamentari, permangono aree grigie e nodi interpretativi. Un primo elemento di incertezza riguarda la _portata esatta delle categorie di alto rischio_: alcuni use case bancari non rientrano in modo netto nell'Allegato III. Ad esempio, l'utilizzo di IA per **Anti-Money Laundering** (rilevazione di operazioni sospette) non è elencato tra gli high-risk a meno che avvenga direttamente da autorità di contrasto - e infatti il legislatore UE ha escluso i sistemi usati dalle FIU dagli ambiti di polizia[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo). Ciò solleva dubbi: un algoritmo che _blocca preventivamente_ transazioni o conti correnti potrebbe incidere su diritti fondamentali (es. libertà economica) quasi quanto un sistema di credit scoring, ma formalmente l'AI Act non lo copre come high-risk. La scelta sembra motivata dal voler favorire l'innovazione anti-frode, ma resta ambigua la linea di confine: le banche dovranno decidere se trattare questi sistemi "non classificati" comunque con un approccio conservativo (applicando volontariamente requisiti affini a quelli high-risk) per prudenza e accountability.

Ulteriore ambiguità concerne la **definizione di "rischio significativo"** nell'AI Act. La norma prevede infatti che i sistemi elencati in Allegato III _non_ siano considerati ad alto rischio se, "in deroga", _non presentano un rischio significativo_ per salute, sicurezza o diritti[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III). Questa clausola di esenzione è di non facile applicazione pratica: ad esempio, una piccola soluzione AI usata in via sperimentale su pochi clienti, pur tecnicamente rientrando in una categoria (mettiamo credit scoring), potrebbe essere sostenuta come a rischio trascurabile dal fornitore; tuttavia i criteri per stabilirlo non sono esplicitati e c'è il rischio di interpretazioni difformi. Le aziende potrebbero essere restie a "declassare" un sistema da high-risk a non, temendo contestazioni a posteriori - si profila quindi un atteggiamento prudenziale, ma la mancanza di linee guida attuative al riguardo è un gap che richiederà chiarimenti (la Commissione è delegata a emettere atti per modificare l'Allegato III e fornire criteri, ma occorrerà vedere come verrà gestito).

Un terzo profilo di incertezza riguarda la **metodologia e governance della FRIA**. Trattandosi di un adempimento nuovo, non esistono ancora standard consolidati su _come condurre una valutazione di impatto etico/fondamentale_. L'AI Act prevede che l'_AI Office_ (nuovo organismo europeo) fornisca un modello di questionario anche via tool automatizzato per facilitare i deployer[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato), ma fino a che ciò non sarà sviluppato, le imprese dovranno arrangiarsi ispirandosi a linee guida analoghe (es. quelle del Garante per valutazioni etiche, o framework come l'Assessment List for Trustworthy AI dell'UE). Quali competenze dovranno coinvolgere? Chi sarà l'"autorità di notifica" della FRIA in Italia per il settore bancario - il Ministero dello Sviluppo Economico, Banca d'Italia o un nuovo organo? - Non è ancora definito con precisione, e ciò crea ambiguità operative. Inoltre, mentre per la DPIA privacy esiste l'obbligo di consultare il Garante solo se residuano rischi elevati non mitigati, per la FRIA sembra esserci un obbligo generalizzato di notifica pre-uso[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica), senza però un chiaro processo su cosa accade se l'autorità ritiene la FRIA inadeguata o l'uso troppo rischioso: potrà bloccarlo? servirà una sorta di "nulla osta"? Saranno temi da chiarire a livello attuativo.

Ambiguità anche nella **distinzione di ruoli e responsabilità** lungo la filiera AI. In molti casi le banche utilizzano soluzioni di IA fornite da vendor terzi o basate su modelli generativi pre-addestrati (es. un foundation model linguistico integrato nel chatbot). Il regolamento distingue **"fornitore"** (chi immette sul mercato il sistema AI) e **"utilizzatore (deployer)"** finale; nel caso bancario, però, una banca che sviluppi internamente un algoritmo per uso proprio potrebbe essere considerata sia fornitore che deployer, con obblighi cumulativi (inclusa la conformità tecnica e marcatura CE del sistema high-risk). Se invece acquista un servizio AI esterno, dovrà comunque garantire gli obblighi dei deployer (FRIA, registrazione nel database UE, sorveglianza d'uso) ma dipende dal fornitore per la documentazione tecnica conforme. È incerto come gestire contrattualmente questa condivisione di responsabilità: le banche dovranno pretendere dai vendor garanzie di conformità AI Act (es. **EU Declaration of Conformity** per sistemi high-risk) e accesso alle info sul modello per poter fare la FRIA, ma non è ancora pratica comune.

Un'altra area grigia è la **gestione della spiegabilità ed esercizio dei diritti GDPR** in presenza di modelli AI opachi (es. deep learning). Il GDPR dà all'interessato diritto ad avere spiegazioni significative sulla logica di decisioni automatizzate; tuttavia, le tecniche di explainable AI sono ancora emergenti e potrebbe non essere possibile fornire spiegazioni semplici di modelli complessi. Le banche dovranno bilanciare questo obbligo con la tutela del segreto industriale sui propri algoritmi. Non esiste ancora un consenso su quale livello di trasparenza sia "sufficiente" - ambito in cui sono attesi orientamenti dal EDPB o dal futuro AI Office.

Infine, permane incertezza su **come tradurre in prassi concrete i principi etici condivisi**. La letteratura e le autorità convergono su principi come _non-discriminazione, trasparenza, oversight umano_[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in), ma - come notato da Banca d'Italia - risulta meno agevole incorporarli in norme vincolanti e procedure operative efficaci[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti). Ad esempio, tutti concordano sull'evitare bias, ma definire metriche quantitative di fairness e soglie accettabili di disparità è complesso e lasciato all'autonomia delle imprese per ora. Similmente, è pacifico che debba esserci un intervento umano, ma quanta discrezionalità e in quale fase del processo è adeguato? Sono aspetti su cui si naviga ancora a vista, con approcci conservativi (ad es. richiedere sempre un doppio controllo umano indipendente per certe decisioni critiche) in attesa di prassi consolidate.

**Implicazioni operative per l'MVP:** Le evidenze sopra delineate informano direttamente i requisiti del prototipo di tool ("AI Act Navigator" e "FRIA/DPIA Evidence Builder"). In sintesi, l'MVP dovrà: **(1)** incorporare un sistema di _triage dei casi d'uso_ basato su domande mirate che consentano di identificare se un use case ricade in categorie di _alto rischio AI Act_ o presenta trigger DPIA GDPR, guidando l'utente (es. un Compliance officer) nelle classificazioni corrette. **(2)** Dovrà implementare un insieme di **regole decisionali** (business rules) trasparenti: ad esempio, se l'utente indica che il sistema AI effettua valutazione del merito creditizio di clienti retail, il wizard dovrà automaticamente segnalarlo come _AI Act Allegato III - high-risk_ e predisporre gli step successivi (es. elenco requisiti da soddisfare, obbligo FRIA, ecc.)[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Se il caso d'uso comporta trattamento di categorie particolari di dati o profilazione estesa, il tool dovrà suggerire obbligo di DPIA[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza). **(3)** L'output del wizard dovrà includere un _cruscotto di evidenze e obblighi_: ad es. una checklist personalizzata con "Documenti da predisporre" (es. _Scheda descrittiva sistema AI_, _DPIA_, _FRIA_, _registro trattamento_, _contratto con fornitore_…), "Requisiti applicabili" (es. _art. 10 AI Act - data governance, art. 14 - oversight umano_…), "Azioni consigliate" (es. _valutare bias su dataset_, _previsto intervento umano prima decisione definitiva_, _informativa agli interessati da aggiornare_). **(4)** L'MVP dovrà integrare **disclaimer e note esplicative** in ogni sezione critica, per gestire le ambiguità normative: ad esempio una nota che chiarisca "_Se il vostro caso non rientra esattamente nelle categorie AI Act ma presenta rischi potenziali, si consiglia l'approccio più prudente - vedere sezione 'Ambiguità'_". Oppure, in caso di dubbio sulla necessità di DPIA: "_In base alle informazioni fornite, non sussiste obbligo esplicito di DPIA ai sensi art.35 GDPR, ma si raccomanda comunque una valutazione documentata dato l'uso esteso di dati personali (opzione conservativa)_". **(5)** Fondamentale sarà la **tracciabilità e giustificazione** delle raccomandazioni: il deliverable "Evidence table" fornirà il razionale (fonte normativa) dietro ogni regola del wizard, aumentando la confidenza dell'utente nelle indicazioni fornite. L'MVP quindi non solo guiderà step-by-step (wizard) ma fungerà anche da _knowledge base_ consultabile, con sezioni "Perché ti chiediamo questo?" o "Perché è richiesto questo documento?" che attingono alle fonti autorevoli (Garante, EBA, normativa) raccolte nella ricerca[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario)[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti).

# Evidence Table

| **Tema** | **Evidenza** | **Fonte** | **Implicazione per wizard** | **Confidenza** |
| --- | --- | --- | --- | --- |
| **Use case bancari ad alto rischio (AI Act)** | L'AI Act classifica come _alto rischio_ i sistemi IA usati per valutare affidabilità creditizia di persone (credit scoring) e per impieghi in ambito occupazionale (es. selezione del personale). I sistemi di questo tipo possono infatti incidere significativamente su diritti e tenore di vita degli individui, rischiando di perpetuare discriminazioni[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati). Sono esclusi invece dall'Allegato III i sistemi IA per rilevare frodi finanziarie o calcolo requisiti patrimoniali (non considerati ad alto rischio)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi). | _Reg. UE 2024/1689 (AI Act)_, consid. 58 e 59[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi); _Paradigma, 2025_[\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano). | Identificare subito se un use case rientra in categorie di Allegato III (es. credito, HR) - in tal caso marcare come "High-Risk AI Act" e attivare i moduli di valutazione conformità (FRIA, requisiti art. 8-15 AI Act). Per ambiti esclusi (es. anti-frode) segnalare comunque obblighi settoriali ma con regime AI Act diverso. | **Alta** (testo normativo chiaro; confermato da dottrina) |
| **DPIA - trigger nel settore bancario** | Il GDPR richiede la DPIA per trattamenti ad alto rischio; linee guida WP29/EDPB elencano criteri: tra essi profilazione o scoring su larga scala su situazione economica, decisioni automatizzate con effetti giuridici (es. concessione prestiti), monitoraggio sistematico, uso di dati sensibili o tecnologie nuove (come IA)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Il Garante ha specificato 12 tipologie obbligatorie, includendo: _"trattamenti valutativi o di scoring su larga scala"_ e _"decisioni automatizzate che incidono significativamente sull'interessato (es. screening clienti di una banca per concessione finanziamento)"_[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). | _Linee Guida WP29 n.248/2017_ (EDPB)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza); _Garante Privacy, Provv. 467/2018_[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). | Nel questionario wizard, domande per rilevare se il caso d'uso coinvolge profilazione finanziaria, decisioni automatizzate su clienti, monitoraggio transazioni, uso di biometria, ecc. - in caso affermativo, far scattare alert "DPIA obbligatoria" e aggiungere il task "Esegui DPIA" nell'output. | **Alta** (linee guida ufficiali EDPB recepite dal Garante) |
| **FRIA - obbligo e contenuti** | L'AI Act (art. 27) obbliga i _deployer_ di sistemi IA ad alto rischio (p.a. e privati che forniscono servizi pubblici, nonché chi usa sistemi di Allegato III punti 5(b) e (c)) a effettuare una **Valutazione d'Impatto sui Diritti Fondamentali** prima dell'uso[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti). La FRIA deve includere: descrizione del contesto d'uso e scopo del sistema, durata e frequenza d'uso, categorie di persone impattate, rischi specifici per diritti (tenendo conto info fornite dal provider ai sensi art.13 AI Act), misure di sorveglianza umana previste, e misure di mitigazione/gestione in caso di problemi (incl. meccanismi di reclamo)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d). L'esito va notificato all'autorità di vigilanza di mercato, usando il modello che sarà predisposto (anche via tool automatizzato dall'AI Office)[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica)[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato). Se il deployer ha già svolto una DPIA GDPR che copre parte degli aspetti, la FRIA può integrare quella analisi senza duplicarla[\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=4,d%27impatto%20sulla%20protezione%20dei%20dati). | _Reg. UE 2024/1689_, art. 27[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista); Consiglio UE, Comunicato 9/12/23[\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons). | Il wizard deve spiegare chiaramente quando è richiesta la FRIA (es. "Use case classificato High-Risk → obbligatoria FRIA prima della messa in esercizio"). Dovrà guidare l'utente nell'assemblare gli elementi per la FRIA: es. chiedere di descrivere finalità e contesto, elenco interessati impattati, ecc., e produrre uno schema di report. Inoltre, deve ricordare la necessità di _notifica all'autorità_ e fornire eventualmente un template di output conforme (es. modulistica standard AI Office). Deve anche segnalare che se è stata fatta una DPIA, questa va aggiornata/integrata piuttosto che duplicata. | **Alta** (disposizione di legge dettagliata) |
| **Human oversight - obbligo e modelli** | Le normative settoriali e l'AI Act convergono sull'obbligo di mantenere **supervisione umana efficace** sui sistemi IA ad alto rischio. Art. 14 AI Act impone che tali sistemi siano progettati per essere "sorvegliabili" da persone, e che le persone deputate al controllo abbiano competenze, formazione e autorità per intervenire, anche interrompendo il sistema se necessario[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario). Le **Linee Guida EBA** sul credito prescrivono che l'IA non operi in autonomia piena: l'intermediario deve poter rivedere ed _eventualmente derogare_ alle decisioni del modello (principio del _"human-in-the-loop"_)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). Ad es., l'art. 172(3) CRR già richiede che nelle banche IRB vi sia possibilità di _override umano_ dei risultati dei modelli interni di rating[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=%E2%80%A2%20CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving). Dalle analisi EBA emerge che le banche UE stanno adottando approcci graduati dove l'**intervento umano** è garantito specie nelle fasi iniziali di adozione di IA avanzata, per controllare i rischi[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants). | _AI Act_, art. 14[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in)[\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana); _EBA Risk Report 2024_[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences); _Paradigma 2025_ (cit. EBA GL)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). | Il wizard deve includere campi/domande per verificare la presenza di meccanismi di oversight: es. "È previsto un intervento umano prima che la decisione finale venga applicata al cliente?"; "Il personale addetto ha facoltà di bloccare o correggere l'output dell'AI?". In base alle risposte, fornire alert se l'oversight risulta inadeguata (trigger di non conformità art. 14). Inoltre, nelle _specifiche di output_ per ogni use case, aggiungere raccomandazioni su modelli di controllo sostenibili (es. doppia verifica umana per decisioni critiche, training specifico per operatori AI, audit periodici dei risultati del modello). | **Alta** (requisito legale + best practice EBA) |
| **Trasparenza verso individui** | L'AI Act impone obblighi di trasparenza anche per sistemi non high-risk: ad esempio, chi utilizza un **chatbot** o sistema che interagisce con persone deve informare chiaramente l'utente che si tratta di un sistema automatizzato[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Inoltre, se un contenuto (testo, immagine) è generato da IA, occorre dichiararlo all'utente finale (per prevenire inganni). In ambito bancario, il GDPR art.13-14 e 22 già richiedono di comunicare agli interessati l'esistenza di decisioni automatizzate e fornire informazioni significative sulla logica usata e sulle conseguenze previste[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). La normativa nazionale (es. TUB art. 124-bis) ribadisce che se la concessione di credito avviene con strumenti automatizzati, il cliente va informato in modo chiaro e ha diritto a spiegazioni adeguate. | _AI Act_, art. 52 (ora 50)[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system); _GDPR_, art. 13-14, 22; _Paradigma 2025_[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). | Il wizard deve chiedere se l'AI interagisce con clienti o determina output rivolti a persone. In caso affermativo, indicare tra gli output obbligatori: "Preparare un'informativa specifica per gli utenti", "Inserire disclaimer visibili nell'interfaccia (es. 'Assistente virtuale automatizzato')". Inoltre, fornire linee guida su come redigere spiegazioni delle decisioni in linguaggio comprensibile. Il tool potrebbe includere un modulo di **template di informativa** da riempire con i dettagli del caso d'uso (es. tipo di logica algoritmica, dati usati) in conformità a GDPR. | **Alta** (norme chiare GDPR + AI Act) |
| **Non discriminazione e fairness** | Il principio di non discriminazione non è dettagliatamente regolato nelle norme finanziarie, ma è un focus centrale dell'AI Act e delle autorità. La Banca d'Italia nota che nelle disposizioni di trasparenza bancaria vi sono scarsi riferimenti espliciti alla _parità di trattamento_, e l'uso di tecniche AI-ML sollecita nuove attenzioni su questo fronte[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove). L'AI Act vieta sistemi di scoring sociale e categorizzazione in base a dati sensibili, e nei considerando evidenzia il rischio che modelli di credito o di recruiting possano _perpetuare bias storici_ (ad es. sfavorire donne, minoranze)[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al). Casi reali confermano il pericolo: algoritmi di HR troppo opachi o di credito basati su dati correlati a etnia/zona possono produrre disparità. Il Garante privacy italiano ha richiamato come la profilazione creditizia debba evitare di trattare dati sensibili o proxy di quelli (es. cap di residenza) senza adeguate cautele[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi). | _Banca d'Italia QEF 721/2022_[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove); _AI Act_ consid. 58[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al); _Paradigma 2025_[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano). | Il wizard deve includere checkpoint dedicati alla fairness: es. chiedere se il dataset è stato controllato per bias (squilibri rappresentativi), se il modello utilizza attributi potenzialmente discriminanti (diretti o indiretti). In output, per casi d'uso sensibili (credito, HR), raccomandare di effettuare un _"bias audit"_ e documentare gli esiti nella FRIA. Inoltre, segnalare l'opzione conservativa di escludere dal modello variabili non pertinenti o potenzialmente proxy di categorie protette. Fornire riferimenti a linee guida (es. Appendice tecnica su metriche di fairness) nella bibliografia del wizard per approfondimento. | **Media** (principio generale chiaro, ma mancano metriche univoche) |
| **Overlap normativo e approcci conservativi** | L'EBA ha riscontrato che molte esigenze dell'AI Act (es. data governance, robustezza, oversight) sono in parte coperte dalle norme finanziarie esistenti, sebbene non vi siano _deroghe esplicite_ nell'AI Act per il settore bancario[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of). Ciò significa che banche e intermediari dovranno rispettare entrambe le cornici: ad es., un modello di credito IRB deve seguire le regole CRR/EBA _e_ soddisfare i requisiti AI Act (documentazione tecnica, testing, ecc.). Questo doppio binario può creare oneri, ma anche opportunità di integrazione. Ad esempio, i controlli periodici sui modelli richiesti da Banca d'Italia/EBA (validazioni, backtesting) possono valere anche come misure di monitoraggio continuo ai sensi AI Act. Nel dubbio interpretativo su categorie borderline, le banche stanno adottando un approccio prudenziale, spesso applicando volontariamente le misure più rigorose. Una prassi raccomandata è considerare la **FRIA e DPIA combinate** come parte di un unico processo di _AI risk assessment_, coinvolgendo funzioni diverse (Compliance, DPO, Risk Management) per coprire tutti gli aspetti. | _EBA Chair Letter 2025_[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of); _Banca d'Italia QEF_[\[37\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono)[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in). | Il wizard dovrebbe fornire all'utente un quadro di _"intersezione normativa"_ - ad esempio una sezione riassuntiva che dica: "Il tuo caso richiede: conformità AI Act (requisiti X, Y, Z) **e anche** rispetto delle regole bancarie ABC (es. Linee guida EBA x)". Suggerire un approccio integrato: output potrebbe consigliare di unificare DPIA+FRIA in un unico documento/procedura aziendale. Inoltre, nelle spiegazioni, il tool evidenzierà dove gli obblighi coincidono (es. qualità dati è sia requisito AI Act art.10 che buona pratica modelli interni) per evitare duplicazioni. In caso di incertezza (use case non esplicitamente normato), il wizard adotterà il flag "approccio conservativo suggerito" e includerà misure extra precauzionali. | **Alta** (ricognizione EBA autorevole; convergenza con prassi Banca d'Italia) |
| **Esempi e precedenti** | I sandbox regolatori italiani hanno già sperimentato soluzioni AI bancarie innovative: es. progetto "Kalaway" per piattaforma di credit risk scoring su PMI e "O-KYC" per adeguata verifica clienti con DLT[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi)[\[39\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi). Dai report finali emerge che tali soluzioni sono considerate implementabili fuori dal sandbox _a condizione del rispetto di tutte le normative applicabili_[\[40\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione)[\[41\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=ICCREA%20Banca%20e%20Banca%20Monte,una%20futura%20commercializzazione%20della%20soluzione). Inoltre, interventi del Garante Privacy (come il caso del chatbot generativo sanzionato nel 2023) mostrano che l'utilizzo di IA deve fondarsi su basi giuridiche chiare e trasparenza: nel provvedimento n.755/2024 il Garante ha multato un provider di IA generativa per aver addestrato il modello su dati personali senza base valida e senza informare adeguatamente gli interessati[\[42\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata)[\[43\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello). Questi esempi indicano sia la fattibilità delle nuove tecnologie in banca, sia le _trappole da evitare_ (es. data breach non notificati, informativa lacunosa). | _Banca d'Italia Sandbox - report_[\[40\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione); _Garante Privacy, Provv. 755/2024_[\[42\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata)[\[43\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello). | Nel wizard si possono integrare _case study_ sintetici: ad esempio, una scheda "Lezioni dal Sandbox" che ricordi all'utente di verificare la base giuridica per training AI (consenso/contratto per usare dati di clienti?) e di predisporre notifiche in caso di data breach AI. L'evidence builder potrà anche includere come "evidence" positiva il fatto che Banca d'Italia ha validato in sandbox il modello X per credit scoring, indicando come implicazione che use case analoghi sono ammessi se conformi alle linee guida EBA sulla concessione credito[\[44\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=intermediari%20finanziari%20per%20offrire%20loro,durata%20massima%20di%2018%20mesi). Ciò aumenta la confidenza per l'utente nel perseguire certi progetti, ma accompagnandola con avvertenze (compliance by design). | **Media** (evidenze empiriche utili, ma riferimenti indiretti) |

# Specifiche MVP "AI Compliance Navigator"

**Campi di input minimi (max 20):** Per alimentare il processo di triage, il tool richiederà all'utente informazioni chiave sul caso d'uso AI in esame. I campi includeranno:  
\- **Descrizione del caso d'uso** (libero): breve testo sullo scenario (es. "Chatbot per assistenza clienti che suggerisce prodotti finanziari");  
\- **Dominio applicativo primario**: menu a scelta (Credito, Assistenza Clienti, AML/Frode, Risorse Umane, KYC/Onboarding, Altro) - ai fini di mappatura vs. Allegato III;  
\- **Finalità e funzione AI**: es. _scoring/valutazione_, _decisione automatizzata_, _supporto decisionale_, _generazione contenuti_, _monitoraggio/anomalia_, ecc.;  
\- **Impatto su individui**: tipo di decisione influenzata (es. _concessione o diniego di servizio_, _classificazione rischio cliente_, _valutazione candidato_, _nessun impatto diretto_);  
\- **Coinvolgimento di dati personali?** (sì/no) - discriminante per DPIA;  
\- **Tipologia di dati trattati**: checklist (dati finanziari, identificativi, demografici, biometrici, sensibili ex art.9 GDPR, anonimi, ecc.);  
\- **Scala del trattamento**: numero indicativo di soggetti interessati (es. <1000, migliaia, milioni) e provenienza geografica (UE/non-UE) - per valutare "larga scala" e trasferimenti;  
\- **Logica del modello**: scelta multipla (Algoritmi deterministici/regole fisse; Machine Learning tradizionale; Deep Learning/NN; Generative AI/foundation model) - per valutare spiegabilità e requisiti tecnici;  
\- **Fonte e qualità dei dati**: origine dei dati di training/input (dati interni banca, dati da terzi, open data, web scraping) e presenza di possibili bias noti (campo note);  
\- **Ruolo dell'azienda**: selezione (Sviluppatore del modello in-house; Utilizzatore di modello fornito da terzi; Entrambi) - per determinare obblighi provider vs. deployer;  
\- **Coinvolgimento di fornitori terzi**: nome/descrizione eventuale vendor o modello pre-addestrato (es. uso API esterna GPT) - per predisporre clausole contrattuali e richieste conformità;  
\- **Presenza di decisione automatizzata**: (sì/no) - se l'output dell'IA viene applicato senza intervento umano diretto (es. rifiuto automatico di richiesta) - trigger per art.22 GDPR e oversight;  
\- **Misure di oversight previste**: menu o checkbox (Revisione umana di tutti gli output critici; Intervento umano su richiesta; Nessun intervento umano; Altro) - per valutare compliance art.14 AI Act;  
\- **Trasparenza verso gli interessati**: checkbox (Informativa dedicata predisposta; Non ancora; Non applicabile - es. tool solo interno);  
\- **Rischi noti/autovalutati**: campo libero o selezione multipla (Possibili bias discriminatori; Rischio errori/false segnalazioni; Impatto su privacy; Altro) - per innescare consigli mirati e predisporre FRIA;  
\- **Ambito regolamentare specifico**: selezione se applicabile (es. _Credito al consumo_, _Credito immobiliare_, _Investimenti/MiFID profiling_, _Pagamenti PSD2_, _HR - Equality_, _AML_…) - per collegare a normative settoriali aggiuntive;  
\- **Esistenza di DPIA precedente**: (sì/no) e se sì, breve riferimento - per integrare con FRIA;  
\- **Stadio del progetto**: (Idea; Sviluppo; Pilota interno; Produzione attiva) - per orientare le raccomandazioni (es. se è ancora in sviluppo, considerare sandbox/testing controllato).

_(NB: Il totale campi è ottimizzato e alcune voci possono essere combinate nell'interfaccia per restare entro ~20 input effettivi, ad es. un'unica sezione "Tipi di dati & categorie interessati" che copra sia dati sensibili che utenti vulnerabili.)_

**Regole/Trigger logici principali:** Sulla base degli input sopra, l'MVP applicherà regole "if-then" per determinare obblighi e output:  
\- **Classificazione High-Risk AI Act:** se _Dominio_ = Credito **oppure** se _Finalità_ = valutazione affidabilità creditizia **→** flag "Possibile AI Act Allegato III (5)(b) - sistema ad alto rischio (credit scoring)"[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Se _Dominio_ = HR/Recruiting **→** flag "Alto rischio (assunzione/lavoro)". Questi flag attiveranno: obbligo FRIA, indicazione di requisiti AI Act (conformità, registrazione, etc.), se del caso notifica database UE[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)[\[45\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=usati%20nel%20settore%20elencati%20nell%27allegato%C2%A0III%2C,una%20valutazione%20dell%27impatto%20sui%20diritti).  
\- **DPIA obbligatoria:** se _Coinvolgimento dati personali_ = sì **e** (Profilazione automatizzata su larga scala **oppure** Decisione senza intervento umano con effetti giuridici **oppure** Uso dati sensibili/biometrici **oppure** Monitoraggio sistematico comportamento clienti)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento) **→** suggerire "DPIA necessaria ai sensi art.35 GDPR" con elenco motivazioni (evidenziando i criteri specifici soddisfatti).  
\- **Richiesta consenso art.22 GDPR:** se _Decisione automatizzata_ = sì **e** impatta interessato significativamente (esito credito, assunzione) **→** avviso: "Verificare base giuridica per decisione automatizzata: consenso esplicito dell'interessato _oppure_ deroga di legge" (es. necessario per esecuzione contratto).  
\- **Trasparenza e diritto di spiegazione:** se _Decisione automatizzata_ = sì **→** output da includere: "Informare l'interessato della logica del sistema (GDPR 13-15) e predisporre canale per richiesta intervento umano"[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori).  
\- **Oversight insufficiente:** se _Misure oversight_ = "Nessun intervento umano" **oppure** _Finalità_ = decisione automatica _e_ oversight = limitato **→** flag di _non conformità potenziale_: raccomandare inserimento di controllo umano (es. "Si raccomanda di prevedere revisione umana: attualmente non conforme a art.14 AI Act")[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in).  
\- **Bias/Fairness:** se _Dominio_ ∈ {Credito, HR, AML} **oppure** utente ha selezionato rischio "bias discriminatori" **→** includere nel report sezione "Valutazione Fairness": suggerire audit algoritmico, rimozione variabili proxy, ecc., richiamando principio non discriminazione[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al).  
\- **GenAI/Foundation model:** se _Logica modello_ = Generative **→** raccomandare misure extra: es. validazione output, filtro dei prompt, disclosure contenuti generati (obbligo trasparenza)[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system), controllo di proprietà intellettuale sui dati di training, ecc.  
\- **Vendor management:** se _Ruolo_ = utilizzatore terzo **→** far comparire checklist: "Richiedere dal fornitore: scheda sicurezza AI, documentazione tecnica (Annex IV AI Act), clausole su aggiornamenti, diritto audit, etc.".  
\- **Sandbox/regulatory testing:** se _Stadio progetto_ = Pilota/Ideazione **→** suggerire valutazione ingresso in sandbox (citando iniziative come quella di Banca d'Italia) come opzione.  
\- **Non applicabilità AI Act:** se _Dominio_ = "Altro" _e_ nessun rischio rilevante **→** output minimale: es. "Il caso d'uso non sembra ricadere in obblighi specifici AI Act oltre ai requisiti generali di trasparenza - si applicano comunque norme GDPR se dati personali".

**Output generato dall'MVP:** L'output si comporrà di diversi elementi strutturati, pensati come deliverable A-G richiesti:  
\- **Executive Summary personalizzato:** un riassunto in 8-10 punti delle finding per lo specifico use case inserito. Es: _"Il sistema proposto rientra tra quelli AI Act ad alto rischio (Credito): dovrai ottenere marcatura CE e condurre una FRIA prima dell'uso_[_\[6\]_](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)_. Presenta profilazione di dati finanziari su larga scala: è necessaria una DPIA GDPR_[_\[4\]_](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)_. Sono emerse possibili ambiguità normative (es. decisione automatica su base profilazione creditizia): si consiglia approccio conservativo con intervento umano finale e informativa rafforzata al cliente…"_. Questo summary includerà anche 3-5 implicazioni chiave per design MVP (ad es. _"integrare modulo bias testing"_, _"prevedere disclaimer verso l'utente finale"_).  
\- **Evidence Table personalizzata:** una tabella simile a quella generale ma focalizzata sul caso specifico, con righe su temi rilevanti. Ad esempio, per un caso di credit scoring compariranno righe su "Credito = High-risk (fonte: AI Act)", "Profilazione = DPIA (fonte: GDPR)", "Bias rischio (fonte: considerazioni AI Act/EBA)", ciascuna con implicazioni per il progetto. Questa tabella attinge al database di evidenze generali ma filtra quelle applicabili.  
\- **Checklist Requisiti & Azioni:** un elenco puntato di attività da svolgere per conformità: es. _Redigere Documento Tecnico AI Act (Annex IV)_, _Effettuare test di accuratezza e robustezza_ (art.15), _Iscrivere il sistema nel registro UE High-Risk_ (art. 60) se applicabile, _Condurre training a personale addetto (oversight)_, _Aggiornare privacy policy per trasparenza_, ecc. Ogni voce verrà marcata come "Obbligatorio" o "Raccomandato" a seconda della fonte (hard law vs. best practice).  
\- **Specifiche MVP funzionali:** se pertinente, l'output includerà raccomandazioni di design per l'**implementazione tecnica** dell'AI stesso in ottica compliance (ad es.: _"Loggare tutte le decisioni del modello per audit (art.12 record-keeping AI Act)"_, _"Implementare alert se input fuori distribuzione (monitoraggio drift)"_, _"Interfaccia utente: prevedere campo spiegazione decisione per il cliente"_). Queste specifiche aiutano IT e Data Science a costruire il sistema "compliant by design".  
\- **Template "Use Case Sheet":** il tool genererà un documento sintetico (1-2 pagine) per ciascuno dei 5 casi d'uso target (se rilevanti) - ad esempio, se l'utente ha classificato il caso come "Credito", verrà allegata la scheda pre-compilata per _AI in Credit Scoring_ con sezioni: Descrizione, Rischi specifici, Requisiti regolamentari, Soluzioni di oversight, Esempi pratici. Questo corrisponde al deliverable use case sheet richiesto, adattato con i dati forniti.  
\- **Domande residuali e to-do:** una lista di max 5-10 domande aperte o verifiche da effettuare prima della go-live, tarate sul caso. Es.: _"Verificare con l'ufficio legale se la base contrattuale articolo 6(1)(b) GDPR è applicabile per questo trattamento automatizzato"_; _"Consultare DPO su necessità consultazione preventiva Garante in caso di rischi residui DPIA"_; _"Approfondire metriche fairness adatte (es. disparate impact) e ripetere test bias con dataset ampliato"_.  
\- **Bibliografia commentata:** elenco delle fonti normative e linee guida pertinenti citate, con link e breve descrizione, così l'utente può approfondire (ad esempio: _"Regolamento UE 2024/1689 (AI Act) - Articoli 6, 14, 27: definisce high-risk e obblighi di valutazione impatti"_, _"Provvedimento Garante 467/2018 - Elenco trattamenti richiedenti DPIA in Italia (include scoring creditizio)"_, _"EBA Guidelines on Loan Origination 2020 - par. 74-96: uso di modelli automatizzati nel credito con requisiti di trasparenza e controllo umano"_, etc.).

**Disclaimers (MVP):** Il tool mostrerà avvertenze chiare per gestire le aspettative e limitare le responsabilità. All'avvio, un **disclaimer generale** chiarirà che: _"Questo strumento fornisce un supporto alla conformità basato su fonti normative pubbliche, ma non costituisce consulenza legale né assicura automaticamente la conformità al 100%. L'utente è responsabile delle decisioni finali e dell'implementazione delle misure proposte"_. Inoltre, ogni sezione delicata avrà **note esplicative**. Esempio: alla sezione output, un disclaimer indicherà che _"Le raccomandazioni fornite (es. necessità di DPIA) derivano dalle informazioni inserite; assicurarsi di averle fornite accuratamente. In caso di dubbio, consultare il DPO o l'Autorità competente."_[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). Per i casi borderline, il wizard potrebbe includere un disclaimer tipo: _"Approccio conservativo: dato il quadro normativo non definitivo su questo punto, suggeriamo di adottare tutte le misure come se fossero obbligatorie, in ottica di prudenza."_ In calce al report finale, un ultimo disclaimer ricorderà la data e versione delle fonti (es. _"Normativa aggiornata a dicembre 2025"_), avvisando che future modifiche regolamentari (es. linee guida in arrivo dalla Commissione o EBA) potrebbero richiedere revisione delle indicazioni.

**Out-of-scope (limiti dell'MVP):** Saranno esplicitati anche gli ambiti non coperti dal tool. Ad esempio: _"Non tratta scenari di AI esclusivamente militare o di sicurezza nazionale (esclusi dall'AI Act)", "Non valuta aspetti di proprietà intellettuale o brevetti sull'algoritmo", "Non copre la compliance di dettaglio su normative settoriali non direttamente collegate all'uso di AI (es. requisiti di trasparenza precontrattuale MiFID se l'AI è usato in consulenza investimenti, che andranno valutati a parte)"._ Inoltre verrà dichiarato che il tool _non sostituisce test tecnici di sicurezza o validazione matematica_ del modello: esso suggerisce di farli ma non li esegue. Infine, sarà fuori scope qualsiasi giudizio di merito sull'opportunità commerciale o etica di un progetto AI: l'MVP si limita a mappare obblighi e gap, lasciando alle funzioni aziendali la decisione finale se procedere o meno.

# Schede Use Case (5 casi target)

## **Use Case 1: Valutazione del Credito (Credit scoring)**

**Descrizione:** Impiego di algoritmi di AI per valutare la solvibilità dei clienti e supportare/determinare decisioni di concessione di credito (es. rating automatico di richieste di prestito al consumo o fidi PMI). Può includere modelli di machine learning addestrati su dati storici di credito per prevedere probabilità di default. Spesso sostituisce o integra sistemi di credit scoring tradizionali (es. CRIF score) con modelli più complessi (random forest, reti neurali).

**Rilevanza regolamentare:** _Alto rischio AI Act_. Il credito figura esplicitamente nell'Allegato III: _"sistemi di IA utilizzati per valutare il merito creditizio delle persone fisiche"_ sono high-risk[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al), comportando obbligo di conformità ai requisiti degli artt. 8-15 AI Act (gestione rischio, qualità dati, documentazione tecnica, trasparenza, oversight umano, accuratezza). Inoltre, la banca come _utilizzatore (deployer)_ dovrà effettuare la **FRIA** prima di mettere in uso il sistema[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti), valutando impatti su diritti (in primis rischio di discriminazione finanziaria). Va previsto l'inserimento del sistema nel registro UE dei sistemi ad alto rischio (tramite autorità di riferimento, salvo esenzioni). _Nota:_ se la banca sviluppa internamente l'algoritmo e lo impiega soltanto per sé, essa cumula anche il ruolo di _fornitore_ ai sensi AI Act, dovendo effettuare la valutazione di conformità (verosimilmente tramite autocertificazione con controllo interno per Annex III punto 5(b)) e rilasciare la Dichiarazione UE di Conformità[\[46\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,coinvolgimento%20di%20un%20organismo%20notificato)[\[47\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=match%20at%20L7793%20alto%20rischio,dati%20dell%27UE%20di%20cui%20all%27articolo%C2%A071).

**Privacy & DPIA:** Quasi certamente richiesta. Il credit scoring tratta dati personali (finanziari, comportamentali, socio-demografici) spesso su larga scala e incide significativamente sugli interessati (accettazione/rifiuto credito = effetto giuridico). Rientra quindi in almeno due criteri DPIA (profilazione + decisione automatizzata)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Il Garante italiano include esplicitamente lo _"screening clienti con dati di centrale rischi per decidere finanziamento"_ come esempio che _richiede DPIA_[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Occorre quindi condurre una DPIA ex art.35 GDPR, valutando necessità e proporzionalità dei dati usati, rischi per diritti (es. errore, esclusione di soggetti meritevoli) e misure di sicurezza. Particolare attenzione a: eventuale uso di dati _sensibili_ (diretti o indiretti) - es. se il modello inferisce reddito da CAP/residenza rischiando disparità geografiche (potenziale discriminazione indiretta su base etnica/socio-economica). GDPR art.22 è rilevante: una decisione puramente automatizzata di rifiuto credito richiede base contrattuale adeguata o consenso. In UE di solito le banche inseriscono ancora un intervento umano finale per evitare l'applicazione rigida di art.22, oppure giustificano la profilazione come necessaria all'esecuzione di un contratto (erogazione del credito) - impostazione possibile ma non esente da dibattito[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi). La **trasparenza** verso il cliente è cruciale: vanno comunicati almeno gli elementi generali della logica di scoring (quali categorie di dati incidono: es. storico pagamenti, rapporto debito/reddito, ecc.) e garantito il diritto di ottenere spiegazioni e contestare la decisione. Spesso ciò si traduce, operativamente, nell'istituzione di un _"ufficio reclami AI"_ o procedure interne per riesaminare manualmente le pratiche respinte su richiesta del cliente.

**Obblighi/controlli aggiuntivi:** Settore strettamente regolato da norme di trasparenza bancaria (TUB, provv. Banca d'It. Trasparenza) e dalle _EBA Guidelines on Loan Origination and Monitoring_ (EBA/GL/2020/06). Queste ultime richiedono che nell'utilizzo di modelli statistici/AI per concessione credito, la banca garantisca: qualità e pertinenza dei dati, **assenza di bias illegittimi** (divieto di discriminare per genere, etnia, etc.), _documentazione_ delle metodologie, e controllo umano sulle decisioni finali[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). Inoltre impongono valutazioni del merito creditizio _affidabili e comprensibili_: l'IA deve essere _"explainable"_ agli addetti e in parte al cliente. La normativa consumer credit (direttiva 2008/48/CE e succ.) prevede l'obbligo di spiegare al richiedente i motivi dell'eventuale rifiuto, specialmente se basato su processi automatizzati o data base (es. credit bureau) - il che si allinea al GDPR. Dal punto di vista di **modellistica prudenziale**, se il credit scoring IA è usato anche per calcolo accantonamenti o rating regolamentari (AIRB), deve rispettare CRR e linee guida EBA sui modelli interni: ciò comporta requisiti di validazione indipendente, _stress test_ periodici, evidenza che il modello non invecchi (monitoraggio del _drift_ delle performance). Fortunatamente, come evidenziato dall'EBA, tali pratiche di model risk management collimano in parte con gli obblighi AI Act su robustezza e accuracy[\[48\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving). Un'attenzione specifica: se il sistema utilizza dati provenienti da **fonti esterne non tradizionali** (es. social media, dati telco per credit scoring "alternativo"), occorre verificarne la liceità (consenso dell'interessato o fonte pubblica) e la qualità/pertinenza; inoltre, l'uso di dati non convenzionali potrebbe aumentare i rischi di discriminazione e va ponderato nella FRIA.

**Misure di oversight e monitoraggio:** Praticare il _"four eyes principle"_ sulle decisioni: se l'algoritmo dà esito "rifiuta", idealmente un funzionario lo rivede (almeno a campione per verificare che il modello non penalizzi ingiustamente). Implementare soglie di allerta: es. se il modello scarta più del X% di una certa categoria (es. residenti in una zona) valutarne le cause. Costituire un comitato interno AI per il credito, coinvolgendo compliance, risk e data science, che rivede periodicamente i risultati (tassi di default reali vs predetti, segnalazioni reclami ricevuti da clienti per ingiusto rifiuto, etc.) e approva eventuali revisioni del modello o delle policy. Prevedere **formazione** ai credit analyst sul funzionamento dell'AI, affinché possano spiegare ai clienti e gestire i casi eccezionali. Sul monitoraggio post-deployment: loggare tutte le decisioni con gli score e fattori principali, per consentire audit a posteriori (richiesto da AI Act art.12 record-keeping). L'AI Act richiederà anche di mettere in atto misure per garantire _accuracy, robustness e cybersecurity_ (art.15): nel credit scoring, ciò significa testare il modello su dati storici e in simulazione per assicurare che le previsioni siano sufficientemente accurate e stabili, e che il modello non possa essere facilmente ingannato (ad esempio da dati falsi). In caso di aggiornamenti del modello (retraining), andrà rifatta la validazione e aggiornata la documentazione tecnica, notificando se necessario sostanziali modifiche nel registro UE.

**Esempi pratici:** Diversi istituti hanno sperimentato IA nel credit scoring. Un caso è stato portato in Sandbox regolamentare italiano: la piattaforma di Kalaway S.r.l., in collaborazione con Banca Patavina, per _early warning_ e valutazione automatica su imprese[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi)[\[49\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=finalizzato%20a%20fornire%20alle%20banche,gestione%20del%20rischio%20di%20credito). La sperimentazione ha mostrato benefici in efficacia e capacità di profilare il rischio in linea con linee guida EBA, ma ha evidenziato la necessità di rispettare tutte le normative applicabili prima della commercializzazione[\[50\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=La%20sperimentazione%20%E2%80%93%20nell%27ambito%20della,operare%20al%20di%20fuori%20dell%27ambiente). Questo conferma che tali use case sono ammessi se ben governati. Sul fronte internazionale, si ricorda il caso **Apple Card 2019**, dove un algoritmo di credito fu accusato di discriminare sul genere (limiti di fido inferiori alle donne a parità di condizioni) - le autorità USA hanno indagato e, pur non avendo normative AI equivalenti al AI Act, il caso ha sollevato attenzione globale sui bias nei modelli creditizi. Ciò evidenzia la necessità per le banche di testare ex ante l'output del modello su diversi sub-gruppi demografici (gender, etnia se disponibile, età) per assicurare che non vi siano disparità ingiustificate (_fair lending test_). Nel contesto europeo, tali verifiche diventano parte integrante della FRIA e delle pratiche di _ethical AI_ promosse anche dall'EBA.

## **Use Case 2: Contact Center e Assistenti Virtuali (Customer service AI)**

**Descrizione:** Utilizzo di sistemi di intelligenza artificiale (es. chatbot testuali, voicebot IVR intelligenti) per gestire l'assistenza clienti in banca. Questi sistemi possono rispondere a FAQ, fornire informazioni su saldo e movimenti, aiutare nell'esecuzione di operazioni semplici (es. reset PIN), o anche proporre prodotti (finanziamenti, investimento) in modalità conversazionale. Possono basarsi su NLP (Natural Language Processing) e, più recentemente, su modelli generativi tipo GPT addestrati su knowledge base bancarie. In alcuni contact center, l'IA funge da primo livello, con possibilità di escalation a un operatore umano in caso di richieste complesse o insoddisfazione.

**Rilevanza regolamentare:** _Non tipicamente alto rischio secondo AI Act_, a meno che il bot svolga funzioni riconducibili a categorie critiche (cosa rara per un assistente generico). Tuttavia, l'AI Act impone _obblighi di trasparenza_ specifici: un sistema destinato a interagire con persone deve dichiararsi come tale[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Quindi, il cliente va informato chiaramente che sta parlando con un agente virtuale e non umano. Inoltre, se il contact center AI incorpora funzionalità di **riconoscimento emotivo** (analisi del tono di voce per inferire lo stato d'animo del cliente) - cosa a volte proposta per routing chiamate in base a sentiment - l'AI Act la classifica come pratica a rischio (in contesti come lavoro, l'emotion recognition è persino proibita)[\[51\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=For%20some%20uses%20of%20AI%2C,of%20predictive%20policing%20for%20individuals). Nel contesto customer service non lavorativo non è espressamente vietata, ma se utilizzata va gestita con estrema cautela e informando l'utente che caratteristiche emotive sono analizzate[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Dunque, il contesto impone compliance soprattutto su trasparenza e rispetto dei diritti dei consumatori.

**Privacy & DPIA:** Dipende dalle funzionalità. Se il chatbot tratta dati personali (quasi certo, visto che accede a dati conto cliente su richiesta, etc.) e potenzialmente effettua profilazione (ad es. interpretando richieste per offrire prodotti mirati), potrebbe attivare la necessità di DPIA. Di per sé, un assistente virtuale Q&A su base di dati forniti dal cliente rientra in _trattamento su larga scala di dati degli interessati e uso di tecnologia innovativa (IA)_ - due criteri che combinati suggeriscono DPIA[\[52\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=7,la%20concessione%20di%20un%20finanziamento)[\[53\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=particolari%20misure%20di%20carattere%20organizzativo,compresi%20i%20trattamenti%20che%20prevedono). Se poi c'è anche monitoraggio di conversazioni o analisi sentiment (che è _sorveglianza comunicazioni + dati potenzialmente sensibili_, come stress o emozioni), la DPIA è fortemente raccomandata. Aspetti da valutare nella DPIA: sicurezza delle informazioni (i dialoghi contengono dati riservati bancari, vanno protetti con crittografia e controlli accesso); rischio di errori o _allucinazioni_ del modello (un generative potrebbe fornire risposte errate finanziariamente dannose - es. info sbagliata su bonifici - con impatto su cliente); gestione del consenso per registrazione chiamata/chat (solitamente i call center informano della registrazione a fini qualità - se l'IA elabora la voce, è trattamento ulteriore da coprire). Importante: se il bot prende decisioni che influiscono su contratti (non comune: di solito esegue istruzioni del cliente, non decide al posto suo), allora subentra art.22. Nella generalità, il contact center AI assiste ma non "decide", quindi art.22 GDPR non è attivato; comunque, il cliente deve poter sempre richiedere di parlare con umano (questo potrebbe configurarsi quasi come un "diritto analogo" in termini di user experience, anche se non giuridicamente basato su art.22). La DPIA dovrebbe coprire anche il rispetto del **segreto bancario**: le conversazioni con l'AI contengono dati confidenziali, l'algoritmo e i suoi manutentori non devono violare la riservatezza (es. se si usa un servizio cloud esterno per NLP, è trasferimento dati all'estero? bisogna valutare).

**Obblighi/controlli aggiuntivi:** Al di là di AI Act/GDPR, si applicano normative consumer: il **Codice del Consumo** richiede che pratiche commerciali siano corrette e non ingannevoli - il chatbot deve dare info veritiere, aggiornate e comprensibili. Se il bot propone contratti (es. un prestito) potrebbe configurarsi come comunicazione commerciale o offerta contrattuale: va quindi conforme a regole di trasparenza precontrattuale (Fogli informativi, ECC…). È improbabile lasciare a un'AI generativa la spiegazione di condizioni di un mutuo senza supervisione, ma se lo facesse, la banca ne resta responsabile. In Italia, normative come l'art. 8-ter TUB (per l'assistenza ai clienti bancari) implicano che i sistemi di risposta non umani non aggravino gli obblighi di risposta entro tempi definiti ai reclami: quindi se l'AI gestisce parzialmente i reclami, deve rispettare procedure. Dal punto di vista ICT, le linee guida Banca d'Italia in materia di sicurezza e continuità richiedono che i sistemi - compresi quelli di front-end innovativi - abbiano adeguati piani di continuità: un contact center AI deve avere fallback (es. se il bot non capisce o è offline, deve subentrare un operatore umano o un messaggio di scuse con invito a canali alternativi). Se l'AI viene "addestrata" con trascrizioni di conversazioni passate, bisogna valutare base giuridica per riutilizzare quei dati di clienti - tipicamente lo si fa anonimo/aggregato, altrimenti serve includere nell'informativa privacy che i dati di contatto cliente potranno essere usati per migliorare i servizi AI.

**Misure di oversight e monitoraggio:** Anche se questo caso d'uso non è high-risk, _human-in-the-loop_ è comunque importante per qualità di servizio. Best practice: predisporre che **almeno il X% delle conversazioni** (specie quelle su operazioni dispositive) siano revisionate a posteriori da personale quality control, per correggere eventuali risposte scorrette fornite dall'AI e migliorare il sistema. Implementare metriche di monitoraggio continuo: tasso di richieste non comprese (fallback to human), tasso di feedback negativi/rating basso da clienti post-chat, tipologie di errori comuni. Queste metriche vanno riportate in dashboard al responsabile customer care e al _team AI governance_. Oversight in tempo reale: se l'utente esprime frustrazione o inserisce parole chiave (es. "voglio parlare con operatore"), il sistema deve immediatamente trasferire la chat/call a un umano. Inoltre, addestrare specificamente il personale di secondo livello su come rilevare se il bot ha commesso errori informativi, in modo da rettificare subito le info date al cliente. Dal lato tecnico, per modelli generativi è utile inserire _validazione_ dell'output: es. se il cliente chiede saldo conto, il bot deve usare l'API interna e restituire il numero esatto, non "inventare"; per questo, architetture come Retrieval-Augmented Generation (RAG) possono essere adottate, garantendo che l'AI fornisca solo risposte basate su fonti certe (base dati bancaria). L'AI Act richiede trasparenza logging: si dovrebbero conservare le chat e registrazioni (previo info al cliente) per eventuali audit - questo già avviene per i call center tradizionali (registrazioni a fini qualitativi).

**Esempi pratici:** Molte banche hanno lanciato chatbot: es. _Intesa Sanpaolo_ con il suo virtual assistant in app, _UniCredit_ con il chatbot sul sito, etc. Un caso interessante è **Widiba** (banca online italiana) che nel 2018 lanciò un assistente virtuale vocale ("Widdy") integrato con smart speaker, ma lo ritirò dopo poco: si era rilevato che i clienti preferivano interfacciarsi via chat o app e c'erano dubbi su privacy (dialoghi su device come Alexa). Questo insegna di testare l'aderenza del canale AI alle preferenze utente e percezione di fiducia. Sul fronte normativo, la **Garante Privacy spagnolo (AEPD)** ha pubblicato linee guida sui chatbot indicando che devono informare chiaramente l'utente e che l'utente mantenga diritti GDPR pieni (incluso sapere se sta parlando con AI). Inoltre alcuni casi mediatici, es. il chatbot di _Bank of America_ ("Erica"), hanno mostrato che la chiave del successo è definire bene l'ambito: Erica risponde a query finanziarie ma non dà consigli d'investimento personalizzati (per evitare rischi compliance MiFID). Quindi restringere il _scope_ dell'AI e mettere confini (il nostro contact center AI potrebbe essere configurato per non rispondere su temi sensibili o legali, indirizzando subito ad umano).

## **Use Case 3: Anti-Money Laundering e Monitoraggio Transazioni**

**Descrizione:** Utilizzo di sistemi IA (in particolare machine learning, anche deep learning) per analisi di transazioni finanziarie e individuazione di attività sospette legate a riciclaggio di denaro o frodi. Tradizionalmente le banche usano regole fisse e scenari predefiniti (es. importi soglia, determinati pattern noti) nei sistemi AML. L'IA avanzata promette di rilevare anche schemi anomali non predefiniti ("_unknown unknowns_") tramite algoritmi di anomaly detection o classificazione addestrati su dati storici di segnalazioni. Ad esempio, un modello può apprendere il profilo di operatività normale di un cliente e segnalare deviazioni atipiche. Oppure può correlare dati di diverse fonti (movimenti, profili di rischio, info OSINT) per alzare alert mirati.

**Rilevanza regolamentare:** _Non classificato come high-risk AI Act,_ perché l'Allegato III copre solo usi di IA in attività di _law enforcement_ pubblico. L'AML sta in un'area ibrida: la _Direttiva AML_ impone agli enti obbligati (banche) controlli su transazioni e segnalazione di sospetti all'UIF (Unità di Informazione Finanziaria), ma l'AI Act non considera la banca come autorità di contrasto. Anzi, il cons. 59 specifica che le unità di informazione finanziaria con compiti amministrativi di analisi non sono incluse negli usi ad alto rischio di polizia[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo). Quindi, l'impiego di AI per AML rientra nei sistemi "non high-risk" (a meno che si usi in collaborazione diretta con forze dell'ordine per indagini, scenario fuori standard). Ciò significa niente obbligo di certificazione ex ante AI Act né FRIA obbligatoria _per legge_. Tuttavia, da un punto di vista di impatto sui diritti, un algoritmo AML può congelare temporaneamente operazioni o segnalare clienti all'autorità, con potenziali gravi effetti (blocco conto, indagini) - quindi _la banca prudentemente dovrebbe condurre una valutazione di impatto_ (FRIA volontaria o simil-DPIA estesa) per assicurarsi che il sistema non violi diritti di individui inconsapevoli. Inoltre, l'AI Act vieta espressamente l'uso di IA per _profilazione massiva in contesto di polizia_ e altre pratiche invasive: un sistema AML borderline (che profilasse tutti i clienti in chiave criminale) deve comunque rispettare principi di _necessità e proporzionalità_.

**Privacy & DPIA:** Molto probabile che sia necessaria una DPIA. Monitorare sistematicamente le transazioni di tutti i clienti alla ricerca di illeciti è un esempio classico di _"sorveglianza sistematica su larga scala"_[\[54\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=profilazione,del%20volume%20dei%20dati), combinata con profiling comportamentale potenzialmente anche di dati sensibili (es. bonifici a organizzazioni religiose, spese mediche possono rivelare dati sensibili). Il Garante francese (CNIL) ha in passato indicato che i sistemi antiriciclaggio rientrano nelle liste di trattamenti da valutare. Il Garante italiano nelle sue 12 categorie menziona _"trattamenti per prevenire frodi"_ e _"interconnessioni di dati di consumo con dati di pagamento"_[\[55\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=8,personali%20raccolti%20per%20finalit%C3%A0%20diverse)[\[56\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,ovvero%20della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di) - entrambi attinenti all'AML - come casi da esaminare. Quindi DPIA sì, focalizzata su: liceità (banca ha obbligo legale di AML, quindi base giuridica è adempimento di obbligo di legge - art.6(1)(c) GDPR, esente da consenso), minimizzazione (usare solo i dati necessari per pattern: però AML tende a _massimizzare_ dati per scoprire correlazioni - trovare equilibrio), limitazione conservazione (dati di allerta non confermati vanno cancellati dopo un tempo, etc.). La DPIA deve considerare anche _falsi positivi_: un elevato numero di segnalazioni non giustificate può ledere la reputazione e i diritti dei clienti (che possono subire disservizi). Andrebbe quindi predisposta una misura per mitigare ciò (es. threshold ragionevoli per allarmi, verifica umana prima di segnalare all'UIF). Dal punto di vista privacy, l'AML sta in equilibrio con normative antiriciclaggio: GDPR consente trattamento senza consenso e eventuali limitazioni dei diritti (es. diritto informazione può essere limitato per non allertare il sospetto, secondo art.23 GDPR attuato da D.Lgs 231/2007). Quindi al cliente non verrà detto "ti stiamo profilando per AML", e ciò è lecito per legge - ma internamente la banca deve monitorare attentamente gli accessi a questi dati e garantire che non si ecceda (logiche di _"data protection by design"_ qui complesse: serve tracciare chi vede gli alert, etc.).

**Obblighi/controlli aggiuntivi:** Fortissima regolamentazione settoriale: D.Lgs. 231/07 (recepimento AMLD) richiede che la banca applichi approccio basato su **rischio** (risk-based approach) nella sua funzione antiriciclaggio. L'uso di modelli avanzati è incoraggiato implicitamente, purché la banca ne capisca i risultati e li inserisca nella propria valutazione rischio. Le autorità di vigilanza (UIF, Banca d'It) si aspettano che l'utilizzo di strumenti informatici non sostituisca il giudizio del compliance officer AML. Ci sono obblighi di _conservazione_ (tenere traccia di tutte le segnalazioni e analisi 10 anni) e di _riservatezza_ (non rivelare al cliente segnalato). Un algoritmo AI che segnala in automatico all'UIF andrebbe calibrato per minimizzare errori clamorosi - perché l'UIF poi può chiedere spiegazioni. Bisogna quindi poter spiegare (almeno internamente) perché il modello ha segnalato X -> questo è critico: molti modelli ML non sono facilmente spiegabili; la banca dovrà magari avere un modulo di _explanation_ per ogni alert (es. evidenziare la sequenza di transazioni anomala che ha portato al punteggio elevato). Dal punto di vista _auditing_, l'Organo di controllo interno e i revisori ispezioneranno il processo AML: occorre documentare metodologia AI, validazione fatta (per assicurare che non "buchi" qualche tipologia di rischio noto), e risultati. Se il sistema IA sostituisce parametri delle _Disposizioni Banca d'Italia_ su adeguata verifica semplificata vs rafforzata, etc., bisogna verificare di restare entro cornice normativa: l'AI può aiutare a attribuire automaticamente classi di rischio cliente (low/medium/high risk) basandosi su dati transazionali oltre che tipologia cliente, ma la metodologia dovrebbe essere validata e approvata dall'AML Officer e soggetta a update continuo.

**Misure di oversight e monitoraggio:** Approccio raccomandato: _AI as assistant, not decider_ in AML. Quindi: l'algoritmo genera _segnalazioni interne_ ("alerts"), ma una **analista AML umano le rivede tutte** e decide quali far confluire in Segnalazione di Operazione Sospetta ufficiale all'UIF. Questo già avviene con scenari statici (l'analista filtra molti falsi positivi generati dalle regole); con IA potrebbe cambiare volume e natura degli alert, ma il principio di doppio controllo rimane fondamentale (ed è richiesto dalla normativa AML di fatto). Inoltre, serve _monitoraggio dell'efficacia_ del modello: metriche come % di alert AI che diventano effettive SOS inviate (precision), % di SOS effettive originate dall'AI vs da altri canali (recall), e confronti con anni precedenti - se l'AI riduce drasticamente o aumenta troppo le SOS, va investigato. Oversight da parte del _Compliance Officer AML_: costui deve poter capire il sistema, avere possibilità di intervenire sui parametri (es. alzare soglia sensibilità se sta segnalando troppo, o aggiungere un controllo specifico se normativa cambia - se modello black box, prevedere meccanismi per integrarlo con regole aggiuntive). Implementare un processo periodico (annuale almeno) di revisione del modello con stakeholder: IT, Compliance, Data Scientist e anche rappresentanti legali per valutare se il modello sta rispettando requisiti e se emergono nuovi rischi (ad es. il modello potrebbe "imparare" a ignorare certe transazioni comuni ma potenzialmente lecite - rischio di _blind spots_).

**Esempi pratici:** Molte banche stanno valutando IA in AML: es. _ING_ ha dichiarato l'uso di ML per rilevare transazioni sospette migliorando il segnale, _Swedbank_ dopo uno scandalo di mancato rilevamento ha investito in AI. In Italia, la **Banca d'Italia** ha avviato dal 2021 un progetto (_"Gianos"_) di machine learning applicato ai dati aggregati di segnalazioni antiriciclaggio per aiutare l'UIF a identificare fenomeni nascosti - segno che anche le autorità vedono utile l'AI, ma con grande cautela. Un case concreto: _HSBC_ ha usato AI per incrociare dati di transazioni internazionali e social network nel famoso progetto _"FXogle"_, scoprendo pattern di frodi valutarie; tuttavia questo ha comportato questioni su privacy (utilizzo di dati personali non finanziari). Dal punto di vista normativo, un fatto interessante: nel Regno Unito il regolatore (FCA) ha organizzato _TechSprints_ sull'AML, evidenziando che i modelli black box sono problematici per auditors e preferendo modelli interpretabili (_white box AI_). Quindi, uno _shift_ possibile è privilegiare algoritmi più spiegabili (es. alberi decisionali, reti bayesiane) rispetto a deep learning puro, almeno finché normative richiedono spiegazioni. Per la sandbox italiana, nella prima finestra _Vidyasoft_ ha testato "Hands-Free SCA" per frodi pagamenti usando AI[\[57\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20un%20servizio%20evoluto,durata%20massima%20di%2018%20mesi) - ambito leggermente diverso (Strong Customer Authentication con esenzioni AI), ma affine: il report finale di sandbox (seconda finestra) non è pubblico, ma la lezione generale è che AI può operare se integrata con controlli come quelli sopra.

## **Use Case 4: Recruitment e Gestione HR con AI**

**Descrizione:** Applicazione di sistemi IA nel processo di selezione, assunzione e gestione del personale in banca. Include strumenti che fanno _screening automatico dei CV_ (ad esempio software che filtrano candidati in base a requisiti e punteggi, o che usano NLP per analizzare lettere motivazionali), sistemi di _video-interview analysis_ (algoritmi che valutano espressioni facciali, tono e linguaggio del candidato in video-colloqui), fino a AI che propongono una shortlist di candidati o addirittura forniscono un punteggio di idoneità. Altre applicazioni in ambito HR: tool per valutazione delle performance dei dipendenti, analisi predittiva di churn (chi potrebbe lasciare l'azienda), ottimizzazione turni. Qui ci concentriamo sull'assunzione e mobilità interna, essendo menzionata come area target.

**Rilevanza regolamentare:** _Alto rischio AI Act_. L'Allegato III §4 include _"sistemi di IA destinati a essere utilizzati per il reclutamento o la selezione delle persone fisiche, per decidere su assunzione, promozione, cessazione, allocazione di compiti, o valutazione in ambito lavorativo"_[\[58\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei). Chiaramente quindi un ATS (Applicant Tracking System) con AI rientra. Conseguenze: requisiti AI Act pienamente applicabili (dati di training da controllare per qualità e bias, documentazione tecnica sul modello, misure di sicurezza etc.). Il fornitore del software ATS dovrà curare la certificazione CE del sistema come conforme. La banca come utente dovrà svolgere la FRIA prima di usare tali sistemi su candidati/lavoratori[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti), dato che deployer di sistemi di Allegato III punto 4 sono presumibilmente compresi (in quanto datori di lavoro). Inoltre, l'AI Act mette l'accento sui rischi di discriminazione in ambito occupazionale: i considerando notano come tali sistemi possano replicare discriminazioni storiche (contro donne, fasce d'età, etnie) e incidere sulle opportunità di vita[\[59\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=lavoratori,I%C2%A0sistemi%20di%20IA). Quindi la FRIA dovrà in particolare valutare fairness e impatto su diritti del lavoro (es. diritto alla pari opportunità, non-discriminazione in fase di selezione, dignità). Segnaliamo anche che alcune pratiche AI HR possono incrociarsi con divieti: _es._ l'AI Act **vieta** l'uso di sistemi di riconoscimento emotivo nel contesto lavorativo[\[60\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=will%20be%20banned%20from%20the,of%20predictive%20policing%20for%20individuals) - ciò riguarda ad esempio software che analizzano le emozioni durante test o sul posto di lavoro (questo è _banned_, quindi la banca non potrebbe lecitamente usare un sistema che monitora emozioni dei dipendenti per valutarli). Altra pratica proibita: _social scoring generalizzato_ - non comune in HR, ma significa evitare sistemi che profilano la "affidabilità" del lavoratore su parametri non pertinenti lavorativi (es. vita privata).

**Privacy & DPIA:** Obbligatoria al 99%. Trattare dati dei candidati o dipendenti con sistemi automatizzati di valutazione rientra nel profiling e monitoraggio. Il GDPR, art.88, consente Stati membri di prevedere garanzie per trattamenti in ambito lavoro: il D.Lgs 101/2018 in Italia richiama l'art. 113 del Codice Privacy, che affida al Garante definire misure. Il Garante italiano e l'EDPB hanno chiarito che decisioni automatizzate in contesto HR sono particolarmente delicate: di norma serve il _consenso_ o altra base e non devono violare statuto lavoratori (art. 4 legge 300/1970 su controlli). Una DPIA valuterà quindi: base giuridica (es. consenso del candidato a screening automatico, oppure legittimo interesse? Consenso rischia di non essere "libero" in contesto lavoro; in genere si preferisce base legittimo interesse bilanciato con garanzie); necessità - la banca deve dimostrare che l'AI migliora l'obiettività e efficienza senza ledere diritti; rischi di errori (scartare il candidato ideale erroneamente) e come rimediare (ex: far comunque visionare un campione di CV scartati a recruiter umani per controllo). Importante: se c'è _decisione automatizzata pura_ su un candidato (ad es. "il sistema esclude Tizio senza intervento umano"), questo attiva art.22 GDPR - tipicamente, i datori di lavoro in EU evitano di avere decisione solo automatica, includendo un HR nella decisione finale di chi invitare a colloquio, proprio per conformità. Se per assurdo la banca volesse decisione fully automated (pochissimo probabile per rischi reputazionali e normativi), dovrebbe ottenere consenso esplicito dei candidati interessati - scenario complesso (il candidato può sentirsi obbligato a darlo). Inoltre, andrebbe garantito il diritto di ottenere un intervento umano (un recruiting manager che riconsideri la candidatura su richiesta). Dal lato dati: CV e cover letter spesso contengono _dati personali delicati_ (foto = dati biometrici se usato face recognition; testo può rivelare origine etnica, età, stato familiare, che sono aspetti sensibili in HR per normative antidiscriminatorie; anche eventuali dati sanitari o penali se il candidato li menziona). Un'AI nel leggerli deve ignorare info irrilevanti e la banca deve configurare il sistema per non tenere conto di attributi protetti. Questo può implicare tecniche di _bias mitigation_, ad esempio far analizzare CV anonimi (rimuovendo nome, genere, età). La DPIA dev'essere coordinata con le normative lavoro: lo Statuto dei Lavoratori art.4 vieta controlli a distanza non accordati su dipendenti - se l'AI fosse usata per monitorare performance (ad es. analisi email, chat, produttività), serve accordo sindacale o autorizzazione. Nel recruiting, ciò non si applica ai candidati (non sono dipendenti ancora), ma nel mobility interna sì (un dipendente che concorre ad altra posizione - i suoi dati usati da AI vanno tutelati come dati dipendente).

**Obblighi/controlli aggiuntivi:** Norme antidiscriminatorie in assunzione: in UE e Italia è vietata discriminazione diretta o indiretta per sesso, età, etnia, religione, disabilità, orientamento, ecc. Un sistema AI recruiting è soggetto a tali leggi (D.Lgs 216/2003 in IT attuazione direttiva eguaglianza). Se venisse fuori che l'algoritmo di recruiting esclude sistematicamente persone di una certa fascia d'età o genere, la banca potrebbe essere citata per discriminazione, anche se il bias è "non intenzionale" (il concetto di _disparate impact_ si applicherebbe). Quindi come obbligo derivato c'è: testare e assicurare che i criteri di AI siano job-related e non escludano categorie protette in percentuale sproporzionata, a meno di giustificato motivo. Ad esempio, un algoritmo che penalizza gap lavorativi lunghi potrebbe svantaggiare le donne (maternità): va calibrato o giustificato come essenziale. Inoltre, la banca deve rispettare obblighi di trasparenza verso i candidati: il Decreto Trasparenza Lavoro (D.Lgs 104/2022) recependo la direttiva UE 2019/1152, _art. 4, comma 5-6_, impone al datore di informare il lavoratore (o candidato, se interpretabile estensivamente) sull'uso di sistemi decisionali o di monitoraggio automatizzati in fase preassuntiva o nel rapporto. In particolare, se si usano strumenti algoritmici che incidono su assunzione, il candidato/dipendente ha diritto a essere informato del funzionamento di tali sistemi, dei parametri principali e degli obiettivi perseguiti, nonché delle logiche di valutazione. Questo è importantissimo: dal agosto 2022 in Italia i datori devono consegnare un'informativa scritta _anche per gli algoritmi in recruiting_ e permettere al lavoratore di chiedere chiarimenti. L'AI Act aggiungerà registrazione nel database (se ente pubblico, quell'uso di AI andrà reso noto). Quindi c'è un intreccio di obblighi: la banca dovrà predisporre una sorta di "scheda trasparenza algoritmo di selezione" da fornire se richiesto (un'anticipazione di quell'obbligo sta già spingendo aziende a rivelare se usano AI in HR). Oltre a questo, normative privacy collegate: se il sistema fa analisi video (biometria facciale su colloqui video), serve o consenso specifico o base giuridica forte e autorizzazione Garante, perché dati biometrici e giudiziari (casellario) sono soggetti a regimi speciali.

**Misure di oversight e monitoraggio:** Principio cardine: _decisione finale umana_. L'AI può rankare i CV, ma un recruiter deve poter modificare la graduatoria, includere candidati che l'AI ha escluso se li ritiene validi (_override_), e documentare perché. Ogni tanto, il team HR dovrebbe fare controlli qualitativi: prendere un campione di CV scartati e vedere se c'erano candidati potenzialmente validi scartati ingiustamente - se sì, analizzare il motivo (feature del modello che ha penalizzato quell'aspetto). Ad esempio, scoprire che l'AI penalizza chi vive a >50km dalla sede (supponendo minore probabilità di accettare il lavoro) - è lecito? Potrebbe essere discriminatorio verso chi abita in Sud Italia candidandosi a Nord (indiretto su origine). Serve allora rimuovere o attenuare quel criterio. Quindi, predisporre un processo di _bias audit periodico_, magari coinvolgendo anche il Diversity Manager aziendale. In fase di implementazione, assicurarsi che il modello sia addestrato su dati "puliti": se i dati storici riflettono un bias (es. in passato l'azienda assumeva pochissime donne in IT), il modello lo replicherà. Potrebbe essere utile applicare tecniche di re-balancing (dare peso maggiore ai casi minoritari). L'oversight può anche essere esterno: informare i sindacati o il Comitato Unico di Garanzia interno sull'utilizzo dell'AI in selezione, e concordare principi (ad esempio, un principio aziendale potrebbe essere: "nessun candidato viene mai escluso solo dall'algoritmo, se non dopo verifica umana"). Monitoraggio continuo: misurare se post-implementazione è cambiata la composizione del personale assunto (es. l'AI involontariamente ha alzato o abbassato la diversità?). Se emergono trend negativi, intervenire. Dal lato sicurezza/protezione dati: loggare gli accessi al sistema di recruiting AI (chi ha consultato i profili, ecc.), e garantire che dopo conclusa la ricerca i dati dei candidati non assunti siano conservati solo per il tempo lecito (di solito max 6-12 mesi salvo consenso per opportunità future). L'IA Act inoltre richiederà all'utente (banca) di registrare l'uso dell'AI HR nel database (se soggetto pubblico o che fornisce servizi pubblici - le banche non lo sono, ma se partecipassero a programmi pubblici di job placement forse; comunque la FRIA risulterà documentata).

**Esempi pratici:** Caso famoso: **Amazon Recruiting Tool** - un algoritmo che dava punteggi ai CV per ruoli IT, fu dismesso perché si scoprì essere sessista (aveva appreso da dati storici che la maggior parte degli assunti erano uomini e penalizzava parole chiave femminili nei CV)[\[61\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=I%20rischi%20non%20sono%20teorici,economiche%20e%20sociali%20che%20comportano). Questo è citato spesso come monito: feed di training bias → output bias. Dopo quell'episodio, molte aziende hanno frenato sull'AI HR o puntato a modelli trasparenti. In UE, la start-up _HireVue_ che offriva video-interview AI fu criticata da autorità e advocacy groups, portando l'azienda a rimuovere l'analisi facciale/emotiva e mantenere solo analisi del contenuto verbale (meno intrusiva). La **Germania** ha un approccio cauto: il Betriebsrat (consiglio lavoratori) spesso chiede di approvare l'uso di software di selezione automatizzata. Con l'AI Act in arrivo, grandi gruppi come Deutsche Telekom hanno pubblicamente affermato di stare testando i propri algoritmi HR per eliminare possibili pregiudizi e di predisporre _"Algorithmen TÜV"_ (certificazione). In Italia, alcune banche hanno iniziato ad usare test attitudinali online con scoring automatizzato e poi HR li usano come uno dei fattori: questa mitigazione (AI come "punteggio aggiuntivo" e non unico fattore) è considerata più sicura. Dal 2023, con il Decreto Trasparenza, abbiamo visto alcune aziende includere nell'informativa ai candidati frasi come: "il Suo CV potrà essere sottoposto a pre-analisi mediante sistemi automatizzati, ferma restando valutazione finale da parte di recruiter" - segno che già ora la disclosure è necessaria. Probabilmente assisteremo in audit futuri a controlli su come l'AI decide in HR: la banca farebbe bene ad essere in anticipo, documentando tutto (ad esempio tenendo i _dati di training del modello HR_ archiviati e l'analisi di impatto, così da poterli esibire se il Garante o l'ispettorato lavoro chiedessero chiarimenti su criteri di selezione).

## **Use Case 5: KYC e Verifica Documentale (onboarding clienti)**

**Descrizione:** Sistemi IA impiegati nel processo di adeguata verifica della clientela (_Know Your Customer_) e nell'autenticazione/validazione di documenti. Tipicamente, durante l'onboarding digitale di un nuovo cliente, vengono raccolti documenti di identità, foto/selfie, eventualmente video; l'IA può essere usata per: riconoscere il tipo di documento e leggerne automaticamente i dati (OCR intelligente), verificare l'autenticità del documento (document analysis AI che controlla pattern di sicurezza, font, foto vs ologrammi, etc.), confrontare la foto del documento con il selfie del cliente (**face matching biometrico** per assicurare che sia la stessa persona), controllare in background liste sanzioni o PEP (anche qui con algoritmi name-matching), e validare in generale se i dati forniti soddisfano requisiti normativi. Inoltre, l'IA può essere usata per estrarre informazioni da documenti come buste paga, bollette, visure camerali presentati dal cliente, semplificando la due diligence.

**Rilevanza regolamentare:** _Non espressamente in high-risk AI Act_, a meno che includa elementi biometrici con finalità riconducibili a controllo pubblico. L'uso di **biometria per verifica identità** in contesto privato (onboarding bancario) non è classificato in Allegato III come tale. L'AI Act tratta la biometria per identificazione remota in luoghi pubblici da parte forze ordine (un'altra cosa, spesso vietata salvo eccezioni) e la _verifica di documenti_ è menzionata come esclusa per contesto migrazione[\[62\]](https://artificialintelligenceact.eu/annex/3/#:~:text=,the%20verification%20of%20travel%20documents). Quindi, un sistema di face matching 1:1 (confronto selfie vs foto documento) non rientra tra high-risk secondo lettera stretta. Tuttavia, c'è margine interpretativo: alcuni potrebbero sostenere che garantire l'identità certa è _essenziale per accesso a servizi finanziari_, e un errore qui può avere impatto su sicurezza (es. account takeover). Ma l'AI Act non l'ha incluso in categoria 5 (che copre credito, assicurazioni, etc.). Pertanto, niente obbligo di certificazione high-risk per il software di face recognition usato dall'istituto, né FRIA obbligatoria per legge. _Tuttavia_, va considerato che in KYC si toccano dati molto sensibili (documenti identificativi, biometria facciale) e diritti fondamentali come privacy e diritto all'identità personale - una _FRIA volontaria_ sarebbe opportuna per valutare come l'AI incide (es. rischio di esclusione: un sistema di verifica documento mal calibrato potrebbe respingere clienti legittimi, magari più spesso appartenenti a certe nazionalità con documenti meno noti, creando disparità). Inoltre, l'AI Act vieta la categorizzazione biometrica per inferire dati sensibili (es. non si potrebbe usare la foto per dedurre etnia o età per scopi non necessari) - non è l'obiettivo del KYC, ma assicurarsi che il fornitore del software non faccia cose oltre la semplice verifica. In sintesi, KYC AI è **limited risk AI Act**, ma con obblighi generali di trasparenza se interagisce (qui il cliente sa di essere ripreso, quindi ok) e con forti requisiti privacy.

**Privacy & DPIA:** Quasi certamente sì. Il Garante ha incluso _"trattamenti sistematici di dati biometrici"_ tra quelli che richiedono DPIA[\[63\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=relativi%20a%20condanne%20penali%20e,della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di%20trattamento). Una procedura di video-identificazione con riconoscimento facciale rientra in questo (tratta volti su scala potenzialmente ampia). Anche l'OCR di documenti ID tocca dati identificativi critici (numero documento, ecc.) e può rientrare in "dati estremamente personali" (tra cui i documenti identità) su larga scala[\[64\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=tramite%20reti%20o%20di%20sorveglianza,Big). La DPIA valuterà: base giuridica (nella maggior parte dei casi, l'identificazione cliente è obbligo di legge AML, quindi base di legge art.6(1)(c) GDPR - il Garante italiano su SPID e CIE ha detto che il riconoscimento facciale può basarsi su obbligo normativo visto che la legge lo prevede come metodo possibile, oppure su interesse legittimo se non obbligatorio ma offerto come opzione di comodo); necessita di verificare che sia usato il _minimo necessario_: es. molti sistemi registrano un breve video dell'utente che mostra documento e fa movimenti (liveness detection), ciò genera molti dati biometrici - la DPIA analizza se alternative meno invasive esistono (es. solo foto + sms OTP? etc.). Attenzione particolare ai **consensi**: per dati biometrici, il GDPR richiede una base specifica (art.9); qui la base principale è l'adempimento di obbligo antiriciclaggio, che può coprire anche l'acquisizione foto e documento. Il Garante con provv. 2021 su riconoscimento facciale per FEA aveva richiesto consenso esplicito; per onboarding video, finora l'ha permesso come misura AML (es. la guida IVASS 2021 e circolari Banca d'It 2019 su video riconoscimento l'hanno consentito con misure). DPIA inoltre su: _accuratezza algoritmi_ - tasso di falso rifiuto, soprattutto per alcuni gruppi (studi mostrano che face recognition può avere errori più alti su persone di colore o molto giovani/anziane). Va considerato e mitigato: e.g. fornire opportunità alternative di identificazione a chi viene erroneamente non riconosciuto (come intervento manuale di un operatore). _Sicurezza:_ questi sistemi raccolgono dati ultrasensibili (video volti, immagini documenti) - protezioni cifratura forte, canali sicuri, e limitare l'accesso (solo personale compliance, per es.). _Conservazione:_ il video di identificazione va tenuto il tempo richiesto da normative AML (5 anni dalla fine rapporto) poi distrutto. _Diritti interessato:_ qui si scontra con AML - in genere il cliente non può opporsi all'identificazione (o accetta quell'onboarding o non apre conto), ma va informato adeguatamente sul processo (con informativa chiara che spiega c'è un'analisi automatica del documento e riconoscimento volto). Se la banca volesse fare decisione automatica ("la sua identificazione è fallita, contratto non aperto"), ricade in art.22 - mitigato dal fatto che in pratica se l'AI fallisce, c'è di solito fallback come "ripeti la procedura" o "vai in filiale". Comunque, includere intervento umano in caso di fallimento è fortemente consigliato per evitare problemi.

**Obblighi/controlli aggiuntivi:** La regolamentazione specifica di Banca d'Italia e IVASS consente la _video-identificazione a distanza_ a patto di seguire linee guida tecniche (Circolare 285 agg. 2020 di Banca d'Italia, Provv. IVASS 97/2020). Queste linee guida spesso prescrivono: uso di sistemi di riconoscimento di sicurezza adeguata (livello SPID 2 o 3), presenza di controlli di _liveness_ (per evitare spoofing con foto o deepfake), conservazione delle registrazioni, intervento umano in casi dubbi. Quindi la banca deve assicurare che l'AI usata soddisfi tali requisiti. Per esempio, per liveness si usa AI che chiede all'utente di girare la testa o leggere 3 cifre: la normativa chiede che se l'algoritmo non è sicuro al 100%, un operatore ricontrolli. Inoltre, normative antiterrorismo e antifrode: la banca deve verificare il documento con liste di documenti rubati/perduti (spesso integrato nell'AI doc analysis). Il sistema deve generare evidenze per eventuali ispezioni: se la banca viene accusata di aver identificato male un truffatore, deve poter mostrare log: "il sistema ha validato erroneamente il doc X; ecco come è successo". A tal fine, definire KPI di performance e soglie di ri-verifica manuale: es. se confidenza match volto-documento < 90%, far intervenire operatore anziché rifiutare o accettare alla cieca. In quanto non high-risk AI Act, non c'è obbligo di registrazione database, ma essendo legato ad antimoney laundering, la banca dovrà comunque notificare alla Banca d'Italia l'adozione di procedure innovative (spesso richiesto informalmente in vigilanza, per capire come rispetta requisiti).

**Misure di oversight e monitoraggio:** Qui oversight significa: assicurare che quando l'AI dice "documento valido, persona coincide" la decisione possa essere rivista se necessario. Molte banche già prevedono che un _operatore di backoffice_ ricontrolli un campione di pratiche approvate automaticamente dal sistema, soprattutto all'inizio (validazione umana a posteriori, per "tarare" l'AI). Se emergono falsi positivi (documenti falsi accettati), va immediatamente segnalato e il modello ri-addestrato o regole aggiuntive messe. Al contempo, monitorare i _falsi negativi_ (clienti reali scartati): es. se molti utenti con carta d'identità di un certo paese falliscono la verifica, forse l'AI non conosce bene quel documento → aggiungere campioni e migliorare. Implementare feedback loop: es. se utente contatta supporto perché non riesce a farsi riconoscere, quell'evento va registrato e la causa analizzata. L'oversight richiede competenze: il compliance o il responsabile onboarding deve poter interpretare i _confidence score_ del modello e decidere soglie. Ad esempio, decidere che sotto 80% di matching il caso è rifiutato, tra 80-90% lo vede un umano, sopra 90% auto-OK. Queste politiche vanno riviste periodicamente in base ai risultati reali. Un altro controllo: integrazione con antifrode - se poi risulta un account aperto era fraudolento, fare _post-mortem_ per capire se l'AI avrebbe dovuto rilevarlo (magari il truffatore ha usato un documento vero ma non suo, e l'AI ha fallito l'abbinamento? Studiare e migliorare). Quindi un team (IT + compliance + risk) dovrebbe rivedere statistiche mensili: numero di onboarding effettuati automaticamente vs manuali, errori segnalati, etc. Documentare questi controlli per eventuali audit (anche interni di funzione Antiriciclaggio).

**Esempi pratici:** In Italia quasi tutte le banche che offrono apertura conto online usano tecnologie di verifica documenti e face recognition (fornite da vendor come Experian, Onfido, InfoCert, etc.). Il Garante Privacy ha approvato linee guida SPID/CIE che usano riconoscimento facciale per identificare soggetti a distanza, fissando cautele: in particolare richiede che l'algoritmo sia altamente accurato e che all'utente sia data un'alternativa se il riconoscimento fallisce (es. andare in presenza o procedura con operatore in videochat). Questo parallelamente vale in banca: spesso se il self onboarding fallisce 2-3 volte, si viene contattati da un operatore o invitati in filiale. Un caso di cronaca: _2019, truffe conti online_ - criminali sfruttavano falle in sistemi di video riconoscimento usando volti di altri simili; questo ha spinto Banca d'Italia a rafforzare indicazioni su liveness detection. Dal lato customer experience, _Intesa Sanpaolo_ introdusse il riconoscimento facciale in filiale per pre-compilare dati da documento: fu autorizzato dal Garante con garanzie (dati non conservati dopo uso). Di nuovo, dimostra fattibilità ma con DPIA presentata. Sul mercato, errori famosi di Face Recognition su differenti etnie (es. più alti falsi negativi su volti con pelle scura in alcuni algoritmi) mettono in guardia: la banca deve chiedere al fornitore prove di test cross-demografici e magari condurre test interni sul proprio campione clienti (considerando anche molte banche hanno clienti stranieri). A livello AI Act, anche se KYC AI non è high-risk di per sé, il _"Digital Identity Wallet"_ europeo in arrivo e regolamenti eIDAS correlati standardizzeranno i processi di identità: le banche dovranno assicurare che i loro sistemi AI si integrino con questi e rispettino standard certificati (es. qualificazione dei servizi di identità: è probabile che i fornitori di soluzioni di video onboarding cercheranno schemi di certificazione volontaria). Quindi conviene già orientarsi a soluzioni AI KYC di provider noti che seguono standard di settore (ISO/IEC 30107 per anti-spoofing, ecc.). La _sandbox Fintech MEF 2021_ ha ammesso un progetto O-KYC[\[39\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi) basato su DLT per condividere info KYC: pur non centrato sull'uso di AI, indica l'interesse innovazione in questo campo. Un futuro scenario è usare AI per aggiornamento continuo KYC (monitorare anomalie nei documenti nel tempo, etc.), che si legherà al case AML.

# Questioni Residue Aperte

- **Criteri pratici per "rischio significativo" (Art.6(3) AI Act):** non è chiaro come un fornitore o utilizzatore potrà dimostrare che un sistema rientrante in Allegato III **non** presenta un _rischio significativo_ e quindi esentarlo dal regime high-risk[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III). Serviranno linee guida su metriche di rischio e chi avalla tale auto-valutazione, per evitare arbitri e under-classification.
- **Coordinamento tra Autorità (AI Act vs privacy vs settore):** chi sarà in pratica l'autorità di riferimento per vigilare sui sistemi AI bancari? Il _Market Surveillance Authority_ per high-risk AI potrebbe essere Banca d'Italia (vigilanza bancaria) o un nuovo soggetto; il Garante Privacy manterrà ruolo su DPIA e data protection; l'EBA/ECB avranno voce tramite l'_AI Board_. Servono protocolli per evitare conflitti o vuoti di competenza nelle valutazioni ex ante (FRIA) e controlli ex post.
- **Standard di riferimento e certificazioni:** dato il forte carattere tecnico dei requisiti AI Act, ci si chiede quali standard tecnici adotteranno le banche per dimostrare conformità (es. ISO 42001 AI Management? certificazioni di bias/fairness?). EBA nota che Commissione emanerà linee guida classificazione high-risk entro Feb 2026[\[65\]](https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/#:~:text=EBA%20Factsheet%20%E2%80%93%20AI%20Act%3A,banking%20and%20payments%20sector%2C%20by), ma sul piano operativo le banche vorrebbero un framework unificato. La domanda aperta: _conviene attendere standard ufficiali o partire con certificazioni volontarie (es. audit etici, attestazioni da terze parti) per stare avanti?_
- **Integrazione DPIA-FRIA:** come implementare praticamente un processo unico che copra entrambe? Si deve produrre due report separati (uno per Garante, uno per autorità AI Act) o un unico _"AI Risk Assessment Report"_ basterà per entrambi scopi? E in caso di valutazione d'impatto con esito dubbio (rischio residuo alto): per GDPR c'è obbligo di consultazione Garante, per AI Act non è prevista ma magari l'Autorità mercato può intervenire; bisognerebbe capire come allineare queste escalation.
- **Gestione dei fornitori terzi e liability:** se un vendor fornisce un sistema AI non conforme e la banca subisce una violazione (es. multa per discriminazione), su chi ricade la responsabilità? Il regime AI Act prevede responsabilità primaria del _provider_ per requisiti tecnici e del _user_ per uso improprio. Ma nei contratti reali tra banca e vendor serviranno clausole robuste su garanzie, indennizzi e accesso alle informazioni (audit). La questione aperta: le banche potranno chiedere contrattualmente ai vendor di condurre e condividere una FRIA da _provider_? Oppure ogni banca dovrà farla in solitudine anche per pacchetti standard?
- **Metriche di fairness e soglie accettabili:** i regolatori richiederanno alle banche di quantificare e mantenere certi livelli di fairness nei modelli (es. _"disparate impact ratio"_ non peggiore di 80%)? O rimarrà tutto qualitativo? L'assenza di criteri quantitativi univoci è un problema: una banca potrebbe considerare accettabile un leggero scostamento, un'altra no. Ci si chiede se l'EBA o l'AI Office elaboreranno guidance in tal senso.
- **Uso di dati sensibili per finalità etiche (bias correction):** paradosso noto - per testare se un modello è discriminante servirebbe a volte considerare la variabile protetta (es. genere) nei dati; ma ciò è vietato per decisione. Il Garante permetterà di usare dati sensibili simulati o raccolti post-assunzione per validare fairness? Su questo c'è incertezza e le banche faticano a definire metodologie di bias audit rispettose del GDPR.
- **Interoperabilità con regolamenti futuri (ESG, AI liability):** come si combineranno i requisiti AI Act con altri emergenti, ad esempio le iniziative sull'_AI liability_ (responsabilità civile per danni da AI) o la normativa ESG (che potrebbe includere uso etico di AI)? Le banche dovranno mappare anche questi aspetti - e rimane domanda aperta se la documentazione predisposta (es. registro eventi AI, log decisioni) potrà essere usata contro la banca in cause civili (un tema di liability non risolto: troppa trasparenza potrebbe esporre a contenziosi).
- **Ruolo dell'**AI Office **UE vs Autorità nazionali:** l'AI Office avrà poteri di supervisione soprattutto su fondation models e high-risk cross-border. Ma potrà emanare linee guida vincolanti anche per settori regolati? Le banche dovranno tenere un occhio a possibili indicazioni sovranazionali aggiuntive. La questione: se l'AI Office (Commissione) identifica un modello bancario come _"alto impatto"_ GPAI, potrebbe intervenire direttamente? Va chiarito.
- **Aggiornamento continuo del tool vs evoluzione norme:** riconoscendo che le interpretazioni e prassi attorno ad AI Act e DPIA evolveranno (giurisprudenza, orientamenti EDPB, nuove modifiche regolamentari), come mantenere il Navigator sempre aggiornato? Il processo di desk research ha limiti - alcune questioni saranno risolte solo con _regulatory feedback loop_ (es. prime FRIA effettivamente svolte, sanzioni inflitte, ecc.). È aperto il tema di governance del tool: chi in azienda (o consorzio ABI Lab) lo aggiornerà con le _lessons learned_ e nuove fonti normative man mano che emergono? Questo determina la sostenibilità a lungo termine della soluzione proposta.

# Bibliografia Commentata

- **Regolamento (UE) 2024/1689 "AI Act"** - Testo normativo fondamentale adottato nel giugno 2024[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti). Definisce il quadro regolatorio UE per l'IA con approccio basato sul rischio. _Rilevanza:_ Allegati e articoli citati identificano i casi bancari high-risk (credito, HR) e impongono obblighi (es. art. 14 oversight umano, art. 27 FRIA). Base per molte compliance action del Navigator.
- **Considerando AI Act (58) e (57)** - Parti introduttive del Regolamento che spiegano le motivazioni per includere credito e occupazione tra gli usi ad alto rischio[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[58\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei). _Rilevanza:_ evidenziano i rischi di discriminazione e impatto sociale di tali sistemi, fornendo giustificazione (da citare in FRIA) e ricordano eccezioni (fraud detection esclusa[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi)).
- **Garante Privacy, Provv. n. 467/2018 (Allegato DPIA)** - Delibera italiana che elenca i tipi di trattamento obbligatoriamente soggetti a DPIA[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Include scoring, decisioni automatizzate, biometria, monitoraggio lavoratori ecc. _Rilevanza:_ è il riferimento normativo nazionale per capire subito se un progetto AI bancario richiede DPIA. Il Navigator ne incorpora i criteri.
- **Linee Guida WP29/EDPB WP248 sulla DPIA** - Linee guida europee (2017, confermate EDPB 2018) che dettagliano criteri di rischio elevato e metodologia DPIA[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza). _Rilevanza:_ forniscono la checklist dei 9 criteri usata nel wizard per determinare obbligo DPIA e suggeriscono best practice su coinvolgimento stakeholder, aggiornamento DPIA, ecc.
- **Comunicato Consiglio UE 9 Dec 2023 (Accordo AI Act)** - Nota stampa del Consiglio[\[66\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=Transparency%20and%20protection%20of%20fundamental,rights) che annuncia l'accordo politico. _Rilevanza:_ contiene in linguaggio chiaro le novità come l'obbligo di FRIA per i deployer e l'estensione delle trasparenze (es. emotion recognition disclosure). Utile per estrapolare concetti chiave e spiegazioni non tecniche agli utenti.
- **EBA Risk Assessment Report, "Special topic AI" (Nov 2024)** - Rapporto dell'Autorità Bancaria Europea[\[67\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=Regarding%20use%20cases%2C%20AI%20is,banks%20are%20leveraging%20AI%20in)[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent). Descrive diffusione AI in banche EU, use case comuni (profilazione clienti, supporto, fraud detection, credit scoring) e relative sfide (skill gap, governance). _Rilevanza:_ offre dati e conferme sull'uso di AI nei 5 use case target e raccomandazioni (ad es. human-in-loop nel GPAI[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)). Il Navigator lo usa per tarare la sezione "Ambito di utilizzo nel settore" e per supportare raccomandazioni di prudenza.
- **Lettera EBA Chair José M. Campa (Nov 2025) - Mapping AI Act vs Regolamentazione bancaria**[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of)[\[68\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving) - Documento indirizzato alla Commissione che esamina sovrapposizioni tra obblighi AI Act e normativa finanziaria (CRR, CRD, MiFID, ecc.). _Rilevanza:_ conferma che molti requisiti (es. human oversight, data governance) hanno già equivalenti nelle regole esistenti, ma nessuna esenzione ad hoc è prevista. Aiuta il Navigator a evidenziare dove un adempimento AI Act può essere soddisfatto tramite compliance esistente e dove invece è addizionale.
- **Paradigma.it - "AI e concessione del credito: tra innovazione e responsabilità" (articolo Ott 2025)**[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario)[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano) - Approfondimento giuridico italiano. Sintetizza obblighi bancari (TUB 124-bis: merito creditizio con info adeguate; obbligo verifica umana) e principi delle EBA Guidelines 2020 sul loan origination (trasparenza, tracciabilità, non-discriminazione, supervisione costante) con commento Banca d'It 2022 sul ruolo ausiliario dell'AI[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). _Rilevanza:_ fornisce autorevole interpretazione locale su come bilanciare innovazione AI e responsabilità nelle decisioni credito. Citato nel Navigator per supportare raccomandazione "AI a supporto, non in sostituzione del giudizio umano" e inquadrare obblighi legali italiani di controllo umano.
- **Banca d'Italia - Quaderno Economia e Finanza n.721 "Intelligenza artificiale nel credit scoring" (2022)**[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove)[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in) - Studio empirico e regolatorio. Analizza benefici/rischi AI nel credito, copertura normativa e risultati di survey sulle banche italiane. Conclude che la normativa prudenziale vigente copre gran parte dei rischi AI-ML (governance, controlli), ma evidenzia lacune su principio di non-discriminazione e complessità di tradurre i principi etici in prassi[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove)[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti). _Rilevanza:_ questa fonte interna all'authority italiana rinforza l'importanza di misure di governance (che il Navigator enfatizza) e anticipa possibili richieste del regolatore su fairness. È utilizzata come base per suggerire nel wizard misure aggiuntive anche se non strettamente richieste dal testo di legge, in quanto "buona prassi prudenziale".
- **Garante Privacy - Provv. n. 755/2024 (caso sanzione IA generativa)**[\[42\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata)[\[43\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello) - Provvedimento sanzionatorio contro una nota società sviluppatrice di un chatbot generativo (presumibilmente OpenAI). Contesta mancata notifica data breach, carenza di base giuridica per training data personali, violazione trasparenza. _Rilevanza:_ benché non settore bancario, segnala posizioni stringenti del Garante su punti cruciali: obbligo di base giuridica identificata prima di usare dati per addestrare modelli, necessità di informare gli interessati anche in contesti innovativi. Il Navigator usa questo precedente per avvisare le banche di evitare simili lacune (es. se sviluppano modelli con dati cliente interni, badare a finalità dichiarate e informative).
- **Direttiva (UE) 2019/1152 ("Direttiva Trasparenza Lavoro") - D.lgs. 104/2022 (art.4)** - Norma che impone ai datori di informare i lavoratori sull'uso di sistemi decisionali automatizzati in ambito lavorativo. _Rilevanza:_ specificamente citata per il case HR: obbliga banche a disclosure verso candidati/dipendenti se impiegano AI in selezione o valutazione, con dettagli su logica, fattori e obiettivi. Il Navigator integra questo vincolo nelle raccomandazioni per l'uso case Recruiting (es. generazione di un'informativa AI per candidati).
- **D.Lgs 231/2007 (Normativa Antiriciclaggio) e Regole tecniche UIF/Banca d'It.** - Corpus normativo che disciplina KYC, adeguata verifica e controlli antiriciclaggio. Include l'obbligo di identificazione a distanza secondo regole precise e il principio di approccio basato su rischio. _Rilevanza:_ per use case AML/KYC, definisce il perimetro legale entro cui l'AI deve operare (non può abbassare gli standard di due diligence). Il Navigator ne ricorda gli obblighi (es. conservazione dati 5 anni, analisi manuale dei sospetti) integrandoli con quelli tecnologici.
- **ISO/IEC TR 24027:2021 (Bias in AI Systems)** e **NIST AI Risk Management Framework 1.0 (2023)** - Standard e framework internazionali (non normativi) che affrontano metodologie per valutare e mitigare bias nei sistemi AI e gestire i rischi AI. _Rilevanza:_ utili come riferimento tecnico per l'utente più avanzato; il Navigator li cita in bibliografia commentata come risorse per implementare concretamente le indicazioni di fairness e risk management (ad esempio: _"ISO 24027 fornisce tassonomia di bias e approcci per misurarli"_). Questo aiuta i team IT/Data Science a tradurre le raccomandazioni generali in azioni.
- **Linee Guida EBA 2020 on Loan Origination & Monitoring (EBA/GL/2020/06)** - Orientamenti vincolanti emanati dall'EBA[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). Richiedono, tra l'altro, che l'uso di modelli automatici nel processo di concessione crediti rispetti principi di non-discriminazione e incorporate human judgement. _Rilevanza:_ direttamente applicabili alle banche EU, completano il quadro normative credito. Il Navigator li incorpora nella scheda credito (es. sul dovere di spiegabilità e controllo umano) e come evidenza in evidence table sulla supervisione costante.
- **Circolare Banca d'Italia n.285 (agg. 2020), Disposizioni in materia di adeguata verifica a distanza** - Normativa secondaria italiana che permette l'identificazione via audio-video e stabilisce requisiti tecnico-procedurali (es. qualità video, conservazione registrazioni, intervento umano in caso di dubbi). _Rilevanza:_ definisce lo _standard minimo_ per use case KYC digitale. Il Navigator ne tiene conto raccomandando misure come liveness detection, verifica manuale se confidenza bassa, etc., conformi a tali disposizioni.
- **Documentazione Sandbox Regolamentare Italiana (2021-2023)** - In particolare: _Comunicato Banca d'Italia su esiti prima finestra sandbox_[\[69\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Informativa%20sulla%20conclusione%20della%20sperimentazione,di%20richiesta%20della%20certificazione%2C%20e)[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi). Descrive progetti Fintech innovativi testati, es. piattaforme di credit scoring e soluzioni KYC in DLT, evidenziando benefici e condizioni di successo ("soluzione idonea a operare fuori sandbox salvo rispetto di tutte le norme")[\[40\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione). _Rilevanza:_ fornisce **casi concreti italiani** da citare nelle schede use case, a dimostrazione di fattibilità e punti di attenzione emersi. Il Navigator li usa per dare confidenza (es. credit scoring AI testato con successo in sandbox) e sottolineare che compliance full rimane necessaria anche dopo innovazione.
- **Fonti EDPB/Europrivacy su AI & Privacy:** es. _EDPB Statement 2022 on the AI Act_, _EDPS Opinion on AI Act (June 2021)_. Queste esprimono posizioni dei garanti privacy europei sul progetto di AI Act (invocando rigore su FRIA, ban di alcuni usi) e sul rapporto con GDPR. _Rilevanza:_ benché anteriori al testo finale, ribadiscono l'importanza di non abbassare gli standard privacy nell'applicare AI (il Navigator ne tiene conto assicurando che DPIA e principi GDPR restino centrali). Inserite in bibliografia come approfondimento per chi volesse la prospettiva delle Autorità privacy sul regolamento AI (ad es. EDPS chiedeva FRIA obbligatoria per tutti gli high-risk - poi accolta).
- **Linee Guida OCSE & Commissione UE su AI Trustworthy (2019-2020)** - Queste non sono norme, ma contengono i 7 principi etici per IA affidabile (trasparenza, controllo umano, diversità/non discriminazione, responsabilità, ecc.). _Rilevanza:_ costituiscono il fondamento concettuale di molte obbligazioni poi normative. Il Navigator le menziona in background (es. nella evidence table su convergenza principi[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in)) e le indica in bibliografia come lettura di contesto per comprendere la filosofia della regolamentazione (es. perché human oversight è cruciale).

_(N.B.: Tutte le fonti primarie citate - leggi, linee guida EBA, provvedimenti Garante - sono da considerarsi autorevoli. Le fonti secondarie (articoli, studi) selezionate sono supportate da riferimenti ufficiali e servono a chiarire l'interpretazione pratica. Questo elenco finale contiene 18 voci, rimanendo entro il limite di 20 fonti.)_

[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al) [\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati) [\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi) [\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti) [\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista) [\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d) [\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica) [\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=4,d%27impatto%20sulla%20protezione%20dei%20dati) [\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la) [\[15\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario) [\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo) [\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III) [\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato) [\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario) [\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in) [\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana) [\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al) [\[45\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=usati%20nel%20settore%20elencati%20nell%27allegato%C2%A0III%2C,una%20valutazione%20dell%27impatto%20sui%20diritti) [\[46\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,coinvolgimento%20di%20un%20organismo%20notificato) [\[47\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=match%20at%20L7793%20alto%20rischio,dati%20dell%27UE%20di%20cui%20all%27articolo%C2%A071) [\[58\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei) [\[59\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=lavoratori,I%C2%A0sistemi%20di%20IA) Regolamento - UE - 2024/1689 - EN - EUR-Lex

<https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689>

[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza) [\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento) [\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento) [\[52\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=7,la%20concessione%20di%20un%20finanziamento) [\[53\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=particolari%20misure%20di%20carattere%20organizzativo,compresi%20i%20trattamenti%20che%20prevedono) [\[54\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=profilazione,del%20volume%20dei%20dati) [\[55\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=8,personali%20raccolti%20per%20finalit%C3%A0%20diverse) [\[56\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,ovvero%20della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di) [\[63\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=relativi%20a%20condanne%20penali%20e,della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di%20trattamento) [\[64\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=tramite%20reti%20o%20di%20sorveglianza,Big) La valutazione d'impatto

<https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto>

[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario) [\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario) [\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori) [\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano) [\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi) [\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano) [\[61\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=I%20rischi%20non%20sono%20teorici,economiche%20e%20sociali%20che%20comportano) AI e concessione del credito: tra innovazione e responsabilità

<https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/>

[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in) [\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti) [\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove) [\[37\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono) bancaditalia.it

<https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf>

[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent) [\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences) [\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants) [\[67\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=Regarding%20use%20cases%2C%20AI%20is,banks%20are%20leveraging%20AI%20in) Special topic - Artificial intelligence | European Banking Authority

<https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence>

[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system) [\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons) [\[51\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=For%20some%20uses%20of%20AI%2C,of%20predictive%20policing%20for%20individuals) [\[60\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=will%20be%20banned%20from%20the,of%20predictive%20policing%20for%20individuals) [\[66\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=Transparency%20and%20protection%20of%20fundamental,rights) Artificial intelligence act: Council and Parliament strike a deal on the first rules for AI in the world - Consilium

<https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/>

[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=%E2%80%A2%20CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving) [\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of) [\[48\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving) [\[68\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving) eba.europa.eu

<https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf>

[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi) [\[39\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi) [\[40\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione) [\[41\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=ICCREA%20Banca%20e%20Banca%20Monte,una%20futura%20commercializzazione%20della%20soluzione) [\[44\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=intermediari%20finanziari%20per%20offrire%20loro,durata%20massima%20di%2018%20mesi) [\[49\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=finalizzato%20a%20fornire%20alle%20banche,gestione%20del%20rischio%20di%20credito) [\[50\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=La%20sperimentazione%20%E2%80%93%20nell%27ambito%20della,operare%20al%20di%20fuori%20dell%27ambiente) [\[57\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20un%20servizio%20evoluto,durata%20massima%20di%2018%20mesi) [\[69\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Informativa%20sulla%20conclusione%20della%20sperimentazione,di%20richiesta%20della%20certificazione%2C%20e) Banca d'Italia - Progetti ammessi alla prima finestra temporale - sperimentazione conclusa

<https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html>

[\[42\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata) [\[43\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello) Privacy e sviluppo sistemi di IA: trattamento illecito di dati personali - DB

<https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/>

[\[62\]](https://artificialintelligenceact.eu/annex/3/#:~:text=,the%20verification%20of%20travel%20documents) Annex III: High-Risk AI Systems Referred to in Article 6(2) | EU Artificial Intelligence Act

<https://artificialintelligenceact.eu/annex/3/>

[\[65\]](https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/#:~:text=EBA%20Factsheet%20%E2%80%93%20AI%20Act%3A,banking%20and%20payments%20sector%2C%20by) EBA Factsheet - AI Act: Implications for the EU banking and ...

<https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/>
