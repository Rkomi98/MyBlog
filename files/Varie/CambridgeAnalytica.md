# Da Cambridge analytica ai chatbot: quanto √® a rischio la nostra privacy? 

## Abstract

La vicenda **Cambridge Analytica (CA)** ha svelato come la profilazione psicometrica possa trasformare dati dei social media in potenti strumenti di micro‚Äëtargeting politico. Dal 2014 [CA ha raccolto dati Facebook([1])](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf) di decine di milioni di persone tramite un quiz ("thisisyourdigitallife"), ottenendo [**profili OCEAN**([2])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the) (i "Big Five" della personalit√†) e abbinandoli a informazioni demografiche e di consumo. Queste **schede psicometriche** sono state impiegate per segmentare l'elettorato e sperimentare messaggi politici mirati: ad es. modulando [inserzioni pro-armi in base al tratto "nevroticismo" ([3])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). CA ha propagato contenuti su **Facebook** (tramite **Custom Audiences** e simili), testando varianti di annunci con tecniche di **A/B testing** per massimizzare l'impatto. 

Ok ma qual √® stato l'impatto di tutto questo?

Le evidenze sull'efficacia reale sono miste: [CA stessa, ([4])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups) ha riportato aumenti del **39%** nella sensibilizzazione su certi temi e un **+30%** all'affluenza di gruppi mirati in campagne USA del 2014. 
Tuttavia, altre analisi indipendenti [\[5\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=In%20an%20email%20to%20me%2C,like%20race%2C%20age%2C%20and%20gender)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed) hanno notato che il modello predittivo di CA **non superava di molto i normali criteri demografici**.

Oggi, logiche simili di **raccolta e utilizzo intensivo dei dati utente** si ritrovano nei principali servizi **chatbot e LLM** (modelli linguistici di grandi dimensioni). Piattaforme consumer come **ChatGPT** (OpenAI, [\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out)) e **Gemini** (Google) di default **registrano prompt, conversazioni e feedback degli utenti** e li usano per migliorare continuamente i modelli[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en). Gli utenti privati possono "_opt-out_" (insomma rifiutarsi di condividere i propri dati) limitando la condivisione della conservazione e dei propri dialoghi, ma in assenza di tale scelta i dati delle chat possono essere conservati per periodi estesi (es. **OpenAI** mantiene le chat degli utenti generici a tempo indefinito per addestrare modelli, a meno di opt-out, mentre **Anthropic** dal 2025 offre la scelta: no training e conservazione 30 giorni oppure training attivo con conservazione **5 anni**[\[9\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention). Al contrario, i servizi **business/enterprise** offrono garanzie di **isolamento**: ad esempio **OpenAI API/Enterprise** e **Microsoft 365 Copilot** assicurano che input e output **non alimentano i dati di training** dei modelli pubblici[\[10\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform)[\[11\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important), restando confinati nell'ambiente del cliente. Sul fronte **trasparenza e governance**, le aziende hanno introdotto **controlli per utenti e amministratori** (come dashboard privacy, impostazioni di retention, moduli di opposizione per l'UE[\[12\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Beginning%20this%20week%2C%20people%20based,well%20as%20newly%20submitted%20ones)[\[13\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes)) e adottato impegni contrattuali (es. **SOC 2**, **DPA** sul trattamento dati) per rassicurare imprese e regolatori[OpenAI \[14\]](https://openai.com/enterprise-privacy/#:~:text=Comprehensive%20compliance)[Google \[15\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). Non mancano, tuttavia, preoccupazioni: ad esempio il chatbot **DeepSeek**, popolare app cinese, raccoglie **ogni input, file e cronologia chat**, inviando tutto su server in Cina[\[16\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D)[\[17\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D); questo solleva dubbi di sicurezza e ha attirato l'attenzione di esperti per potenziali rischi di accesso governativo[\[18\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so).

> Per accedere a questo articolo serve l'account premium di Wired

In sintesi, dall'era Cambridge Analytica all'odierno boom dei chatbot AI, **dati personali e comportamentali** degli utenti sono diventati il **"carburante di modelli predittivi e generativi"**. Se da un lato ci√≤ abilita servizi pi√π "intelligenti" e campagne su misura, dall'altro impone nuove sfide di **privacy, controllo e responsabilit√†**. Le piattaforme stanno rispondendo con maggior trasparenza (policy dedicate) e opzioni di controllo, ma spetta anche agli utenti, e soprattutto ai regolatori, pretendere **chiarezza sull'uso dei propri dati**, esercitare i diritti di opt-out/off e valutare con cautela cosa condividere con queste AI.

## Cambridge Analytica: cos'√® successo?
Ora vediamo nel dettaglio cos'√® successo con il caso Cambridge analytica. Per farlo user√≤ le prime due fonti che abbiamo visto nell'articolo [1](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf),[2](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the)

### I dati raccolti
Cambridge Analytica (consociata di **SCL Elections**) acquis√¨ nel 2014-2015 un enorme dataset di utenti Facebook tramite la societ√† **GSR** (Global Science Research) del ricercatore Aleksandr Kogan. Kogan svilupp√≤ un'app di quiz della personalit√† ("thisisyourdigitallife") sfruttando la Graph API di Facebook, che all'epoca consentiva di estrarre non solo i dati dell'utente consenziente ma anche quelli dei suoi **amici** (funzionalit√† _friends permissions_ valida fino al 2014). Circa **320.000** utenti Facebook, principalmente statunitensi, compilarono il test OCEAN accedendo via Facebook Login; in cambio di pochi dollari, diedero all'app il permesso di leggere una vasta gamma di informazioni: profilo pubblico (nome, genere), data di nascita, citt√† attuale, **pagine "Like"**, post in bacheca, lista amici, persino messaggi privati e foto taggate. 

Poich√© l'app ereditava i diritti d'accesso della precedente app accademica di Kogan (sviluppata prima delle restrizioni Facebook del 2015), questa poteva raccogliere anche i dati degli amici dei partecipanti, se questi ultimi non avevano impostato diversamente la privacy. **In totale ~87 milioni di persone** (di cui oltre **1 milione** nel Regno Unito) subirono questa raccolta massiva di dati senza saperlo. Facebook conferm√≤ la stima e pubblic√≤ nel 2018 la lista dei paesi coinvolti. Circa **30 milioni** di individui avevano sia i dati Facebook sia il risultato del quiz psicometrico abbinati, costituendo il nucleo per le analisi predittive.

### Caratteristiche psicometriche (modello OCEAN)

Il modello di personalit√† Big Five, detto **OCEAN** (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), fu al centro della profilazione CA[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the). Kogan e colleghi applicarono le metodologie del Psychometrics Centre di Cambridge (note per il progetto "MyPersonality") che dimostrarono come dai **Facebook Likes** si potessero predire con sorprendente accuratezza i tratti OCEAN e altri attributi personali[\[19\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=OCEAN%20model%20and%20pioneer%20the,as%20ethnicity%20and%20political%20affiliation)[\[20\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that).

Nel contratto stipulato con SCL il 4 giugno 2014, Kogan dichiarava che le sue tecniche permettevano di raggiungere predittivit√† "**vicina al test-retest**" nei punteggi di personalit√†, con correlazioni tali che un algoritmo basato sui like risultava _pi√π accurato_ nel descrivere una persona rispetto alla conoscenza dei suoi amici o persino familiari[\[21\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Nix%20told%20us%3A%20%E2%80%9CWe%20do,41)[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the).
    
> Questa affermazione riprendeva uno studio accademico del 2015 [[22](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that)], dove Michal Kosinski, collega di Kogan, mostr√≤ che 70 like di Facebook superavano gli amici nel delineare il profilo psicologico di un individuo. 

In pratica, CA disponeva di punteggi OCEAN stimati per milioni di elettori USA, ottenuti direttamente dal quiz o frutto di inferenza tramite modelli addestrati sui dati di Kogan. 

Dato che ora potrebbe sembrare complicato, facciamo [un esempio](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). Un utente con punteggio alto in "Nevroticismo" e basso in "Apertura" veniva identificato come **sensibile a messaggi di paura e ordine**, mentre uno altamente "Estroverso" poteva rispondere meglio a contenuti ottimistici e sociali. Cambridge Analytica **clusterizzava** il pubblico in gruppi psicografici e individuava temi chiave per ciascun segmento: secondo l'ex CEO Alexander Nix, "presentare un fatto supportato da un'emozione" era la strategia, adattando l'argomentazione al profilo emotivo dell'audience[\[23\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=We%20are%20trying%20to%20make,109).

### Modelli predittivi e ML impiegati

Pur non pubblicando dettagli tecnici dei suoi algoritmi, CA combin√≤ approcci di **machine learning** con analytics tradizionali. Kogan in [un'email](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/) ha spiegato che il suo modello per CA operava in modo simile al sistema di raccomandazione di Netflix, ossia tramite **SVD/factor analysis**: riducendo una matrice utente-like a componenti latenti, che incorporavano insieme personalit√†, demografia e reti sociali. In sostanza, l'algoritmo non isolava "apertamente" i 5 tratti, ma li mescolava con decine di altre variabili (et√†, genere, orientamento politico, etc.) in fattori correlati utili a predire il comportamento elettorale. 

Come abbiamo fatto nella sezione precedente, facciamo un esempio. CA sfruttava _regressioni_ e _alberi decisionali_ per stimare la probabilit√† che un individuo sostenesse determinate cause o candidati, date le sue caratteristiche psicografiche e demografiche note. 

I data scientist di CA costruirono modelli per identificare i cosiddetti **persuadables**, ovvero elettori indecisi fortemente influenzabili da specifiche leve emotive. La mole di dati (like, test, comportamenti online e offline) permise anche l'uso di **reti neurali shallow** o modelli di classificazione multivariata per associare profili a _outcome_ di interesse (voto, donazione, astensione). Un ruolo importante lo ebbero i **modelli look-alike** di Facebook: CA poteva caricare liste di utenti noti (ad es. individui con alto punteggio "apertura" identificati dal quiz) e usare l'algoritmo di Facebook per trovare altri utenti simili, ampliando il raggio del targeting.

**Pipeline dati‚Üíprofilo‚Üítargeting**

![Schema della pipeline Cambridge Analytica](../Assets/ca-data-pipeline.svg)

_Figura:_ la numerazione segue le fasi con cui CA trasforma dati social e commerciali in messaggi micro-targettizzati e rifinisce continuamente i modelli.

In primo luogo CA (via GSR) [**estrae i dati grezzi**](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to) dal social media (Facebook) e li combina con altre fonti (ad es. **elenchi elettorali** pubblici, dati da broker commerciali come acquisti e abbonamenti). 
Questi dati alimentano l'**analisi psicometrica**: dal quiz e dai like si calcolano i tratti OCEAN individuali, che poi vengono **aggregati e inseriti** in un profilo unico per ogni elettore (inclusi et√†, genere, posizione, inclinazioni politiche, etc.). Su questa base, i data analyst di CA hanno sviluppato **modelli predittivi** per segmentare la popolazione in gruppi chiave (es. "neurotici insicuri", "aperti progressisti") e prevedere per ciascuno la suscettibilit√† a specifici messaggi.

Parallelamente, un altro team elabora **varianti di messaggi** (meme, video, slogan) tarati sugli insight psicologici dei segmenti: ad esempio, lo stesso tema (come la vendita di armi) viene confezionato in versione _paura e protezione_ per individui ad alto nevroticismo, e in versione _hobby sportivo_ per soggetti aperti/tranquilli. 

I messaggi vengono quindi **inviati tramite micro-targeting**: CA caricava su Facebook le liste di utenti target (identificati per nome/ID, email o telefono) e utilizzava strumenti come **Custom Audience** e **Dark Posts** per mostrare inserzioni diverse a gruppi diversi, senza che fossero visibili pubblicamente ad altri. Questa fase includeva **A/B test** e controlli di efficacia: si monitoravano clic, condivisioni, tempo di visualizzazione e tassi di conversione (es. iscrizione a un evento, donazione) per ciascuna variante, iterando poi sulla creativit√† vincente. Infine, i risultati di campagna (engagement effettivo, cambiamenti nei sondaggi interni) venivano **retroalimentati** nel processo: le reazioni degli utenti servivano a affinare ulteriormente i modelli di persuasione, in un ciclo continuo di ottimizzazione.

### Canali di delivery e sperimentazione A/B

Il principale veicolo dei messaggi di CA fu **Facebook**. La societ√† creava inserzioni mirate utilizzando il sistema pubblicitario di Facebook, che permetteva di indirizzare annunci a cluster demografici e psicografici molto specifici (per area geografica, interessi, simili a una lista fornita). CA ha anche sfruttato il partner canadese **AggregateIQ** per campagne su altre piattaforme e la **rete display** (ad es. banner web mirati): AIQ gest√¨ spese pubblicitarie per gruppi [pro-Brexit](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=sharing%20of%20data%20in%20the,overseas%20elections%20in%20Chapter%206) e [pro-Trump](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter), utilizzando i dati e segmenti forniti da CA. Inoltre, CA non disdegnava metodi tradizionali: in alcuni casi forn√¨ _scripts_ per **telemarketing** o per volontari politici, calibrati sul profilo dell'elettore da contattare (es. enfatizzare l'immigrazione parlando con un soggetto "chiuso" e timoroso, vs. economia con un "aperto" cosmopolita). 

Gli **esperimenti A/B** sono stati centrali: in pratica venivano testate **centinaia di varianti** di annunci  simultaneamente, cambiando ad esempio colore, tono emotivo, call-to-action e misurando quale versione avesse il maggior tasso di click o conversione in ciascun segmento. 

Un famoso esempio riportato da Wylie √® la campagna ["Defeat Crooked Hillary"](https://edition.cnn.com/2018/03/21/opinions/trump-cambridge-analytica-clinton-slogan-opinion-psaki) in cui CA avrebbe testato decine di messaggi anti-Clinton (dai pi√π moderati ai pi√π complottisti) per capire quali risuonavano con gruppi di elettori dubbi, bombardandoli poi con il messaggio ottimizzato. Facebook in quel periodo **non tracciava** efficacemente questi _dark ads_ n√© ne limitava la personalizzazione estrema, il che permise a CA di condurre una sorta di laboratorio di propaganda invisibile al pubblico e alle stesse vittime.

### Integrazione con dati esterni
Oltre ai dati Facebook, CA disponeva di un mosaico di altre informazioni. Negli USA sfrutt√≤ i **registri elettorali** statali (contenenti storia di voto, affiliazione di partito, etc.) e li [incroci√≤ con dataset commerciali](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to) (provenienti da broker come Acxiom, Experian) su abitudini di acquisto, tipo di auto posseduta, riviste lette, donazioni caritatevoli‚Ä¶ 

Questa **strategia ibrida** (social media + data offline) consent√¨ di identificare correlazioni inedite. Un ulteriore strumento furono i **Lookalike Audiences**: partendo da un pubblico di cui CA conosceva l'orientamento (es. persone profilate come _pro-Trump_), l'algoritmo di Facebook trovava altri utenti con caratteristiche analoghe su cui estendere la campagna.

### Esiti e implicazioni
Nel 2018 lo scandalo Cambridge Analytica, emerso grazie a inchieste (_The Guardian_, _NY Times_) e all'azione del Parlamento UK, ha portato alla chiusura di CA/SCL e a una multa simbolica a Facebook ([¬£500k dall'ICO UK](https://www.bbc.com/news/technology-45976300)) per aver fallito nel proteggere i dati). 

L'eredit√† principale √® stata una presa di coscienza globale sui rischi di abuso dei dati personali a fini di **manipolazione di massa**. Gli strumenti raffinati da CA (profilazione psicologica + social media targeting) in s√© non erano completamente nuovi, ma CA ne spinse l'uso oltre i limiti etici, operando senza trasparenza n√© consenso informato degli interessati. 

Le implicazioni? Parecchie!

L'episodio ha accelerato parecchie riforme: Facebook nel 2018-19 ha limitato ulteriormente le API e reso pi√π controllabili le inserzioni politiche, mentre in ambito normativo casi come questo hanno contribuito alla stesura di linee guida (es. GDPR in UE prevede gi√† dal 2018 il diritto di opposizione a profilazione e decisioni automatizzate, sebbene all'epoca non fosse invocato).

### I chatbot di oggi collezionano dati? 

La domanda nasce spontanea.

Con l'avvento di ChatGPT e simili (2022+), le aziende tech hanno applicato un paradigma analogo: raccogliere quante pi√π **interazioni utente** possibili per addestrare e perfezionare modelli di intelligenza artificiale generativa. In questa seconda parte analizziamo le politiche dei principali fornitori di LLM (OpenAI, Google, Anthropic, Microsoft, Meta, DeepSeek), focalizzandoci su come gestiscono i dati utenti (tipologie raccolte, finalit√†, tempi di conservazione, training, controlli, base legale) e quali meccanismi di **trasparenza e governance** offrono.

## Chatbot e LLM: dati raccolti, utilizzi e confronto tra piattaforme (2023-2025)

Le piattaforme di **AI generativa conversazionale** hanno modelli di business e utenza diversi (consumer vs enterprise), ma presentano tendenze comuni: **registrazione delle richieste utente (prompt)**, monitoraggio delle conversazioni (log) e sfruttamento di tali dati per **migliorare i modelli tramite "training continuo"** (questo lo chiariremo dopo). 

Di seguito esponiamo per ciascun vendor chiave lo stato attuale (al 2025) su raccolta dati, utilizzo per training, retention, revisione umana, controlli offerti e base legale, con un quadro comparativo riassuntivo.

### OpenAI - ChatGPT
Se parliamo di OpenAI, ovviamente parliamo dei modelli GPT e del suo prodotto e chatbot di riferimento: ChatGPT.
#### Dati raccolti
OpenAI [raccoglie ogni input](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance) fornito dagli utenti ai suoi modelli, sia tramite l'**interfaccia ChatGPT** (conversazioni in chat, allegati caricati) sia via **API**. In dettaglio, le **chat** su chat.openai.com includono prompt dell'utente e risposte dell'AI; se l'utente fornisce file (es. nell'uso di plugin o tramite la funzione di upload immagini/PDF), anche quei contenuti vengono acquisiti. Sono registrati inoltre **feedback espliciti** (i voti üëç/üëé sulle risposte, segnalazioni) e **metadata tecnici** (timestamp, indirizzo IP, tipo di dispositivo/browser) per motivi di sicurezza e monitoraggio di performance.

OpenAI non raccoglie dati biometrici o di sensori perch√© il servizio √® testuale/visivo/vocale. A proposito di quest'ultimo, usare la funzione voice di ChatGPT implica che l'audio utente venga trascritto e processato (usando modelli come Whisper). 

Oltre ai dati forniti direttamente dall'utente, OpenAI pu√≤ raccogliere **telemetria d'uso** (es. frequenza delle sessioni, lunghezza dei prompt, tassi di errore) e applica cookie/tracking sul sito ChatGPT come da sua Privacy Policy generica.

#### Finalit√† e base legale

OpenAI **non utilizza i dati degli utenti per fini di marketing/pubblicit√† personalizzata**, una differenza rispetto ai social network classici: i prompt non servono a profilare l'utente per vendere inserzioni, ma a _profilare il modello_ stesso, per cos√¨ dire. 

Dal punto di vista GDPR, OpenAI (dopo le vicende col Garante italiano nel 2023) ha introdotto un modulo di consenso facoltativo per l'uso dei dati a fini di training, ma continua a sostenere una combinazione di basi giuridiche: l'esecuzione di un contratto per i servizi di risposta, e il legittimo interesse per il miglioramento del modello, offrendo comunque l'**opt-out** (diritto di opposizione) agli interessati.

#### Uso ai fini di training modelli
Di _default_, per **utenti consumer** (ChatGPT free e Plus), i contenuti delle chat **vengono utilizzati per addestrare e migliorare i modelli** di OpenAI, **salvo opt-out esplicito**. 

OpenAI spiega che ChatGPT "migliora addestrandosi sulle conversazioni che le persone hanno con esso". Questa forma di _training_ comprende sia l'uso nei set di _fine-tuning_ supervisionato/RLHF (ad es. gli specialisti OpenAI esaminano campioni di chat per creare dati di addestramento con il feedback umano, oppure usano valutazioni üëç/üëé degli utenti come segnale di reward nel Reinforcement Learning) sia l'impiego diretto nei _dataset_ per versioni future del modello. Nel **marzo 2023** OpenAI ha cambiato le policy per l'API, annunciando che di default _non_ avrebbe pi√π usato i dati delle chiamate API dei clienti per addestrare modelli, a meno di opt-in. Questo per rassicurare aziende e sviluppatori. Dunque, per **servizi business/API**, l'impostazione √® opposta: **nessun uso dei dati utente per training**, a meno che l'organizzazione scelga volontariamente di condividerli (ad es. inviando apposta esempi di prompt via _Playground_ con flag di opt-in). A settembre 2023 OpenAI ha lanciato **ChatGPT Enterprise**, chiarendo che tutte le conversazioni degli utenti enterprise "non entreranno mai nel training dei nostri modelli" di default.


#### Conservazione dei dati (retention)
Le policy di retention di OpenAI distinguono per servizio e impostazioni:  
- Per conversazioni ChatGPT _con_ cronologia attiva (default consumer): i dati sono conservati nei sistemi di OpenAI a tempo indeterminato, salvo diversa comunicazione. OpenAI non dichiara esplicitamente una scadenza - anzi, il fine √® accumulare un lungo storico per addestrare modelli futuri. 

- Per conversazioni ChatGPT _con_ cronologia _disattivata_ (opt-out training): OpenAI afferma che tali dati "non saranno usati per addestrare modelli" e verranno **conservati solo per 30 giorni** allo scopo di monitoraggio abusi, dopodich√© saranno cancellati. Questa finestra di ~30 giorni serve a eventualmente ispezionare contenuti se emergono problemi (ad es. un utente che genera molte richieste illegali).  

- Per utilizzi [**API standard**](https://openai.com/index/response-to-nyt-data-demands/): analogamente, _log_ di richieste e risposte vengono tenuti per max **30 giorni** per motivi di sicurezza/abuso, dopodich√© eliminati. √à stata una modifica introdotta a fine marzo 2023: prima, i dati API potevano essere conservati pi√π a lungo. Inoltre, OpenAI offre a clienti enterprise un'opzione di "**Zero Data Retention**", ossia non conservare affatto i contenuti delle richieste (se il cliente opta per questa modalit√†, le richieste vengono elaborate e immediatamente scartate, mantenendo solo metriche aggregate).  

- Per **ChatGPT Enterprise**: OpenAI fornisce controllo agli amministratori sull'intervallo di conservazione delle conversazioni dei loro utenti aziendali (fino a poter scegliere [_zero retention_](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law)). Nella documentazione Enterprise (agg. giugno 2025) si afferma: "You control how long your data is retained (ChatGPT Enterprise)". Ci√≤ implica che un'azienda pu√≤ impostare, ad esempio, auto-cancellazione di chat dopo X giorni. Se non configurato diversamente, i dati enterprise dovrebbero seguire la retention di default contrattuale, che √® 30 giorni negli endpoint API e potenzialmente pi√π estesa per l'interfaccia enterprise se l'utente lo consente (ma sempre esclusa dal training).  

- **Eccezioni legali**: come per tutti, eventuali dati soggetti a obblighi di legge (es. ordine di conservazione da autorit√†, o dati necessari per dispute legali) possono essere mantenuti oltre i termini sopra.

In sintesi, l'utente comune di ChatGPT che non tocca le impostazioni accetta una conservazione indefinita a fini di training. L'utente consapevole pu√≤ ridurre l'impatto disattivando la cronologia (riducendo a 30 gg retention e niente training). Le aziende e sviluppatori invece hanno by design retention breve e nessun training, con opzione di stringere ulteriormente a zero log.

#### Revisione umana dei dati

OpenAI impiega [revisione manuale](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance) su una _piccola percentuale_ di conversazioni, sia per migliorare il modello (ad es. etichettare conversazioni difficili per far apprendere risposte migliori) sia per moderazione (analisti che controllano se il modello ha violato le policy). La documentazione spiega che **gli addetti OpenAI possono accedere e visionare** un campione di contenuti utente, con strumenti per offuscare informazioni personali ove possibile.

I dati destinati a training vengono **pseudonimizzati**: OpenAI [dichiara](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance) di _rimuovere o aggregare quante pi√π informazioni personali possibile_ prima di usare i dati nelle dataset di addestramento, per ridurre il rischio di ricostruire identit√†. Ad esempio, nomi, email, numeri potrebbero essere mascherati automaticamente. Non √® per√≤ una garanzia assoluta di anonimato, come hanno notato vari enti (il Garante italiano ha chiesto di implementare meglio queste misure). La revisione umana OpenAI avviene anche per i **feedback**: se un utente segnala un output come inappropriato, un membro del team di sicurezza potr√† leggere quell'intera conversazione. 

> Importante: per le **utenze API/enterprise**, OpenAI afferma di _non_ far intervenire umani sulle richieste contenute, a meno che il cliente stesso le condivida via feedback. Dunque i dati aziendali rimangono riservati (anche perch√© spesso contengono segreti industriali); in ChatGPT Enterprise √® previsto che i log delle conversazioni non siano visibili al personale OpenAI n√© usati per training, salvo situazioni di abuso eccezionali.

### Gemini - Google

#### Dati raccolti

Google raccoglie un ampio spettro di dati attraverso i suoi servizi di AI conversazionale, che nel 2023 si sono evoluti da _Google Bard_ a una suite unificata sotto il nome di **Gemini**. La **Privacy Hub di Google Gemini** dettaglia le [categorie](https://support.google.com/gemini/answer/13594961?hl=en#):  

- **Contenuti forniti dall'utente:** tutto ci√≤ che l'utente _dice o inserisce_ in interazione con l'AI. Ci√≤ include il **prompt testuale** o vocale, gli eventuali file caricati (immagini, documenti per farli analizzare dal modello), i dati condivisi tramite funzioni di _"connected apps"_ (es. se l'utente chiede a Gemini di leggere una pagina web o un'email, il contenuto di quella pagina/email viene acquisito). Se l'utente usa l'AI in modalit√† vocal/video (es. _Gemini Live_), le **trascrizioni e registrazioni audio/video** di quell'interazione vengono raccolte.  

- **Contenuti generati dall'AI:** Google registra anche le **risposte generate** dal modello (testo, codice, immagini create, audio, video). Quindi l'intera conversazione (prompt + output) √® memorizzata. Anche _riassunti di chat_ o _link pubblici_ creati con Gemini sono considerati contenuti generati e tracciati.  

- **Dati di app e dispositivo:** ogni interazione con Gemini √® accompagnata da dati di **telemetria**. Google colleziona informazioni sulle **app collegate** e servizi Google integrati (ad es. se l'utente ha attivato volont√† di usare dati da YouTube, il sistema registra quali servizi sono stati consultati). Raccoglie poi dettagli del **dispositivo** (modello, OS, versione app), browser e impostazioni locali, nonch√© identificatori unici e l'**indirizzo IP**. Inoltre, vengono loggate metriche di interazione: tempi di risposta, eventuali crash.

- **Permessi e contesto device (mobile):** se l'utente usa l'app mobile di Gemini (o Google app con assistente), Google pu√≤ raccogliere dati supplementari: la rubrica/contatti (se funzionalit√† "help you keep in touch" attiva), log di chiamate e messaggi (per rispondere a prompt tipo "quando mi ha chiamato X?"), elenco di app installate, contenuto schermo (se abilita "overlay" per porre domande sullo schermo corrente). Queste sono informazioni potenzialmente molto sensibili, gestite con permessi Android/iOS, ma se concesse diventano input al modello e sono trattate come dati utente raccolti.  

- **Informazioni di posizione approssimata:** Google rileva una stima di posizione (basata su IP o geolocalizzazione account) per contestualizzare risposte e per registrare da dove proviene l'attivit√† (ad es. per applicare policy regionali).

- **Feedback espliciti e Gems:** quando l'utente fornisce un **feedback**  o definisce istruzioni personalizzate (le "Custom instructions" di Gemini) o _Gems_ (agenti salvati dall'utente), questi dati vengono memorizzati.  

- **Dati supplementari da funzioni opt-in:** se l'utente opta per alcune feature sperimentali (es. condividere conversazioni pubblicamente via link, o usare ["Canvas"]((https://support.google.com/gemini/answer/13594961?hl=en#pn_canvas)) per creare app visuali con l'AI), Google potrebbe raccogliere dati aggiuntivi specifici (es. il contenuto generato su una Canvas app).

In pratica, **tutto ci√≤ che passa per l'AI di Google viene loggato**: prompt, output, interazioni collegate (ricerche effettuate dall'AI per conto dell'utente), contesto applicativo. Google pu√≤ anche incrociare queste info con la cronologia utente su altri servizi: la Privacy Hub menziona l'uso di dati da [_Search_ o _YouTube history_](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=YouTube%20history,%20or%20your%20page%20context%20and%20URL) come contesto che Gemini pu√≤ prendere se l'utente lo permette.

#### Finalit√† d'uso: 

Google applica la sua **Privacy Policy generale** anche a Gemini, enumerando varie finalit√†:

- **Erogazione del servizio**: usare i dati per fornire le funzionalit√† richieste (es. generare la risposta, ricordare le preferenze dell'utente nelle chat).
- **Mantenimento e miglioramento**: impiegare i dati raccolti per addestrare e affinare i modelli di AI, migliorare la qualit√† delle risposte e la robustezza. 
I prompt e chat degli utenti servono a _trainare_ i modelli (parte di "maintain and improve service").  
- **Sviluppo di nuovi servizi**: dati usati in forma aggregata per creare funzionalit√† future, prototipi.  
- **Personalizzazione servizi**: se l'utente ha optato per la personalizzazione, i prompt possono influenzare suggerimenti futuri (ad es. se chiedo spesso ricette a Gemini, potrebbe personalizzare i risultati di Google Search verso ricette).  
- **Comunicazioni con l'utente**: es. invio di avvisi su nuove feature.  
- **Misurazione performance**: capire metriche di successo delle risposte, utilizzo (questo rientra nell'analitica).  
- **Protezione di Google, utenti e pubblico**: significa usare i dati per moderazione, prevenire abusi, rimuovere contenuti illeciti, rispettare normative (ad es. filtri su hate speech).

Nella Privacy Hub viene sottolineato che [_revisori umani_](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve) visionano parte dei dati "per questi scopi". 

#### Uso per training e miglioramento modelli

Questo punto √® il cuore di tutto, per cui ti chiedo di prestare attenzione.

Di default, **Google utilizza i dati utente delle interazioni generative per addestrare e perfezionare i suoi modelli AI**. 

Come OpenAI, anche Google pu√≤ usare i dati raccolti per allenare modelli di filtro (es. riconoscimento di richieste proibite) o per migliorare l'integrit√† dei suoi modelli.

Google afferma di creare per questi scopi dataset _anonymized_. Secondo un'[analisi indipendente](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini), i dati delle chat che Google seleziona per training vengono **conservati fino a 3 anni** insieme a metadati (lingua, device) ma scollegati dall'account utente. In pratica, Google campiona conversazioni, rimuove identificativi diretti e le archivia in un corpus di training su cui fanno tuning del modello generativo (Gemini e affini). In conclusione, a meno di opt-out, se chiediamo a Gemini "Consigliami un film per bambini", quella chat, opportunamente anonimizzata, potrebbe contribuire a far s√¨ che in futuro il modello risponda meglio a richieste simili.

#### Differenze consumer vs enterprise 

Per **utenti consumer (account Google personali)**, l'uso dei dati per training √® opt-out (cio√® attivo salvo disattivazione), mentre per **clienti Google Workspace (azienda/scuola)** Google ha preso l'impegno opposto: _nessun dato del cliente verr√† usato per addestrare modelli generativi al di fuori del suo dominio_. In un documento sulle AI in Workspace si [legge](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20does%20not%20share%20your%20content%20outside%20your%20organization%20without%20your%20permission): "**Gemini does not share your content outside your organization without your permission.**". 

Ci√≤ significa che se un'azienda usa le funzioni AI (ad es. "Help me write" in Gmail con Gemini), i prompt e testi generati **restano entro il tenant** e Google non li utilizza per migliorare il modello base destinato al pubblico.

Questo ricalca l'approccio di Microsoft con Copilot (no training cross-tenant). Dunque Google distingue due flussi:  

- _Ambito consumer:_ dati usati per training modelli generali (Gemini), a meno di opt-out.  
- _Ambito enterprise:_ dati isolati, niente training globale. (Google potrebbe in futuro offrire opzioni di _opt-in_ per le aziende che vogliono contribuire, ma ad oggi non risulta).

#### Revisione umana e sicurezza

Google [specifica](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve) che **personale umano (incluso personale di fornitori terzi contrattati)** **esamina una parte dei dati raccolti** per gli scopi elencati.

In particolare, team di **analisti/annotatori** controllano conversazioni estratte per valutare la qualit√† delle risposte, identificare errori o bias del modello e segnare contenuti problematici. Prima del lancio pubblico di Bard, ad esempio, Google ha fatto esaminare campioni di output ai suoi _rater_. 

Ora, su base continua, c'√® un pipeline: certe chat (ad es. lunghe conversazioni o quelle con thumbs down dall'utente) vengono inviate a un **tool di labeling** dove i revisori le leggono e marcano attributi (come "contiene disinformazione", "risposta utile?" ecc.). 

Google afferma: "per favore, non inserire informazioni confidenziali che non vorresti che un revisore umano veda o che Google usi per migliorare i nostri servizi". Questo √® un avviso molto diretto: vuol dire che qualunque cosa scriviamo potrebbe essere letta dallo staff. Proprio questa frase appare nel disclaimer di Bard e Gemini. Per i **dati enterprise** invece Google [promette](https://support.google.com/a/answer/15706919?hl=en#:~:text=Your%20content%20is%20not%20human%20reviewed%20or%20used%20for%20Generative%20AI%20model%20training%20outside%20your%20domain%20without%20permission.): "il tuo contenuto non √® soggetto a revisione umana fuori dalla tua organizzazione senza permesso". Ossia, Google non far√† leggere a dipendenti i prompt di utenti Workspace, salvo che l'azienda stessa li condivida (o forse in forma aggregata per debug, ma non sistematico).

#### Controlli utente e impostazioni

Pu√≤ sorgere spontanea la domanda: cosa possiamo fare per preservare la nostra privacy? Vediamo ora come possiamo modificare le varie impostazioni.

Google fornisce vari controlli, ma il pi√π importante √® il seguente:  

Nel **Google Account** > sezione **Dati e Privacy**, c'√® la voce **"Attivit√† delle App Gemini"** dove l'utente pu√≤ vedere ed eventualmente eliminare la cronologia delle domande fatte all'AI, impostare l'**auto-cancellazione** dopo 3, 18 o 36 mesi, e scegliere se **consentire l'uso dei dati per migliorare Google AI** (dato che queste informazioni potrebbero cambiare, vi lascio il link [qui](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Gemini%20Apps%20Activity%20from%20the%20default%20of%2018)). 

Questa ultima opzione √® cruciale: se disattivata, i prompt non vengono utilizzati per training (in teoria almeno). 

Mi spiego meglio. Come specificato, disattivando l'attivit√† si impedisce l'uso per _miglioramento modelli_ e _revisione_, ma Google [avverte](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Gemini%20safe%20and%20secure,%20including%20with%20help%20from%20human%20reviewers) che _comunque i tuoi prompt potranno essere usati per risponderti e per mantenere Gemini sicuro, anche con aiuto di revisori_. C'√® una distinzione sottile: _Gemini Apps Activity_ off blocca l'uso _per addestrare_ modelli (i dati non entrano nel dataset anonimo), ma Google potr√† ancora farli controllare per moderazione e tenerli 72h. Quindi un utente non pu√≤ impedire totalmente che i suoi contenuti siano letti da umani, poich√© la moderazione rientra nel _legittimo interesse/sicurezza_ (a meno di non usare affatto il servizio).
 
Inoltre √® importante spendere due parole sul **modulo di opposizione (UE):** per i dati pubblici usati nel pretraining, Google (come Meta e OpenAI) offre modulistica GDPR per opporsi all'uso di propri dati personali presenti nel web scraping. Non direttamente legato ai prompt dell'utente, ma parte della governance complessiva.

Direi che possiamo passare al prossimo grande provider, Antrophic.

### Claude - Anthropic

#### Dati raccolti

**Claude** √® accessibile via API e attraverso un'interfaccia chat (claude.ai) in versioni Free, Pro e Max. I **dati utente raccolti** includono: i **prompt** inseriti (testo e eventuali allegati; ad es. codice inviato per analisi), l'intera **conversazione** (dialogo cumulativo), i **metadati di utilizzo** (orari, indirizzo IP, info browser) e qualsiasi **feedback esplicito** inviato (reazioni, segnalazioni). La [_Privacy Policy_](https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training) di Anthropic (agg. 5 settembre 2023) definisce "**chat and coding session data**" come il contenuto che possono usare per migliorare i modelli. 

Inoltre raccoglie i dati di account (email, telefono per 2FA) e dati di pagamento se utente Pro/Max. Non risultano funzionalit√† di integrazione con app esterne per ora, quindi i dati sono principalmente quelli forniti dall'utente nella chat stessa. A livello tecnico, Anthropic conserva anche **log delle richieste API** simili a OpenAI, con id e timestamp.

#### Finalit√† dichiarate

Anthropic, come da Privacy Policy, usa i dati utente per: erogare il servizio (generare risposte con Claude), migliorare e addestrare i suoi modelli AI, prevenire abusi e problemi di sicurezza (moderazione), comunicare con gli utenti (assistenza, avvisi di policy), e rispettare obblighi legali. 

In UE, nel suo privacy center afferma che la base legale per i normali utenti consumer √® il **consenso** per usare i dati nel training (consenso che dal 2025 chiede esplicitamente), mentre per i clienti commerciali la base √® contrattuale e i dati non vengono usati oltre l'esecuzione del servizio. Anthropic non fa pubblicit√†, quindi non usa dati a scopi di marketing.

#### Uso per training modelli - evoluzione delle policy

Voglio spendere due parole sull'evoluzione della policy perch√© √® secondo me degno di nota. Vediamo insieme il perch√©.

Inizialmente Anthropic era tra i pi√π attenti: fino al 2023 dichiarava che **non utilizzava le conversazioni degli utenti per addestrare i modelli di base**. 

Questo scenario √® cambiato nel **fine 2023-2024**: con la necessit√† di migliorare Claude e competere, Anthropic ha rivisto la policy.

**Fino a met√† 2023:** per utenti Claude (ad esempio su Poe o Slack) i dati non venivano usati per training by default; venivano per√≤ conservati per moderazione per 30 giorni. L'utente _poteva_ acconsentire a condividere conversazioni per ricerca, ma era opt-in.

A fine agosto del 2025 per√≤ Antrophic ha deciso di fare un cambiamento. Mi ricordo molto bene un recente articolo di [Wired](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Prior%20to%20Anthropic%20updating%20its,user%20chooses%20to%20opt%20out) che celebrava CLaude dicendo che "Claude era stato uno dei pochi chatbots principali a **non** addestrarsi automaticamente sulle chat degli utenti". Successivamente nello stesso articolo hanno spiegato come poter tornare alle impostazioni precedenti e come fare quindi opt-out.

La policy era prevista entrare in vigore il 28/9/23 ma poi l'hanno posticipata all'**8 ottobre 2025** per dare pi√π tempo agli utenti di scegliere.

Il default √® diventato di fatto **opt-in al training** (il toggle in app √® pre-selezionato su ON), quindi se l'utente accetta i nuovi termini senza toccarlo, viene incluso. Chi rifiuta (opt-out) potr√† continuare a usare Claude ma con i dati esclusi dal training.

**A partire da ottobre 2025:** tutte le nuove conversazioni degli utenti consumer che hanno acconsentito verranno utilizzate per **addestrare le nuove versioni di Claude**[\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=). Quelle di chi ha negato, no. 

> **Importante**: Anthropic precisa che **solo le chat nuove o riprese** dopo l'opt-in contano. Le vecchie chat pregresse non verranno retroattivamente usate a meno che l'utente le riapra (riaprendole, diventano "attive" e quindi "idonee" al training da quel punto in poi). Dunque c'√® un focus su dati freschi.

Per i **[clienti aziendali](https://www.anthropic.com/news/updates-to-our-consumer-terms) (Claude for Work, Claude Pro per organizzazioni, API, partner cloud)**, Anthropic **non utilizza** i dati per training a meno di accordi specifici. Lo ribadisce: "questi cambiamenti non si applicano ai servizi sotto i Commercial Terms, inclusi Claude for Work (Team ed Enterprise), API, Amazon Bedrock, Google Vertex, Claude Gov/Ed". In altre parole, come OpenAI e Google, anche Anthropic rispetta l'isolamento dei clienti business.

**Durata di conservazione:** fino al 2023, Anthropic affermava di eliminare i dati delle conversazioni entro **90 giorni** o al massimo **1 anno** nei log, mantenendo 30 giorni per moderazione attiva. Ma nel 2025 con la nuova policy c'√® un grosso cambiamento:  
\- Se l'utente **consente l'uso per training**, Anthropic estende la retention delle conversazioni a **5 anni**[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention). L'idea √® che avendo uno storico lungo aiutano il ciclo di sviluppo modelli (Anthropic motiva che i cicli di training durano 1-2 anni e serve consistenza nei dati raccolti[\[108\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,the%20data%20retention%20period)). Dunque chat e codice di utenti opt-in potranno restare nei sistemi fino al 2030!  
\- Se l'utente **non consente (opt-out)**, rimane la retention breve precedente: i dati della chat saranno conservati solo **30 giorni** (per motivi di sicurezza/abuso) e poi cancellati[\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period)[\[13\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,training%20during%20the%20signup%20process).  
\- In entrambi i casi, l'utente ha sempre la possibilit√† di **cancellare manualmente singole conversazioni** dall'UI Claude.ai; se lo fa, Anthropic garantisce che quelle conversazioni _non saranno usate nel training futuro_[\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period). (Anche se potrebbero averle gi√† viste per moderazione, ma vengono escluse dai dataset).  
\- I **feedback inviati** (es. segnalazioni su output) se utente √® opt-in saranno conservati 5 anni come le chat, se opt-out suppongo 30gg come resto.

Questa conservazione lunga (5 anni) √® notevole e ha suscitato critiche (c'√® stata discussione su Reddit e post su Medium evidenziando "Anthropic now retains your data for 5 years if you opt in"[\[110\]](https://www.reddit.com/r/ClaudeAI/comments/1nd73si/anthropics_new_privacy_policy_is_systematically/#:~:text=Anthropic%27s%20New%20Privacy%20Policy%20is,Claude%20can%20potentially%20access)). Anthropic dice che i 5 anni valgono solo per i dati raccolti con permesso dal momento del cambio in poi, e servono per migliorare classifcatori di abuso di lungo periodo[\[111\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=The%20extended%20retention%20period%20also,keep%20Claude%20safe%20for%20everyone).

#### Revisione umana

Come gli altri, Anthropic utilizza annotatori umani per migliorare Claude. Ad esempio, la RLHF di Claude √® stata condotta con principi di "Constitutional AI" (dove piuttosto che feedback umani su ogni risposta, usano un insieme di regole e poi intervento umano su casi di possibile indecisione). Comunque, la Privacy Policy ammette che **personale di Anthropic pu√≤ visualizzare i contenuti dell'utente** per debug e miglioramento (specialmente se segnalati come problematici). Nel _Claude Privacy Center FAQ_ [c'√® scritto](https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training#:~:text=we%20will%20store%20the%20entire%20related%20conversation): "_we may use some chat data to improve models, including entire conversation with any content_", suggerendo che intere chat possano essere lette dagli ingegneri. Con la nuova policy di _opt-in_, solo le chat di chi ha aderito verranno passate al team di modello. Quelle di opt-out dovrebbero essere toccate solo per moderazione se emergono violazioni (p.e. un sistema automatico segnala una conversazione, allora un reviewer la vede per decidere provvedimenti).

### Microsoft - Copilot 

#### Dati raccolti

Microsoft fornisce funzionalit√† di AI generativa principalmente in due categorie:  

- **Copilot per Microsoft 365**: integrato in Office: Word, Excel, PowerPoint, Outlook, Teams, etc.) e **Copilot Chat** (Business Chat che attinge a Microsoft Graph.  

- **Azure OpenAI**: servizio cloud dove i dati dei clienti vengono processati da modelli OpenAI isolatamente.

Qui ci concentriamo su **Microsoft 365 Copilot**, lanciato nel 2023 per utenti business. I dati che sono potenzialmente coinvolti sono:

- **Prompt dell'utente**: la richiesta che l'utente pone a Copilot, spesso in linguaggio naturale (es. "Riassumi le mail non lette di oggi" in Outlook). Questo prompt viene trasmesso dal client (es. Outlook) ai backend di Microsoft.  

- **Contenuti aziendali richiamati**: Copilot accede ai [**dati dell'organizzazione**](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy) (per cui l'utente ha permessi) tramite Microsoft Graph: documenti SharePoint/OneDrive, messaggi di posta, chat Teams, calendario, contatti, ecc. Questi contenuti (ad es. il testo di una mail rilevante per rispondere al prompt) vengono estratti e passati come **contesto** al modello di OpenAI che genera la risposta. Dunque, M365 Copilot di fatto "legge" i dati aziendali ma **non li memorizza permanentemente fuori**, ma li usa solo per produrre l'output.  

- **Risposta generata**: il testo (o tabella, immagine) generato da Copilot come output viene potenzialmente salvato se l'utente lo **inserisce** in un documento o email. In tal caso, diventa parte dei normali dati aziendali (salvati in SharePoint, Exchange etc.). Microsoft registra inoltre la risposta per mostrarla nella history utente.  

- **Log di interazione**: Microsoft **conserva traccia** delle interazioni con Copilot (prompt + risposta) in quella che chiama "Copilot activity history". Questo consente all'utente di rileggere le ultime richieste in Business Chat e per debug interno. I log includono anche riferimenti (citations) ai dati aziendali usati.  
- **Dati tecnici**: come sempre, ID utente, tenant ID, timestamp, performance metrics e possibili errori vengono registrati.

#### Uso per training dei modelli

**No**.

Questa √® la risposta breve, ora argomentiamo perch√© hanno fatto questa scelta a livello di architettura.

Microsoft ha negoziato con OpenAI che i modelli GPT-4 forniti tramite Azure OpenAI per Copilot **non facciano learning sui dati elaborati**. Microsoft stessa sottolinea: "**Prompts, responses, and data accessed through Microsoft Graph _aren't used to train foundation LLMs_**, including those usati da Copilot"[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important). Questo √® un impegno contrattuale: scritto nel Microsoft Product Terms che i dati del cliente non alimentano modelli base. Quindi diversamente dai servizi consumer, qui l'output dell'AI per un cliente non migliora l'AI per altri clienti.

Inoltre, Microsoft afferma di aver **disabilitato il monitoring con revisione umana** che era presente di default in Azure OpenAI: "while abuse monitoring (with human review) is available in Azure OpenAI, Microsoft 365 Copilot services have **opted out** of it"). 

> Ci tengo comunque a notare che che modelli come quelli di OpenAI (GPT) hanno filtri integrati, quindi risponderanno "non posso fare X" a richieste vietate, ma quell'evento non viene inoltrato a nessuna persona.

#### Conservazione e controlli enterprise
Questo √® uno dei punti che ha permesso a Copilot di essere uno dei pi√π scelti nel panorama aziendale italiano. Vediamo meglio ora perch√©:

- **Boundary di servizio e residenza dati:** Microsoft 365 Copilot √® integrato nell'ecosistema M365, quindi i dati trattati rientrano nell'**Enterprise Trust**. Copilot risiede su infrastruttura **Azure nella regione scelta**: per clienti UE, √® dichiarato conforme all'EU Data Boundary, elaborando i prompt preferibilmente in datacenter UE (salvo necessit√† di fallback in altre regioni, comunque con garanzie). A marzo 2024, Microsoft ha aggiunto Copilot tra i servizi coperti dagli impegni di residenza dati e Multi-Geo. Quindi, un'azienda pu√≤ stare certa che i suoi prompt e risposte rimangono in quell'ambito geografico come da contratto.  

- **Conservazione storici e eliminazione:** i **prompts e responses** di Copilot Chat non sono conservati a lungo termine dall'app: vengono per√≤ registrati in "Copilot activity history" disponibile per l'utente, e anche esportabili via Content Search e Teams Export API. Se un'azienda vuole, pu√≤ **configurare policy di retention** su questi dati con Microsoft Purview, ad esempio per cancellare le interazioni dopo X giorni o conservarle per eDiscovery. L'utente stesso ha un controllo: pu√≤ andare sul portale account Microsoft e cancellare il proprio storico Copilot (funzionalit√† "Delete your Copilot activity history"). Microsoft memorizza i log di interazioni come parte di Exchange Online mailboxes (pare li equipari ad altri dati utente). In mancanza di policy custom, tali dati potrebbero conservarsi a tempo indefinito come parte account, ma presumibilmente soggetti a stesse regole di retention di altri file (es. se l'azienda ha default 5 anni su mailbox, potrebbero applicarsi le medesime regole).

- **Controlli amministratore:** Microsoft 365 admin pu√≤ **abilitare/disabilitare** Copilot per l'organizzazione o per utenti specifici tramite licenze. Pu√≤ gestire le **impostazioni** su cosa Copilot pu√≤ accedere. Il tenant admin pu√≤ controllare l'**invio di feedback a Microsoft**: c'√® una policy "Allow Recommender System Feedback" centralizzabile (come citato, Microsoft permette feedback utente e potenzialmente li usa in aggregate per migliorare). L'admin pu√≤ disattivare questa "telemetria" (termine che ho sempre sentito solo in inglese e tento una traduzione forzata) se preferisce niente condivisione.  
- **Moderazione contenuti:** Microsoft afferma di aver integrato filtri per bloccare output inappropriati (usando i filtri OpenAI e regole proprie di Microsoft). Se un utente tenta di generare, ad es., uno scenario offensivo, Copilot rifiuter√†. 
- **Trasparenza per utenti finali:** ogni risposta di Copilot fornisce **citazioni** con link alle fonti interne usate (documenti o mail), cos√¨ l'utente sa da dove viene il dato. Ci√≤ √® parte di governance (evitare "allucinazioni" incontrollate).  

In conclusione Microsoft adotta un approccio **walled garden** per i dati di Copilot: i prompt restano del cliente, non diventano parte di un modello pubblico.

Ovviamente Microsoft potrebbe utilizzare _metadati anonimi_ (tipo "il 30% degli utenti chiede riassunti di meeting"), ma non il contenuto specifico. Questo √® stato [ribadito in risposte pubbliche](https://learn.microsoft.com/en-us/answers/questions/5298757/does-microsofts-copilot-pro-for-office-365-use-you): ad es. quando gir√≤ voce su un "Copilot setting: use my data to train", Microsoft chiar√¨ trattarsi di un misunderstanding, confermando _no training on customer data_.


### Meta - Llama

#### Dati raccolti:

Meta (che altro non √® che il gruppo che contiene Facebook, Instagram e Whatsapp) √® entrata nell'AI generativa con vari prodotti nel 2023: modelli open-source (Llama 2) e soprattutto **Meta AI assistant** integrato in Messenger, Instagram, WhatsApp e occhiali Ray-Ban.

Meta sta anche costruendo modelli multimodali ("Emu" per immagini). La gestione dati di Meta AI ha due componenti: 

- i **dati di addestramento iniziale** dei modelli, molti dei quali provenienti dal web (incluse piattaforme Meta)
- i **dati generati dall'uso** dei chatbot.

Per il primo punto, Meta ha annunciato che inizier√† a [usare **contenuti pubblici**](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702) di Facebook e Instagram (post, commenti visibili a tutti) per addestrare i suoi LLM. In USA [lo fa gi√† da anni "silenziosamente"](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=While%20Meta%20has%20been%20training,data%20to%20train%20AI%20models), in EU, grazie al GDPR, aveva messo pausa nel 2024, ma da aprile 2025 ha iniziato con notifiche agli utenti. 

Meta dice che _non_ utilizzer√† i dati di **messaggi privati** (Messenger, WhatsApp, DM IG) n√© i contenuti non pubblici degli utenti per trainare modelli. Inoltre **esclude i dati di minori (<18) in EU anche se pubblici**. 

Spero che sia chiaro il messaggio: se scrivo un post pubblico potrebbe finire nel dataset di training di Llama; se scrivo un messaggio privato a un amico, Meta afferma di non usarlo nel dataset.

Per il secondo punto, quando si chatta con **Meta AI** su Messenger/IG, i dati raccolti includono: l'**input utente** (messaggio di chat, foto inviata al bot), la **risposta generata** dall'assistente, e eventuali _interazioni_ (feedback, se utente mette like al messaggio del bot, ecc.). Meta in app ha avvisato: "Le tue interazioni con Meta AI potranno essere utilizzate per addestrare e migliorare i modelli AI" (notifica in-app). Quindi presumibilmente, a meno di opt-out, i messaggi che scriviamo al bot e le risposte vengono loggati e usati dal team AI per tarare i modelli. 

Vi consiglio di provare almeno una volta ad usare questo chatbot su Whatsapp, Instagram e Messenger anche solo per leggere questi avvisi che ho appena citato.

Direi che abbiamo parlato ampiamenti dell'uso dei dati di training e andrei oltre.

#### Conservazione e protezione

Meta non ha fornito tempi precisi di retention per i prompt/risposte utente con AI. Essendo integrato in Messenger/IG, quelle chat con AI potrebbero essere trattate come normali messaggi (conservati finch√© l'utente non li elimina, e comunque salvati su server finch√© l'account risulta attivo). Meta tende a conservare i dati _fintanto che utili_, a meno di richiesta di cancellazione. Le conversazioni con bot potrebbero essere soggette a moderazione (Meta filtra contenuti per policy).

Tutti gli altri dati, come post sui vari social del gruppo Meta, restano conservati finch√© rimangono su tale piattaforma.

#### Revisione umana

Sapendo [come Meta si preoccupa di gestire certi contenuti sui suoi social](https://transparency.meta.com/policies/improving/prioritizing-content-review/), penso che analogamente Meta impieghi testers umani. Il rischio di privacy √® che se uno confida segreti al bot, degli annotatori possano leggerli. Meta ha evitato di incoraggiare ci√≤, infatti ha messo quell'avviso chiaro "non inserire info confidenziali" quando usi Meta AI (simile a Google e OpenAI). Probabile che Meta abbia un programma di review come gli altri: una percentuale di chat col bot viene analizzata per capire errori, e utilizzata per RLHF. 

A tal riguardo per√≤ non ho trovato molte fonti accurate.

#### Controlli utente
Come negli altri capitoli, parliamo ora di _cosa pu√≤ fare l'utente per la sua privacy_

- **Notifiche e modulo opt-out EU:** come detto, ogni utente EU l'anno scorso ha ricevuto una notifica su FB/IG con spiegazione e link a un **form** per opporsi. Il form sul centro assistenza chiede di confermare account e preferenze (distingue: vuoi opporti all'uso di post, o di immagini, etc). Meta afferma che chi invia l'opposizione verr√† escluso _"at any time"_ (non c'√® scadenza).  

- **Privacy Center:** : Meta non offre un toggle one-click (essendo su base legale LI, serve che l'utente dichiari opposizione).  

- **Eliminazione dati:** l'utente pu√≤ eliminare i propri post dal social, impedendo cos√¨ quell'uso. Le conversazioni col bot pu√≤ cancellarle come chat normali. Ma se gi√† usate nel training, restano nel modello (non retroattivo). 

- **Controlli per minori:** per default sotto 18 esclusi, quindi non serve fare nulla.

- **Trasparenza e audit:** Meta pubblica _AI system cards_ per i propri modelli.[Llama4](https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/ ma non sono fornite informazioni sui dati di training. 

### DeepSeek: il primo chatbot Made in China

**DeepSeek** √® un'azienda cinese con un chatbot con lo stesso nome. Questa azienda √® nata nel 2023, disponibile via web e mobile. 

Ha modelli simil-GPT e funzioni di ricerca. Sul fronte dati, voglio spendere due parole su DeepSeek perch√© √® un caso notevole in quanto **colleziona quasi tutto** ci√≤ che pu√≤ degli utenti. Vediamo bene [nel dettaglio](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html):
- **Dati account:** email, telefono, nome utente, password (per creare account).  
- **Input utente (prompt e file):** qualsiasi testo o audio immesso nella chat viene raccolto e memorizzato. Questo include _intere conversazioni_, file caricati, immagini date in input. La Privacy Policy esplicita: "**we may collect your text or audio input, prompt, uploaded files, feedback, chat history**". In breve, **l'intera cronologia chat e allegati** √® nei server.

- **Output generati:** anche le risposte del bot ("Outputs") possono essere conservate (spesso includono info dell'utente rielaborate, quindi considerate parte dei dati utente).  

- **Dati di dispositivo e rete:** l'app raccoglie **modello dispositivo, OS, indirizzo IP, lingua, identificatore device, log di sistema, dati di crash e performance**. Inoltre monitora le **azioni** in-app (quali funzioni usi, quante query fai). Utilizza cookie e tecnologie simili per tracciare l'uso (sul web e mobile). Un [articolo su Wired](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=A%20WIRED%20review%20of%20the,owner%20ByteDance%20%E2%80%9Cand%20its%20intermediaries) ha scoperto che carica anche librerie di tracking cinesi (Baidu Tongji analytics), e invia dati base a ByteDance.  
- **Dati di input taciti:** un aspetto inquietante, DeepSeek pu√≤ raccogliere ["**keystroke patterns or rhythms**"](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20with%20all%20digital%20platforms%E2%80%94from,analyze%20how%20you%20use%20our), ovvero la dinamica di digitazione, e altri parametri di comportamento (forse per fingerprinting).  
- **Dati da terze parti:** se si effettua login con Google/Apple, prende i dati profilo base. Inoltre, dichiara che pu√≤ ricevere dati da pubblicit√† e cookie ID per linkare attivit√† fuori dal servizio.

In pratica, DeepSeek raccoglie non solo i **contenuti** delle comunicazioni, ma anche un quadro **tecnico completo** dell'utente e di come interagisce.

#### Finalit√† d'uso

Ok ma perch√© tutte queste informazioni?

La [Privacy Policy](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=How%20We%20Use%20Your%20Information) elenca:  
- Fornire e mantenere il servizio (es. memorizzare la chat per continuit√† conversazionale) 
- **Addestrare e migliorare modelli**: esplicitamente, ["to train and improve our technology, e.g. our ML models"](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=user%20support.%20,the%20Platform%2C%20such%20as%20by), compreso monitorare interazioni e analizzare come la gente usa il servizio. Questo passaggio indica chiaramente che DeepSeek utilizza i prompt degli utenti come _dati di addestramento_ per potenziare le sue AI.  
- Comunicazioni con l'utente (email di servizio, ecc.).

- **Conformit√† legale:** forniranno dati alle autorit√† se richiesto e se necessario per proteggere la [salute/vita di qualcuno]((https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=emergency%20services%20with%20information%20in%20urgent%20situations%20to%20protect%20health%20or%20life.)). Quest'ultima √® standard, ma nel contesto cinese significa che i dati possono essere condivisi con le forze dell'ordine locali se ritenuto opportuno.

La base legale varia: per utenti in UE, come da loro tabella, usano _performance contrattuale_ per dati base, _legittimi interessi_ per migliorare il servizio e sicurezza, _consenso_ per cose come cookie e eventuali richieste (anche la creazione account se minorenne 14-18). Per√≤, essendo in Cina, di fatto la compliance GDPR √® sulla carta (non hanno ancora un rappresentante in UE).

#### Uso per training modelli
Qui abbiamo il caso opposto di Copilot e posso rispondere velocemente. **S√¨, by default e senza opt-out**. 

Tutto ci√≤ che digiti pu√≤ essere usato per addestrare i modelli DeepSeek o affini. Non c'√® indicazione di esclusione se non smettere di usarlo. 

Inoltre, non esistono piani business con esclusione: √® un prodotto consumer (anche se vendono API, ma non menzionano escluderle dal training). Quindi presumibile qualsiasi prompt inviato va a un dataset centrale.

#### Retention e archiviazione

DeepSeek [dichiara](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=How%20Long%20Do%20We%20Keep%20Your%20Information) di conservare i dati _finch√© necessari a fornire il servizio e per le altre finalit√†_. Inoltre, li tiene per obblighi legali e interessi legittimi (sviluppo e difesa legale). Non fornisce un tempo fisso (es. X anni), dipende dal tipo: dice _finch√© hai un account manteniamo info account e input_. 

Quindi, possedendo un account, presumibilmente l'intera chat history viene tenuta a tempo indefinito sul loro cloud (a meno che tu la cancelli manualmente).

Una questione critica √® **dove** vanno questi dati: DeepSeek informa apertamente che **"we store the information we collect in secure servers located in the People's Republic of China"**. Ci√≤ significa che tutti i prompt, file, chat, ecc. finiscono su server in Cina soggetti alle leggi cinesi. Ci√≤ comporta che, in base alla legge cinese sulla cybersicurezza e intelligence (es. National Intelligence Law 2017), le autorit√† possono richiedere accesso a quei dati per motivi di sicurezza nazionale. Dunque un utente USA/EU che pensasse di avere confidenzialit√†, in realt√† i suoi dati possono essere visti dal governo cinese se necessario (e l'utente non lo sapr√†).

**Revisione umana:** 

Penso che tutti sappiano che questo chatbot non risponde nulla quando si chiede cos'√® successo in piazza Tiananmen. In caso contrario ti consiglio di approfondire. 

Ci√≤ implica che i prompt degli utenti passano in un sistema di filtro e potenzialmente segnalazione. Essendo un'azienda cinese, se un utente scrive roba vietata, quell'informazione potrebbe essere isolata e trasmessa a chi di dovere (non confermato, ma plausibile). 

Il capitolo "miglioramento modelli" merita una menzione particolare. Certamente i tecnici di DeepSeek esamineranno conversazioni utente in massa per rifinire l'AI. Non c'√® menzione di offuscamento PII, e magari i dati spediti in Cina includono tutto, in tal caso i revisori potrebbero vedere nomi o dati privati.

#### Controlli utente
Ultima volta in cui ci facciamo la celebre domanda, quindi noi cosa possiamo fare?

- **Cancellazione chat:** l'app permette di eliminare la cronologia chat (sull'interfaccia c'√® "Delete all chats" in impostazioni). Se uno usa quell'opzione, immagino rimuovano la visibilit√† di chat dall'account e dal database primario. Resta il dubbio se la tengano in backup. Comunque, l'utente ha almeno quell'opzione per la propria console. Wired l'ha documentato passo passo (non vi sto a linkare ancora il sito).

- **Cancellazione account:** bisogna scrivere a loro (<privacy@deepseek.com>) per richiedere cancellazione dati, secondo la policy. 

- **Opt-out training:** non previsto esplicitamente. Nella policy c'√® una [frase](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=services%20with%20information%20in%20urgent,to%20protect%20health%20or%20life): "Se ti rifiuti di permetterci di trattare i dati come sopra descritto, devi mandare una richiesta via email". Quindi l'unico modo √® scrivere e supplicare di non usare i tuoi dati.

#### Conclsuione

Molti esperti consigliano di **non inserire informazioni personali o sensibili** in quell'app (cosa valida per tutte le AI, ma qui ancora di pi√π).

Servizi come [Proton](https://proton.me/blog/deepseek) e [ExpressVPN](https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj#) hanno pubblicato avvisi evidenziando i punti: tracciamento intenso, condivisione con ByteDance e Baidu, ecc.

Diciamo quindi che per quanto fosse figo usare i loro modelli di reasoning all'inizio, oggi, finch√© non sistemano questi problemini di privacy, sarebbe meglio pensarci bene prima di usarlo.


## Confronto sintetico tra piattaforme LLM principali

Iniziamo a tirare le somme. Per avere una visione d'insieme, la tabella seguente confronta i vari provider di chatbot/LLM rispetto ad alcuni parametri chiave di gestione dati:

_Tabella 1: Confronto pratiche dati e opzioni privacy dei fornitori LLM (agg. 2025)._  
_Fonti: Policy ufficiali e documentazione dei rispettivi servizi che abbiamo sovramenzionato_

| **Vendor (Prodotto)** | **Ambito** | **Dati utente usati per training (default)** | **Opt‚Äëout/Opt‚Äëin** | **Conservazione dati utente** | **Revisione umana** | **Controlli e garanzie** | **Agg. policy** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **OpenAI (ChatGPT)** | Consumer (Web/App) | S√¨ (conversaz. e feedback) | Opt‚Äëout disponibile (cronologia off) | Indefinita se _history on_; 30 giorni se _off_ | S√¨, su campioni (pseudonimiz.) | Toggle "Do not train", Privacy Portal per opposizione, Chat effimera | Apr 2023 (Italia DPA), Jun 2025 (Enterprise) |
| **OpenAI (API / Enterprise)** | Business/Dev | No (per default) | Opt‚Äëin facoltativo (feedback) | Log 30 giorni (standard); opzione _zero log_ | No (opt‚Äëout human monitoring) | Data encryption, SOC2; admin controls retention | Mar 2023 (no-train API); Aug 2023 (ChatGPT Ent.) |
| **Google (Gemini/Bard)** | Consumer (Account Google) | S√¨ (prompt & conv.) | Opt‚Äëout disponibile (disattiva "Attivit√† Gemini") | 18 mesi predef. (3-36 mesi configur.); ~72h se off | S√¨, limitata (0.2% conv.) | Toggle in MyActivity (UE: diritto opposizione); elimina cronologia chat | Jul 2023 (Bard EU); Nov 2025 (Privacy Hub Workspace) |
| **Google (Workspace AI)** | Business (Gsuite) | No (rimane nel dominio) | N/A (opt‚Äëin richiesto se share) | Session only (no retention prompt); conv. facolt. salvabili 3-36 mesi | No (nessuna review esterna) | Admin: abilita/disabilita AI; dati in EU boundary | Nov 2023 (Workspace Hub) |
| **Anthropic (Claude)** | Consumer (Free/Pro) | _Dal Q4 2025:_ S√¨ (se utente consente) | _Opt‚Äëout possibile_ (toggle "no training"); default = opt‚Äëin (toggle on) | 5 anni se training on; 30 giorni se off | S√¨ (su dati opt-in) | Privacy Settings per scelta training; cancella chat; no app EU ancora | Ott 2025 (nuovi Termini) |
| **Anthropic (Claude Ent/API)** | Business/API | No (mai usati x training) | N/A (di default esclusi) | 30 giorni log (per abusi) | No | Contract: dati isolati; opzione self-host in cloud privato | Lug 2023 (Claude 2) |
| **Microsoft (365 Copilot)** | Enterprise (M365) | No | N/A (nessun training est.) | Conservato come dati M365 standard; controllabile via retention policy | No (opt‚Äëout da monit.) | DPA e Product Terms (no data use); EU Data Boundary; user delete history | Mar 2024 (Prod. Terms, EU boundary) |
| **Meta (AI Assistant)** | Consumer (Facebook, IG) | S√¨ (post/commenti pubblici; chat AI) | Opt‚Äëout modulo (UE) - diritto opposizione | N/D (simile ad altri msg FB); posts finch√© online | S√¨ (per moderazione & RLHF) | Notifiche in-app; form "Generative AI" UE; promesse: niente dati privati | Apr 2025 (EU roll-out AI) |
| **Meta (business data)** | Enterprise (Workplace) | No (esclusi training) | N/A | Norme interne (customer data isolati) | No | DPA Meta, sub-processors list | - |
| **DeepSeek** | Consumer (Global) | S√¨ (tutto prompt/chat) | No opt-out (implicit consent) | Finch√© account attivo (no scadenza definita) | S√¨ (accesso staff + poss. autorit√† cinesi) | Cancella chat manuale; nessuna garanzia su trasferimenti (-> Cina) | Feb 2025 (policy PIPL) |

_Legenda:_

_- "Ambito" distingue se servizio consumer o business;_

_- TrainingUseDefault indica se i dati utente alimentano il training del modello per default;_ 

_- Opt-out/in indica se l'utente pu√≤ opporsi o deve acconsentire;_

_- Retention √® il periodo tipico di conservazione dei contenuti utente;_

_- HumanReview se avviene revisione manuale routine;_

_- Controls evidenzia gli strumenti di controllo dati forniti;_

_- Agg. policy d√† ultimo aggiornamento significativo)_

### Osservazioni dalla tabella 

OpenAI, Google e Meta nel contesto consumer adottano un approccio simile: **inclusione predefinita** dei dati utente nel ciclo di miglioramento dell'AI, mitigata dalla possibilit√† di opt-out pi√π o meno nascosta (OpenAI con toggle in UI, Google nell'account, Meta via modulo). Al contrario, i servizi rivolti alle imprese (OpenAI Enterprise/API, Google Workspace, MS Copilot) sono impostati per **non riutilizzare i dati** dei clienti nel training, per rassicurare su privacy e conformit√†: questi di fatto trattano i dati come un cloud privato dell'azienda. 

Il caso Anthropic spicca per il cambio recente: da full opt-out sono passati a un sistema di _consenso obbligatorio_ (l'utente deve scegliere, con default verso training), il che l'ha allineata a OpenAI/Google (in termini pratici, molti utenti lasceranno attivo e i dati verranno usati). 

Sul fronte **retention**, vediamo variabilit√†: da log brevissimi (30 gg) per API di OpenAI e ChatGPT con history off, fino a periodi lunghi come 5 anni se l'utente acconsente (Anthropic) o indefiniti (OpenAI con history on). Google d√† controllo granulare ma di base tiene 18 mesi. Microsoft e Workspace mantengono i dati allineati alle normali retention enterprise (quindi potenzialmente per molti anni se l'azienda li archivia). 

Per **revisione umana**, tutti i consumer (tranne DeepSeek che √® un mondo a parte) indicano un certo grado di intervento umano su dati utente, generalmente _limitato e pseudonimizzato_. Microsoft e Workspace affermano di non farne affatto per i dati cliente. Su **trasparenza e controlli**, Microsoft appare la pi√π restrittiva/protettiva, seguita da OpenAI enterprise; Google e OpenAI consumer forniscono toggles ma richiedono consapevolezza da parte dell'utente; Meta offre il modulo di opt-out in UE ma manca un banale switch in impostazioni (il che potrebbe ridurre le opposizioni per attrito). DeepSeek, infine, rappresenta un _worst-case_: l'utente non ha praticamente voce in capitolo, e i dati finiscono in un contesto di minore tutela giuridica.

## Trasparenza, governance e protezioni: tendenze attuali

Andiamo per gradi, iniziamo a capire in primis quali sono i controlli per l'utente finale

### Controlli per l'utente finale

In risposta alle crescenti preoccupazioni, i provider di AI generativa hanno introdotto strumenti che permettono agli utenti di gestire in parte i propri dati. Ad esempio, quasi tutti ora offrono un modo per **disattivare la conservazione della cronologia** (ChatGPT, Bard/Gemini) o cancellare facilmente le conversazioni (Anthropic, ChatGPT, etc.). 

I servizi includono disclaimer chiari del tipo _"non inserire informazioni sensibili"_, cercando cos√¨ di mitigare il rischio reputazionale. L'utente ha poi diritti esercitabili: in UE, il **diritto di opposizione** (usato da Meta, OpenAI) consente di escludersi dal training globale, mentre il **diritto di accesso/cancellazione** permette di ottenere o eliminare i dati grezzi (OpenAI e Google offrono esportazione e cancellazione account dal portale privacy).

### Strumenti per amministratori e aziende: 

Per le organizzazioni, la governance dei dati AI √® diventata molto importante. I fornitori offrono **dashboard amministrative** dove abilitare/disabilitare funzionalit√† e impostare policy. Ad esempio, [**OpenAI Enterprise**](https://openai.com/it-IT/enterprise-privacy/) consente di fissare la retention su 0-30 giorni e controllare l'accesso tramite SSO, e aggiunge audit logs per tracciare l'uso interno. 

**Microsoft** integra [Copilot](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy) con i suoi strumenti di compliance: un DPO aziendale pu√≤ ricercare tra gli storici Copilot tramite eDiscovery e imporre retention via Purview. 

**Google Workspace** analogamente lascia agli admin decidere se salvare conversazioni e [per quanto](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F), e garantisce che i connettori con dati aziendali rispettino i permessi gi√† in essere. Questi controlli a livello enterprise assicurano che l'adozione di AI non comprometta gli obblighi di privacy e riservatezza che un'azienda ha verso i propri dati: i fornitori si impegnano contrattualmente (con clausole di trattamento dati e allegati sicurezza) a isolare i dati dei clienti e a sottoporvisi ad eventuali **audit** o ispezioni da parte di terzi (ad es. OpenAI dichiara di aver superato audit [SOC 2](https://openai.com/it-IT/enterprise-privacy/)). 

Nei contratti enterprise figurano inoltre specifici **impegni di riservatezza**: Microsoft addirittura indennizza il cliente in caso i loro dati venissero usati per addestrare modelli pubblici contro le policy. In UE, questi accordi costituiscono _Clausole Contrattuali Standard_ per export dati (USA), e molti vendor offrono anche **scelta regione** (OpenAI ha data centers in Europa tramite Azure, Microsoft anche ha EU Data Boundary e Google Cloud permette di selezionare region EU) per soddisfare requisiti di **residenza dei dati**.

### Audit e verifiche indipendenti 

A livello di accountability, alcune iniziative emergono:  
- Le autorit√† privacy (es. il Garante italiano, il CNIL francese, la ICO UK) stanno indagando sulle pratiche di raccolta dati di servizi come ChatGPT. Questi interventi forzano maggiore trasparenza (OpenAI ha dovuto rivelare i tempi di conservazione e basi legali ai garanti). In prospettiva, mi aspetto che l'**AI Act UE** imporr√† ai fornitori di modelli generativi obblighi di documentazione sul training data e meccanismi per segnalare e rettificare dati personali nei dataset. Ci√≤ potrebbe introdurre audit esterni periodici. Il discorso sull'AI Act merita un altro articolo, siamo gi√† andati troppo per le lunghe.

- Per clienti enterprise, in mancanza di normative specifiche, vale la **"due diligence"**: molte aziende richiedono ai vendor AI **questionari di sicurezza e privacy** (sul modello delle checklist ISO27001) e clausole severe. Abbiamo gi√† visto casi: ad esempio, 

Mi ricordo di un articolo su HDBlog. [Nel 2023 Samsung viet√≤ ai dipendenti di usare ChatGPT](https://www.hdblog.it/samsung/articoli/n569257/samsung-vieta-dipendenti-chatgpt/) dopo che del codice interno era trapelato nei prompt. Poi valut√≤ di fornire un LLM interno o usare API con logging disabilitato. 

Questo ha portato i vari provider a offrire soluzioni dedicate (OpenAI ad esempio lavora su istanze GPT private per aziende con dati sensibili, come il caso di Samsung).

### Differenze geografiche 

Va notato che molti controlli sono stati lanciati in risposta a regolamentazioni o pressioni in specifiche aree: l'Europa ha spinto per il consenso/opt-out (Italia vs ChatGPT, UE vs Meta), mentre negli USA il focus √® minore (l√¨ ad es. ChatGPT non offre toggle training, √® attivo per default e l'utente deve scoprirlo nelle impostazioni; idem Bard). 

Ci√≤ crea un potenziale scenario frammentato: ad esempio, **Meta consente opt-out solo agli europei per ora** (o meglio si [poteva](https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/), non so se e nel caso come √® possibile farlo ora). 

OpenAI stessa scrive che i diritti privacy variano a seconda delle leggi locali. 

Aziende come **DeepSeek** evidenziano un altro rischio: giurisdizioni con leggi meno tutelanti possono essere base per servizi globali. Qui la governance dipende dalla consapevolezza utente e da eventuali blocchi (ExpressVPN consiglia di non usarlo, ma non c'√® un Garante che possa intervenire facilmente data la sede extra-UE).

### Sicurezza dei dati

Oltre alla privacy, i provider mettono enfasi sulla **sicurezza**: criptaggio, controlli accesso, audit logging interno. OpenAI ad esempio ha implementato crittografia a riposo e monitoraggio degli accessi ai dati di conversazione (dicono che solo un numero limitato di personale pu√≤ accedere e ogni accesso √® registrato). Microsoft e Google avendo gi√† infrastrutture cloud mature, estendono quelle misure ai sottosistemi AI (nel doc [Microsoft](https://learn.microsoft.com/en-us/compliance/assurance/assurance-encryption-in-transit): "data is encrypted at rest (AES-256) and in transit (TLS 1.2+)"). 

Inoltre c'√® il tema **data leakage**: i modelli potrebbero rigenerare testi presi dai prompt di altri utenti (come successo a Samsung via ChatGPT). I provider lavorano su tecniche per mitigare (OpenAI riduce la probabilit√† che dati sensibili rimangano nei pesi, e ha threshold su lunghezze per non memorizzare numeri di carta, ecc.). Questo rientra nella governance tecnica.

## Conclusione

Le logiche di profilazione e sfruttamento dati viste in Cambridge Analytica (CA), ovvero la raccolta massiva, elaborazione machine learning, uso sperimentale, oggi rivivono potenziate nell'ambito dell'AI generativa. La differenza chiave √® che nel caso CA l'obiettivo era influenzare gli utenti stessi (targeting comportamentale esterno), mentre nel caso dei chatbot l'obiettivo √® "influenzare" l'**intelligenza artificiale** addestrandola sui dati utente. In entrambi i casi, per√≤, c'√® uno **sbilanciamento informativo**: gli utenti forniscono inconsapevolmente dati (like, conversazioni) ignari di come verranno sfruttati a monte. Serve quindi una forte enfasi su **trasparenza** (chiare informative come quelle introdotte post-sanzioni) e su meccanismi di **governance** che diano controllo alle persone e alle organizzazioni sui propri dati. I passi compiuti finora, come per esempio i toggle, moduli, e contratti, sono sicuramente un buon primo passo da parte delle aziende, ma richiedono vigilanza continua da parte nostra. 

L'esperienza Cambridge Analytica ha insegnato che usare i dati personali senza adeguati freni pu√≤ avere impatti sociali enormi. Vorrei mettere il focus non solo su come vengono usati questi dati, ma anche sull'output dei modelli in base ai nostri dati (prima erano dei post, oggi sono le risposte dei chatbot).

Oggi serve pi√π che mai un'educazione all'uso critico di questi strumenti e dalla veridicit√† delle informazioni.

Questa educazione oggi √® pi√π che mai importante.

I grandi provider devono garantire che rispettino la privacy e la volont√† degli utenti. Questo √® fondamentale per un'adozione responsabile e sostenibile dell'AI nella societ√† e una fiducia solida da parte degli utenti.

## Bibliografia annotata

- **ICO (2018)** - _"Investigation into the use of data analytics in political campaigns - Update"_, **Information Commissioner's Office (UK)**. Rapporto ufficiale dell'Authority UK che indaga il caso Cambridge Analytica[\[1\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=The%20total%20number%20of%20users,this%20with%20other%20sources%20of)[\[31\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=Facebook%20to%20be%20approximately%2087,with%20personalised%20advertising%20during%20the). Fonte primaria per numeri (87 milioni coinvolti) e descrizione dell'app GSR (dati estratti, OCEAN, condivisione con SCL)[\[23\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=%E2%80%A2%20Public%20profile%20data%2C%20including,to%20be%20approximately%2087%20million)[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the). Rilevante perch√© conferma da audit forense la pipeline tecnica e le violazioni di legge.
- **UK Parliament DCMS Committee (2019)** - _"Disinformation and 'fake news': Final Report"_, **House of Commons**[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter)[\[193\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=the%20company%20claimed%20that%20there,turnout%2C%20for%20the%20targeted%20groups). Inchiesta parlamentare sulle interferenze digitali. Contiene testimonianze di Wylie, Kogan, Nix e risultati (efficacia dichiarata: +30% turnout)[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter). Utile per comprendere retroscena e contraddizioni nelle versioni fornite dai protagonisti (es. Nix neg√≤ l'uso dei dati GSR, poi ritratt√≤[\[24\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=108,which%20he)[\[194\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=109,of%20events%2C%20in%20February%202018)) e per metriche di successo vantate da SCL[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter).
- **Hindman (2018)** - _"This is how Cambridge Analytica's Facebook targeting model really worked"_, **Nieman Lab**[\[195\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=The%20researcher%20whose%20work%20is,Netflix%20uses%20to%20recommend%20movies)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed). Articolo di analisi che, grazie a informazioni dirette di Kogan, spiega il funzionamento statistico del modello CA (approccio tipo Netflix Prize con SVD)[\[195\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=The%20researcher%20whose%20work%20is,Netflix%20uses%20to%20recommend%20movies)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed). Evidenzia che le "psicografiche" erano in realt√† correlate a demografia, sgonfiando la narrativa mistica. Fonte secondaria ma autorevole e basata su fonte diretta.
- **Kosinski et al. (2015)** - _"Facebook as a research tool for the social sciences"_, **Cambridge University Press** (riassunto citato in DCMS Interim)[\[196\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=2014%2C%20where%20he%20re,to%20this%20approach%2C%20stating%20that)[\[197\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=orientation%E2%80%9D,to%20this%20approach%2C%20stating%20that). Articolo accademico cardine che dimostr√≤ come i "Like" predicessero personalit√† e altri tratti sensibilissimi (orientamento sessuale, idee politiche)[\[196\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=2014%2C%20where%20he%20re,to%20this%20approach%2C%20stating%20that). Importante per capire le basi psicometriche sfruttate poi da CA e per evidenziare i rischi etici gi√† segnalati allora[\[197\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=orientation%E2%80%9D,to%20this%20approach%2C%20stating%20that).
- **OpenAI Help Center (2025)** - _"How your data is used to improve model performance"_, **OpenAI**[\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out)[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform). Spiega la policy attuale di OpenAI sui dati: distinzione netta consumer vs API[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform), possibilit√† di opt-out e portal privacy. Fonte primaria aggiornata (novembre 2025) con language chiaro sulle finalit√† e sui controlli offerti agli utenti.
- **OpenAI (2025)** - _"Enterprise privacy at OpenAI"_, **OpenAI Blog**[\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data)[\[198\]](https://openai.com/enterprise-privacy/#:~:text=Does%20OpenAI%20train%20its%20models,on%20my%20business%20data). Descrive gli impegni per clienti business: niente training su dati business, controllo retention, possesso di input/output dall'utente[\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data)[\[198\]](https://openai.com/enterprise-privacy/#:~:text=Does%20OpenAI%20train%20its%20models,on%20my%20business%20data). Rilevante per evidenziare come vendono la sicurezza dei dati enterprise e quali garanzie contrattuali (SOC 2, encryption) forniscono.
- **Google Support (2025)** - _"Gemini Apps Privacy Hub"_, **Google**[\[181\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies)[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). Documento comprensivo che elenca categorie di dati raccolti (prompt, file, device info)[\[67\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Apps)[\[68\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,settings%2C%20device%20type%20and%20settings) e spiega uso (improvement modelli, revisori umani)[\[181\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies) e controlli (MyActivity per disattivare retention)[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). Fonte primaria cruciale per le policy Google generative AI, con focus utente consumer.
- **Google Workspace Privacy (2025)** - _"Generative AI in Workspace Privacy Hub"_, **Google**[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission)[\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F). Specifica per contesto enterprise: afferma nero su bianco il principio "i contenuti rimangono nell'organizzazione, non usati per training esterno"[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). Include tabella retention differenziata (session ephemeral vs conversazioni facoltative 3-36 mesi)[\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F). Serve per mostrare l'approccio pi√π cauto in ambito aziendale.
- **Anthropic Blog (2025)** - _"Updates to our Consumer Terms and Privacy Policy"_, **Anthropic**[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention)[\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=). Comunicato ufficiale del cambio di policy: rivela il passaggio a opt-in e l'estensione retention a 5 anni per dati condivisi[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention), nonch√© la non applicabilit√† ai clienti enterprise[\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI). Fonte diretta per comprendere la strategia di Anthropic di bilanciamento fra miglioramento modelli e feedback utenti.
- **Wired (2025)** - _"Anthropic Will Use Claude Chats for Training Data. Here's How to Opt Out"_, **WIRED**[\[98\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models)[\[199\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=The%20new%20privacy%20policy%20also,under%20the%2030%20day%20policy). Articolo divulgativo che conferma e commenta la mossa di Anthropic, notando che di default gli utenti sono inclusi (toggle preattivato)[\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy) e paragonando alle impostazioni di OpenAI e Google[\[97\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Prior%20to%20Anthropic%20updating%20its,user%20chooses%20to%20opt%20out). Utile per la prospettiva critica e per estrarre info come la tempistica (ritardo al 8 ottobre)[\[100\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20developers%20hope%20to%20make,of%20their%20chatbot%20over%20time).
- **Microsoft Learn (2023)** - _"Data, Privacy, and Security for Microsoft 365 Copilot"_, **Microsoft Docs**[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important)[\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot). Documentazione tecnica che dettaglia il funzionamento di Copilot: chiarisce che prompt e dati Graph non addestrano gli LLM[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important), e spiega come i dati sono trattati internamente (Copilot activity history)[\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot). Fonte primaria essenziale per illustrare il "no data mingling" di Microsoft e le opzioni di retention via Purview[\[185\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot).
- **TechCrunch (2025)** - _"Meta to start training its AI models on public content in the EU"_, **TechCrunch**[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly). Notizia che riporta l'annuncio di Meta (e.g. citazioni: notifiche agli utenti, modulo opt-out, esclusione di privati e minori)[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[200\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,and%20the%20distinct%20ways%20different). Serve come supporto giornalistico per consolidare le info sulle policy Meta, basandosi anche sul blog ufficiale Meta (linkato nell'articolo)[\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes).
- **Meta Newsroom (2025)** - _"Making AI Work Harder for Europeans"_, **Meta**[\[201\]\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes). Comunicazione ufficiale di Meta che delinea il piano UE: conferma uso di post/commenti pubblici per training[\[202\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Today%2C%20we%E2%80%99re%20announcing%20our%20plans,their%20cultures%2C%20languages%20and%20history)[\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes), rispetto di decisione EDPB, e reitera che niente messaggi privati verr√† toccato[\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes). Fonte primaria di Meta, importante per citare testualmente gli impegni (es. _"non usiamo i messaggi privati per i nostri modelli generativi"_).
- **PrivacyGuides Forum (2025)** - Discussione _"Meta AI training in EU"_ citante TechCrunch[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models). Evidenzia estratti chiave dell'articolo TC (ad esempio la frase "Meta notes it doesn't use private messages or under-18 public data for training"[\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models)). Utile come check incrociato e per la dimensione comunit√† privacy.
- **DeepSeek Privacy Policy (2025)** - **DeepSeek Inc.**[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies)[\[168\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=The%20retention%20periods%20will%20be,necessary%20to%20process%20the%20violation). Testo integrale policy (EN) con dettagli sorprendenti: afferma esplicitamente la raccolta di prompt, file, chat history[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies) e l'archiviazione su server in Cina[\[203\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Where%20We%20Store%20Your%20Information). Fornisce la base per affermazioni su cosa raccoglie e dove vanno i dati, indispensabile per la sezione DeepSeek.
- **Wired (2023)** - _"DeepSeek's Popular AI App Is Explicitly Sending US Data to China"_, **WIRED**[\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D)[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D). Inchiesta giornalistica che analizza la privacy policy e il traffico di DeepSeek: conferma che i server sono in Cina[\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D), e avvisa che chat e file utente vengono inviati integralmente[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D). Anche rivela tracking verso Baidu e ByteDance[\[156\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=A%20WIRED%20review%20of%20the,owner%20ByteDance%20%E2%80%9Cand%20its%20intermediaries). Fonte di terza parte attendibile che d√† autorevolezza alle preoccupazioni su DeepSeek.
- **Proton (2023)** - Blog _"Using DeepSeek? Here's why your privacy is at stake"_, **Proton**. Articolo divulgativo che sintetizza i rischi di DeepSeek, menzionando punti simili a Wired (raccolta prompt e chat, niente opt-out). Non citato direttamente nel testo, ma influente per completare il quadro e verifica incrociata.
- **ExpressVPN (2023)** - _"Is DeepSeek safe? What happens to your data"_, **ExpressVPN Blog**[\[178\]](https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj#:~:text=Is%20DeepSeek%20safe%3F%20What%20happens,chat%20history%2C%20or%20other). Altro commentario che riprende i punti noti (dati inviati in Cina, ecc.) e consiglia di non usare app di dubbia provenienza per query sensibili. Serve come contesto e per mostrare che la comunit√† sicurezza prende sul serio questi aspetti.
- **Reddit GDPR (2023)** - Discussione _"DeepSeek privacy practices"_[\[204\]](https://www.reddit.com/r/gdpr/comments/1lk4baw/what_are_deepseeks_privacy_practices/#:~:text=DeepSeek%20collects%20three%20main%20categories,uploaded). Apporta conferma che DeepSeek raccoglie 3 categorie: dati account, chat ("prompts") e file, e che tutto √® soggetto a quell'informativa. Rileva come quell'approccio violi potenzialmente GDPR. Non citato nel testo finale, ma utilizzato in analisi per consolidare affermazioni.

[\[1\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=The%20total%20number%20of%20users,this%20with%20other%20sources%20of) [\[23\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=%E2%80%A2%20Public%20profile%20data%2C%20including,to%20be%20approximately%2087%20million) [\[26\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=In%20summary%2C%20the%20app%20accessed,implemented%20on%20their%20Facebook%20profile) [\[27\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=20%20%E2%80%A2%20Posts%20on%20the,again%2C%20subject) [\[28\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=to%20the%20settings%20they%20had,the%20app%2C%20was%20estimated%20by) [\[30\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=%E2%80%A2%20Photographs%20in%20which%20the,psychological%20patterns%20and%20build%20models) [\[31\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=Facebook%20to%20be%20approximately%2087,with%20personalised%20advertising%20during%20the) [\[32\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=OCEAN%20model%20and%20pioneer%20the,as%20ethnicity%20and%20political%20affiliation) ico.org.uk

<https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf>

[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the) [\[3\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39) [\[4\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups) [\[24\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=108,which%20he) [\[25\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%20breach%20originated%20at%20the,%E2%80%9D%2046) [\[29\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=103,The%20aim) [\[33\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that) [\[34\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Nix%20told%20us%3A%20%E2%80%9CWe%20do,41) [\[35\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=We%20are%20trying%20to%20make,109) [\[36\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,be%20relevant%20to%20understanding%20their) [\[38\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=3%20The%20issue%20of%20data,GSR%20and%20Cambridge%20Analytica%20allegations) [\[39\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%2C%20involving%20various%20organisations%20including,overseas%20elections%20in%20Chapter%206) [\[40\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to) [\[41\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=required%20under%20the%20contract%20to,electoral%20register%20in%20those%20states) [\[42\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=sharing%20of%20data%20in%20the,overseas%20elections%20in%20Chapter%206) [\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter) [\[44\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=92,Some%20of%20the) [\[46\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=100,43) [\[47\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=101,119) [\[49\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=conflicted%20with%20Mr%20Nix%E2%80%99s%20evidence%3B,as%20they%20were%20never%20enforced) [\[193\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=the%20company%20claimed%20that%20there,turnout%2C%20for%20the%20targeted%20groups) [\[194\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=109,of%20events%2C%20in%20February%202018) [\[196\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=2014%2C%20where%20he%20re,to%20this%20approach%2C%20stating%20that) [\[197\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=orientation%E2%80%9D,to%20this%20approach%2C%20stating%20that) Disinformation and 'fake news': Interim Report - Digital, Culture, Media and Sport Committee - House of Commons

<https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm>

[\[5\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=In%20an%20email%20to%20me%2C,like%20race%2C%20age%2C%20and%20gender) [\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed) [\[37\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=movies%20medium) [\[195\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=The%20researcher%20whose%20work%20is,Netflix%20uses%20to%20recommend%20movies) This is how Cambridge Analytica's Facebook targeting model really worked - according to the person who built it | Nieman Journalism Lab

<https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/>

[\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out) [\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models) [\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out) [\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform) [\[50\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20our%20services,content%20to%20train%20our%20models) [\[51\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=You%20can%20opt%20out%20of,used%20to%20train%20our%20models) [\[52\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models) [\[53\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=capabilities%20and%20safety,it%2C%20unless%20you%20opt%20out) [\[55\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Even%20if%20you%E2%80%99ve%20opted%20out,used%20to%20train%20our%20models) [\[56\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=We%20retain%20certain%20data%20from,become%20more%20efficient%20over%20time) [\[61\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20ChatGPT%2C%20you,used%20to%20train%20our%20models) [\[180\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20our%20services,content%20to%20train%20our%20models) How your data is used to improve model performance | OpenAI Help Center

<https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance>

[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve) [\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager) [\[67\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Apps) [\[68\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,settings%2C%20device%20type%20and%20settings) [\[69\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,or%20%E2%80%9CSaved%20Info%E2%80%9Din%20some%20locales) [\[70\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=public%20links%20%2C%20citations%2C%20chat,metrics%2C%20crash%20and%20debug%20information) [\[71\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,collected%20through%20supplemental%20Gemini%20Apps) [\[72\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,with%20public%20Gemini%20Apps%20content) [\[73\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,to%20Gemini%2C%20subscription%20related%20information) [\[74\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,or%20%E2%80%9CSaved%20Info%E2%80%9Din%20some%20locales) [\[75\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=use%20Gemini%20overlay%20to%20ask,co%2Fprivacypolicy%2Flocation) [\[76\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Audio%20Features) [\[77\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=public%20links%20%2C%20citations%2C%20chat,such%20as) [\[78\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Google%20uses%20this%20data%2C%20as,in%20our%20Privacy%20Policy%2C%20to) [\[79\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,and%20the%20public) [\[89\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=You%20can%20visit%20your%20Google,your%20data%20and%20experience%2C%20like) [\[90\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=If%20you%20change%20your%20Gemini,you%20use%20other%20Google%20services) [\[181\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies) Gemini Apps Privacy Hub - Gemini Apps Help

<https://support.google.com/gemini/answer/13594961?hl=en>

[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention) [\[13\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,training%20during%20the%20signup%20process) [\[101\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Today%2C%20we%27re%20rolling%20out%20updates,be%20done%20at%20any%20time) [\[102\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=You%E2%80%99re%20always%20in%20control%20of,window%20like%20the%20one%20below) [\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=) [\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI) [\[107\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20do%20not%20apply,under%20our%20Commercial%20Terms%2C%20including) [\[108\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,the%20data%20retention%20period) [\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period) [\[111\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=The%20extended%20retention%20period%20also,keep%20Claude%20safe%20for%20everyone) [\[112\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=for%20existing%20Claude%20app%20users) [\[113\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=the%20signup%20process.%20,five%20years%20if%20you%20allow) [\[114\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=If%20you%20change%20your%20setting,our%20data%20retention%20practices%20here) [\[115\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=You%20can%20always%20update%20your,Your%20data%20will%20still%20be) [\[116\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Starting%20today%2C%20we%E2%80%99re%20rolling%20out,Privacy%20Settings%20at%20any%20time) [\[184\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Aug%2028%2C%202025%E2%97%8F2%20min%20read) Updates to Consumer Terms and Privacy Policy \\ Anthropic

<https://www.anthropic.com/news/updates-to-our-consumer-terms>

[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important) [\[63\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Note) [\[118\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=data,accurate%2C%20relevant%2C%20and%20contextual%20responses) [\[119\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20only%20surfaces,you%20give%20to%20users%20outside) [\[120\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20you%20enter%20prompts%20using,Data%20stored%20about%20user%20interactions) [\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot) [\[122\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20a%20user%20interacts%20with,in%20Microsoft%20365%20Copilot%20Chat) [\[123\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot) [\[124\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=While%20abuse%20monitoring%2C%20which%20includes,section%20later%20in%20this%20article) [\[125\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20the,EU%20Data%20Boundary) [\[126\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20data,residency) [\[127\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles) [\[128\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=For%20Microsoft%20Teams%20chats%20with,to%20view%20the%20stored%20data) [\[129\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=LLMs%2C%20including%20those%20used%20by,Microsoft%20365%20Copilot) [\[130\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Deleting%20the%20history%20of%20user,interactions%20with%20Microsoft%20365%20Copilot) [\[131\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20only%20surfaces,shared%20channels%20in%20Microsoft%20Teams) [\[132\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20a%20user%20interacts%20with,24%20meetings%20in%20Microsoft) [\[185\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot) [\[186\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Deleting%20the%20history%20of%20user,interactions%20with%20Microsoft%20365%20Copilot) [\[189\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles) Data, Privacy, and Security for Microsoft 365 Copilot | Microsoft Learn

<https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy>

[\[16\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Beginning%20this%20week%2C%20people%20based,well%20as%20newly%20submitted%20ones) [\[17\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes) [\[143\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Last%20year%2C%20we%20delayed%20training,AI%20to%20people%20in%20Europe) [\[190\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=It%E2%80%99s%20important%20to%20note%20that,many%20of%20our%20industry%20counterparts) [\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes) [\[202\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Today%2C%20we%E2%80%99re%20announcing%20our%20plans,their%20cultures%2C%20languages%20and%20history) Making AI Work Harder for Europeans

<https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/>

[\[18\]](https://openai.com/enterprise-privacy/#:~:text=Comprehensive%20compliance) [\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data) [\[60\]](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law) [\[62\]](https://openai.com/enterprise-privacy/#:~:text=%2A%20Enterprise,ChatGPT%20Enterprise%20and%20API) [\[64\]](https://openai.com/enterprise-privacy/#:~:text=Our%20commitments%20provide%20you%20with,support%20for%20your%20compliance%20needs) [\[65\]](https://openai.com/enterprise-privacy/#:~:text=more%20about%20GPTs%20%E2%81%A0) [\[66\]](https://openai.com/enterprise-privacy/#:~:text=%2A%20Fine,and%20available%20features) [\[182\]](https://openai.com/enterprise-privacy/#:~:text=Updated%3A%20June%204%2C%202025) [\[191\]](https://openai.com/enterprise-privacy/#:~:text=Who%20owns%20inputs%20and%20outputs%3F) [\[192\]](https://openai.com/enterprise-privacy/#:~:text=with%20industry%20standards%20for%20security,and%20confidentiality) [\[198\]](https://openai.com/enterprise-privacy/#:~:text=Does%20OpenAI%20train%20its%20models,on%20my%20business%20data) Enterprise privacy at OpenAI | OpenAI

<https://openai.com/enterprise-privacy/>

[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission) [\[83\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission) [\[84\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20DOES%20NOT%20Gemini%20DOES,your%20existing%20data%20protection%20controls) [\[87\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20in%20Workspace) [\[88\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=The%20Gemini%20app%20enables%20admins,The%20default%20is%2018%20months) [\[91\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Prompts%20and%20responses) [\[92\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Inserted%20or%20generated%20content%20in,Docs%2C%20Gmail%2C%20etc) [\[93\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Last%20updated%3A%C2%A0November%204%2C%202025) [\[94\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20data%20retention) [\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F) Generative AI in Google Workspace Privacy Hub - Google Workspace Admin Help

<https://support.google.com/a/answer/15706919?hl=en>

[\[20\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D) [\[21\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D) [\[22\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so) [\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D) [\[153\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20with%20all%20digital%20platforms%E2%80%94from,analyze%20how%20you%20use%20our) [\[156\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=A%20WIRED%20review%20of%20the,owner%20ByteDance%20%E2%80%9Cand%20its%20intermediaries) [\[158\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20final%20category%20of%20information,%E2%80%9D) [\[161\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=Crucially%2C%20though%2C%20the%20company%E2%80%99s%20privacy,our%20technology%2C%E2%80%9D%20its%20policies%20say) [\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D) [\[171\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20people%20clamor%20to%20test,to%20deflect%20US%20security%20concerns) [\[174\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=already%20reported%20several%20examples%20of,to%20deflect%20US%20security%20concerns) [\[175\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=the%20ways%20data%20can%20be,information%20to%20AI%20chat%20bots) [\[176\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=any%20sensitive%20or%20personal%20information,to%20AI%20chat%20bots) [\[179\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=What%20DeepSeek%20Collects%20About%20You) DeepSeek's Popular AI App Is Explicitly Sending US Data to China | WIRED

<https://www.wired.com/story/deepseek-ai-china-privacy-data/>

[\[45\]](https://www.judiciary.senate.gov/imo/media/doc/Professor%20Emma%20L.%20Briant%20Report%20on%20Cambrige%20Analytica.pdf#:~:text=,really%20championed%20by%20Obama%27s) \[PDF\] Evidence for the US Senate Judiciary Committee on Cambridge ...

<https://www.judiciary.senate.gov/imo/media/doc/Professor%20Emma%20L.%20Briant%20Report%20on%20Cambrige%20Analytica.pdf>

[\[48\]](https://now.tufts.edu/2018/05/17/did-cambridge-analytica-sway-election#:~:text=Tufts%20political%20scientist%20Eitan%20Hersh,a%20Senate%20Judiciary%20Committee%20hearing) Did Cambridge Analytica Sway the Election? - Tufts Now

<https://now.tufts.edu/2018/05/17/did-cambridge-analytica-sway-election>

[\[57\]](https://openai.com/index/response-to-nyt-data-demands/#:~:text=How%20we%27re%20responding%20to%20The,Retention%20API%3A%20If%20a) How we're responding to The New York Times' data ... - OpenAI

<https://openai.com/index/response-to-nyt-data-demands/>

[\[58\]](https://medium.com/@jeffkessie50/openais-zero-data-retention-policy-916ff04a3599#:~:text=OpenAI%27s%20Zero%20Data%20Retention%20Policy,Enterprise%20customers%20can) OpenAI's Zero Data Retention Policy | by J Kes | Sep, 2025 - Medium

<https://medium.com/@jeffkessie50/openais-zero-data-retention-policy-916ff04a3599>

[\[59\]](https://help.openai.com/en/articles/8983130-what-if-i-want-to-keep-my-history-on-but-disable-model-training#:~:text=the%20model%20for%20everyone%20,you%20opt%20out%2C%20new) What if I want to keep my history on but disable model training?

<https://help.openai.com/en/articles/8983130-what-if-i-want-to-keep-my-history-on-but-disable-model-training>

[\[80\]](https://news.ycombinator.com/item?id=38186828#:~:text=Google%20Bard%20introduces%20,learning%20models) Google Bard introduces "Human reviewers," sparking privacy ...

<https://news.ycombinator.com/item?id=38186828>

[\[81\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Human%20Reviews%20of%20User%20Gemini,Data) [\[82\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20retains%20user%20data%20reviewed,learning%20models%2C%20enhancing) [\[85\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Gemini%20Data%20Collection%20and%20Storage) [\[86\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20stores%20user%20data%20from,Apps%20Activity%20settings%20but%20remains) Google Gemini - Is your Data Safe?

<https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini>

[\[95\]](https://services.google.com/fh/files/misc/genai_privacy_google_cloud_202308.pdf#:~:text=,customer%27s%20prior%20permission%20or) \[PDF\] Generative AI, Privacy, and Google Cloud

<https://services.google.com/fh/files/misc/genai_privacy_google_cloud_202308.pdf>

[\[96\]](https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training#:~:text=Center%20privacy,If) Is my data used for model training? - Anthropic Privacy Center

<https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training>

[\[97\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Prior%20to%20Anthropic%20updating%20its,user%20chooses%20to%20opt%20out) [\[98\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models) [\[99\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models) [\[100\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20developers%20hope%20to%20make,of%20their%20chatbot%20over%20time) [\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy) [\[105\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20new%20models) [\[199\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=The%20new%20privacy%20policy%20also,under%20the%2030%20day%20policy) Anthropic Will Use Claude Chats for Training Data. Here's How to Opt Out | WIRED

<https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/>

[\[110\]](https://www.reddit.com/r/ClaudeAI/comments/1nd73si/anthropics_new_privacy_policy_is_systematically/#:~:text=Anthropic%27s%20New%20Privacy%20Policy%20is,Claude%20can%20potentially%20access) Anthropic's New Privacy Policy is Systematically Screwing Over Solo ...

<https://www.reddit.com/r/ClaudeAI/comments/1nd73si/anthropics_new_privacy_policy_is_systematically/>

[\[117\]](https://amstlegal.com/anthropics-claude-ai-updated-terms-explained/#:~:text=Terms,changes%20%26%20protect%20your%20business) Anthropic's Claude AI Updates - Impact on Privacy & Confidentiality

<https://amstlegal.com/anthropics-claude-ai-updated-terms-explained/>

[\[133\]](https://learn.microsoft.com/en-us/answers/questions/5298757/does-microsofts-copilot-pro-for-office-365-use-you#:~:text=,LLMs%20to%20your%20organizational) Does Microsoft's Copilot Pro for Office 365 use your data to train it's ...

<https://learn.microsoft.com/en-us/answers/questions/5298757/does-microsofts-copilot-pro-for-office-365-use-you>

[\[134\]](https://www.thurrott.com/a-i/microsoft-copilot-a-i/313765/microsoft-says-it-is-not-training-copilot-ai-on-your-microsoft-365-data#:~:text=Microsoft%20Says%20it%20is%20Not,Which%20seems%20unnecessary) Microsoft Says it is Not Training Copilot AI on Your Microsoft 365 ...

<https://www.thurrott.com/a-i/microsoft-copilot-a-i/313765/microsoft-says-it-is-not-training-copilot-ai-on-your-microsoft-365-data>

[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,) [\[138\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,messages%2C%20nor%20public%20data%20from) [\[146\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=notifications%20to%20explain%20that%20Meta,) [\[148\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=article) [\[200\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,and%20the%20distinct%20ways%20different) Meta to start training its AI models on public content in the EU - News - Privacy Guides Community

<https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702>

[\[136\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=While%20Meta%20has%20been%20training,data%20to%20train%20AI%20models) [\[137\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=a%20clear%20legal%20basis%20for,data%20to%20train%20AI%20models) [\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly) [\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models) [\[141\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=interactions%20with%20Meta%20AI%20will,used%20to%20train%20its%20models) [\[142\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=regulatory%20pressure%20due%20to%20data,used%20to%20train%20its%20models) [\[147\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=from%20its%20EU%20user%20base,as%20well) [\[187\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20to%20start%20training%20its,public%20content%20in%20the%20EU) Meta to start training its AI models on public content in the EU | TechCrunch

<https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1>

[\[144\]](https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/#:~:text=Reddit%20www,not%20be%20available%20to%20you) How to Opt-Out of Meta Using Your Data for AI Training - Reddit

<https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/>

[\[145\]](https://www.iamexpat.de/expat-info/germany-news/how-european-users-can-opt-out-meta-using-their-data-train-ai#:~:text=How%20European%20users%20can%20opt,to%20train%20its%20AI%20system) How European users can opt out of Meta using their data to train AI

<https://www.iamexpat.de/expat-info/germany-news/how-european-users-can-opt-out-meta-using-their-data-train-ai>

[\[149\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=When%20you%20create%20an%20account%2C,all%20of%20the%20following%20information) [\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies) [\[152\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,in%20experience%20and%20for%20security) [\[154\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,We%20will%20obtain) [\[155\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,see%20our%20Cookies%20Policy) [\[157\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=%2A%20Log,publicly%20available%20information%20via%20the) [\[159\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=How%20We%20Use%20Your%20Information) [\[160\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=user%20support.%20,the%20Platform%2C%20such%20as%20by) [\[162\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Platform%2C%20provide%20customer%20support%20to,to%20protect%20health%20or%20life) [\[163\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,to%20protect%20health%20or%20life) [\[164\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Purpose%20of%20processing%20Personal%20information,DeepSeek%20and%20provide%20user%20support) [\[165\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=such%20as%20our%20machine%20learning,training%20and%20improving%20our%20technology) [\[166\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,Keep%20Your%20Information) [\[167\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=We%20retain%20information%20for%20as,or%20defense%20of%20legal%20claims) [\[168\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=The%20retention%20periods%20will%20be,necessary%20to%20process%20the%20violation) [\[169\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=provide%20you%20with%20the%20Services%2C,necessary%20to%20process%20the%20violation) [\[172\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=services%20with%20information%20in%20urgent,to%20protect%20health%20or%20life) [\[173\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Our%20Corporate%20Group) [\[188\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,2025) [\[203\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Where%20We%20Store%20Your%20Information) DeepSeek Privacy Policy

<https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html>

[\[177\]](https://proton.me/blog/deepseek#:~:text=Using%20DeepSeek%3F%20Here%27s%20why%20your,history%2C%20prompts%2C%20and%20audio%20input) Using DeepSeek? Here's why your privacy is at stake | Proton

<https://proton.me/blog/deepseek>

[\[178\]](https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj#:~:text=Is%20DeepSeek%20safe%3F%20What%20happens,chat%20history%2C%20or%20other) Is DeepSeek safe? What happens to your data when you use it

<https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj>

[\[204\]](https://www.reddit.com/r/gdpr/comments/1lk4baw/what_are_deepseeks_privacy_practices/#:~:text=DeepSeek%20collects%20three%20main%20categories,uploaded) What are DeepSeek's privacy practices? : r/gdpr - Reddit

<https://www.reddit.com/r/gdpr/comments/1lk4baw/what_are_deepseeks_privacy_practices/>
