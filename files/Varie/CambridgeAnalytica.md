# Da Cambridge analytica ai chatbot: quanto √® a rischio la nostra privacy? 

## Abstract

La vicenda **Cambridge Analytica (CA)** ha svelato come la profilazione psicometrica possa trasformare dati dei social media in potenti strumenti di micro‚Äëtargeting politico. Dal 2014 [CA ha raccolto dati Facebook([1])](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf) di decine di milioni di persone tramite un quiz ("thisisyourdigitallife"), ottenendo [**profili OCEAN**([2])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the) (i "Big Five" della personalit√†) e abbinandoli a informazioni demografiche e di consumo. Queste **schede psicometriche** sono state impiegate per segmentare l'elettorato e sperimentare messaggi politici mirati: ad es. modulando [inserzioni pro-armi in base al tratto "nevroticismo" ([3])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). CA ha propagato contenuti su **Facebook** (tramite **Custom Audiences** e simili), testando varianti di annunci con tecniche di **A/B testing** per massimizzare l'impatto. 

Ok ma qual √® stato l'impatto di tutto questo?

Le evidenze sull'efficacia reale sono miste: [CA stessa, ([4])](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups) ha riportato aumenti del **39%** nella sensibilizzazione su certi temi e un **+30%** all'affluenza di gruppi mirati in campagne USA del 2014. 
Tuttavia, altre analisi indipendenti [\[5\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=In%20an%20email%20to%20me%2C,like%20race%2C%20age%2C%20and%20gender)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed) hanno notato che il modello predittivo di CA **non superava di molto i normali criteri demografici**.

Oggi, logiche simili di **raccolta e utilizzo intensivo dei dati utente** si ritrovano nei principali servizi **chatbot e LLM** (modelli linguistici di grandi dimensioni). Piattaforme consumer come **ChatGPT** (OpenAI, [\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out)) e **Gemini** (Google) di default **registrano prompt, conversazioni e feedback degli utenti** e li usano per migliorare continuamente i modelli[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en). Gli utenti privati possono "_opt-out_" (insomma rifiutarsi di condividere i propri dati) limitando la condivisione della conservazione e dei propri dialoghi, ma in assenza di tale scelta i dati delle chat possono essere conservati per periodi estesi (es. **OpenAI** mantiene le chat degli utenti generici a tempo indefinito per addestrare modelli, a meno di opt-out, mentre **Anthropic** dal 2025 offre la scelta: no training e conservazione 30 giorni oppure training attivo con conservazione **5 anni**[\[9\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention). Al contrario, i servizi **business/enterprise** offrono garanzie di **isolamento**: ad esempio **OpenAI API/Enterprise** e **Microsoft 365 Copilot** assicurano che input e output **non alimentano i dati di training** dei modelli pubblici[\[10\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform)[\[11\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important), restando confinati nell'ambiente del cliente. Sul fronte **trasparenza e governance**, le aziende hanno introdotto **controlli per utenti e amministratori** (come dashboard privacy, impostazioni di retention, moduli di opposizione per l'UE[\[12\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Beginning%20this%20week%2C%20people%20based,well%20as%20newly%20submitted%20ones)[\[13\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes)) e adottato impegni contrattuali (es. **SOC 2**, **DPA** sul trattamento dati) per rassicurare imprese e regolatori[OpenAI \[14\]](https://openai.com/enterprise-privacy/#:~:text=Comprehensive%20compliance)[Google \[15\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). Non mancano, tuttavia, preoccupazioni: ad esempio il chatbot **DeepSeek**, popolare app cinese, raccoglie **ogni input, file e cronologia chat**, inviando tutto su server in Cina[\[16\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D)[\[17\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D); questo solleva dubbi di sicurezza e ha attirato l'attenzione di esperti per potenziali rischi di accesso governativo[\[18\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so).

> Per accedere a questo articolo serve l'account premium di Wired

In sintesi, dall'era Cambridge Analytica all'odierno boom dei chatbot AI, **dati personali e comportamentali** degli utenti sono diventati il **"carburante di modelli predittivi e generativi"**. Se da un lato ci√≤ abilita servizi pi√π "intelligenti" e campagne su misura, dall'altro impone nuove sfide di **privacy, controllo e responsabilit√†**. Le piattaforme stanno rispondendo con maggior trasparenza (policy dedicate) e opzioni di controllo, ma spetta anche agli utenti, e soprattutto ai regolatori, pretendere **chiarezza sull'uso dei propri dati**, esercitare i diritti di opt-out/off e valutare con cautela cosa condividere con queste AI.

## Cambridge Analytica: cos'√® successo?
Ora vediamo nel dettaglio cos'√® successo con il caso Cambridge analytica. Per farlo user√≤ le prime due fonti che abbiamo visto nell'articolo [1](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf),[2](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the)

### I dati raccolti
Cambridge Analytica (consociata di **SCL Elections**) acquis√¨ nel 2014-2015 un enorme dataset di utenti Facebook tramite la societ√† **GSR** (Global Science Research) del ricercatore Aleksandr Kogan. Kogan svilupp√≤ un'app di quiz della personalit√† ("thisisyourdigitallife") sfruttando la Graph API di Facebook, che all'epoca consentiva di estrarre non solo i dati dell'utente consenziente ma anche quelli dei suoi **amici** (funzionalit√† _friends permissions_ valida fino al 2014). Circa **320.000** utenti Facebook, principalmente statunitensi, compilarono il test OCEAN accedendo via Facebook Login; in cambio di pochi dollari, diedero all'app il permesso di leggere una vasta gamma di informazioni: profilo pubblico (nome, genere), data di nascita, citt√† attuale, **pagine "Like"**, post in bacheca, lista amici, persino messaggi privati e foto taggate. 

Poich√© l'app ereditava i diritti d'accesso della precedente app accademica di Kogan (sviluppata prima delle restrizioni Facebook del 2015), questa poteva raccogliere anche i dati degli amici dei partecipanti, se questi ultimi non avevano impostato diversamente la privacy. **In totale ~87 milioni di persone** (di cui oltre **1 milione** nel Regno Unito) subirono questa raccolta massiva di dati senza saperlo. Facebook conferm√≤ la stima e pubblic√≤ nel 2018 la lista dei paesi coinvolti. Circa **30 milioni** di individui avevano sia i dati Facebook sia il risultato del quiz psicometrico abbinati, costituendo il nucleo per le analisi predittive.

### Caratteristiche psicometriche (modello OCEAN)

Il modello di personalit√† Big Five, detto **OCEAN** (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), fu al centro della profilazione CA[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the). Kogan e colleghi applicarono le metodologie del Psychometrics Centre di Cambridge (note per il progetto "MyPersonality") che dimostrarono come dai **Facebook Likes** si potessero predire con sorprendente accuratezza i tratti OCEAN e altri attributi personali[\[19\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=OCEAN%20model%20and%20pioneer%20the,as%20ethnicity%20and%20political%20affiliation)[\[20\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that).

Nel contratto stipulato con SCL il 4 giugno 2014, Kogan dichiarava che le sue tecniche permettevano di raggiungere predittivit√† "**vicina al test-retest**" nei punteggi di personalit√†, con correlazioni tali che un algoritmo basato sui like risultava _pi√π accurato_ nel descrivere una persona rispetto alla conoscenza dei suoi amici o persino familiari[\[21\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Nix%20told%20us%3A%20%E2%80%9CWe%20do,41)[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the).
    
> Questa affermazione riprendeva uno studio accademico del 2015 [[22](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that)], dove Michal Kosinski, collega di Kogan, mostr√≤ che 70 like di Facebook superavano gli amici nel delineare il profilo psicologico di un individuo. 

In pratica, CA disponeva di punteggi OCEAN stimati per milioni di elettori USA, ottenuti direttamente dal quiz o frutto di inferenza tramite modelli addestrati sui dati di Kogan. 

Dato che ora potrebbe sembrare complicato, facciamo [un esempio](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). Un utente con punteggio alto in "Nevroticismo" e basso in "Apertura" veniva identificato come **sensibile a messaggi di paura e ordine**, mentre uno altamente "Estroverso" poteva rispondere meglio a contenuti ottimistici e sociali. Cambridge Analytica **clusterizzava** il pubblico in gruppi psicografici e individuava temi chiave per ciascun segmento: secondo l'ex CEO Alexander Nix, "presentare un fatto supportato da un'emozione" era la strategia, adattando l'argomentazione al profilo emotivo dell'audience[\[23\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=We%20are%20trying%20to%20make,109).

### Modelli predittivi e ML impiegati

Pur non pubblicando dettagli tecnici dei suoi algoritmi, CA combin√≤ approcci di **machine learning** con analytics tradizionali. Kogan in [un'email](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/) ha spiegato che il suo modello per CA operava in modo simile al sistema di raccomandazione di Netflix, ossia tramite **SVD/factor analysis**: riducendo una matrice utente-like a componenti latenti, che incorporavano insieme personalit√†, demografia e reti sociali. In sostanza, l'algoritmo non isolava "apertamente" i 5 tratti, ma li mescolava con decine di altre variabili (et√†, genere, orientamento politico, etc.) in fattori correlati utili a predire il comportamento elettorale. 

Come abbiamo fatto nella sezione precedente, facciamo un esempio. CA sfruttava _regressioni_ e _alberi decisionali_ per stimare la probabilit√† che un individuo sostenesse determinate cause o candidati, date le sue caratteristiche psicografiche e demografiche note. 

I data scientist di CA costruirono modelli per identificare i cosiddetti **persuadables**, ovvero elettori indecisi fortemente influenzabili da specifiche leve emotive. La mole di dati (like, test, comportamenti online e offline) permise anche l'uso di **reti neurali shallow** o modelli di classificazione multivariata per associare profili a _outcome_ di interesse (voto, donazione, astensione). Un ruolo importante lo ebbero i **modelli look-alike** di Facebook: CA poteva caricare liste di utenti noti (ad es. individui con alto punteggio "apertura" identificati dal quiz) e usare l'algoritmo di Facebook per trovare altri utenti simili, ampliando il raggio del targeting.

**Pipeline dati‚Üíprofilo‚Üítargeting (schema):**

\[Facebook Graph API\] ‚Üí Raccolta massiva dati (profili, like, amici‚Ä¶) ‚Üí  
Elaborazione Kogan/GSR ‚Üí \[Punteggi personalit√† OCEAN\] ‚Üí  
Integrazione con altri dataset (registri elettorali, dati consumo) ‚Üí  
Data Warehouse (scheda psicografica per elettore) ‚Üí  
Segmentazione e modellazione (cluster e predizioni: es. "persuadabile su tema X") ‚Üí  
Creazione messaggi personalizzati (ads varianti A/B) ‚Üí  
Delivery su misura (Facebook Custom Audiences, email, SMS, canvassing) ‚Üí  
Feedback campagne (click, conversioni) usato per refit modelli/A/B test.

_Descrizione:_ in primo luogo CA (via GSR) **estrae i dati grezzi** dal social media (Facebook) e li combina con altre fonti (ad es. **elenchi elettorali** pubblici, dati da broker commerciali come acquisti e abbonamenti)[\[40\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to). Questi dati alimentano l'**analisi psicometrica**: dal quiz e dai like si calcolano i tratti OCEAN individuali, che poi vengono **aggregati e inseriti** in un profilo unico per ogni elettore (inclusi et√†, genere, posizione, inclinazioni politiche, etc.)[\[41\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=required%20under%20the%20contract%20to,electoral%20register%20in%20those%20states). Su questa base, gli statistici di CA sviluppano **modelli predittivi** per segmentare la popolazione in gruppi chiave (es. "neurotici insicuri", "aperti progressisti") e prevedere per ciascuno la suscettibilit√† a specifici messaggi[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the)[\[41\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=required%20under%20the%20contract%20to,electoral%20register%20in%20those%20states). Parallelamente, il team creativo elabora **varianti di messaggi** (meme, video, slogan) tarati sugli insight psicologici dei segmenti: ad esempio, lo stesso tema (come il diritto alle armi) viene confezionato in versione _paura e protezione_ per individui ad alto nevroticismo, e in versione _hobby sportivo_ per soggetti aperti/tranquilli[\[3\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39). I messaggi vengono quindi **erogati tramite micro-targeting**: CA caricava su Facebook le liste di utenti target (identificati per nome/ID, email o telefono) e utilizzava strumenti come **Custom Audience** e **Dark Posts** per mostrare inserzioni diverse a gruppi diversi, senza che fossero visibili pubblicamente ad altri[\[38\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=3%20The%20issue%20of%20data,GSR%20and%20Cambridge%20Analytica%20allegations)[\[39\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%2C%20involving%20various%20organisations%20including,overseas%20elections%20in%20Chapter%206). Questa fase includeva **A/B test** e controlli di efficacia: si monitoravano clic, condivisioni, tempo di visualizzazione e tassi di conversione (es. iscrizione a un evento, donazione) per ciascuna variante, iterando poi sulla creativit√† vincente. Infine, i risultati di campagna (engagement effettivo, cambiamenti nei sondaggi interni) venivano **retroalimentati** nel processo: le reazioni degli utenti servivano a affinare ulteriormente i modelli di persuasione, in un ciclo continuo di ottimizzazione.

**Canali di delivery e sperimentazione A/B:** Il principale veicolo dei messaggi di CA fu **Facebook**. La societ√† creava inserzioni mirate utilizzando il sistema pubblicitario di Facebook, che permetteva di indirizzare annunci a cluster demografici e psicografici molto specifici (per area geografica, interessi, simili a una lista fornita). CA ha anche sfruttato il partner canadese **AggregateIQ** per campagne su altre piattaforme e la **rete display** (ad es. banner web mirati): AIQ gest√¨ spese pubblicitarie per gruppi pro-Brexit e pro-Trump, utilizzando i dati e segmenti forniti da CA[\[42\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=sharing%20of%20data%20in%20the,overseas%20elections%20in%20Chapter%206)[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter). Inoltre, CA non disdegnava metodi tradizionali: in alcuni casi forn√¨ _scripts_ per **telemarketing** o per volontari politici, calibrati sul profilo dell'elettore da contattare (es. enfatizzare l'immigrazione parlando con un soggetto "chiuso" e timoroso, vs. economia con un "aperto" cosmopolita). Gli **esperimenti A/B** erano centrali: **centinaia di varianti** di annunci venivano testate simultaneamente - cambiando ad esempio colore, tono emotivo, call-to-action - misurando quale versione avesse il maggior tasso di click o conversione in ciascun segmento[\[44\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=92,Some%20of%20the)[\[39\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%2C%20involving%20various%20organisations%20including,overseas%20elections%20in%20Chapter%206). Un famoso esempio riportato da Wylie √® la campagna "Defeat Crooked Hillary" in cui CA avrebbe testato decine di messaggi anti-Clinton (dai pi√π moderati ai pi√π complottisti) per capire quali risuonavano con gruppi di elettori dubbi, bombardandoli poi con il messaggio ottimizzato. Facebook in quel periodo **non tracciava** efficacemente questi _dark ads_ n√© ne limitava la personalizzazione estrema, il che permise a CA di condurre una sorta di laboratorio di propaganda invisibile al pubblico e alle stesse vittime.

**Integrazione con dati esterni:** Oltre ai dati Facebook, CA disponeva di un mosaico di altre informazioni. Negli USA sfrutt√≤ i **registri elettorali** statali (contenenti storia di voto, affiliazione di partito, etc.) e li incroci√≤ con dataset commerciali (provenienti da broker come Acxiom, Experian) su abitudini di acquisto, tipo di auto posseduta, riviste lette, donazioni caritatevoli‚Ä¶[\[40\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to). Nix dichiar√≤ che CA aveva "fino a **5.000 datapoint per individuo**" per i cittadini americani, costruendo un'immagine dettagliata della personalit√† e preferenze di ciascun elettore[\[40\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to). Questa **strategia ibrida** (social media + data offline) consent√¨ di identificare correlazioni inedite - ad esempio, scoprire che i possessori di pickup truck in Carolina del Nord con pochi "likes" politici ma punteggi alti in coscienziosit√† erano potenziali sostenitori repubblicani indecisi. CA caricava in Facebook non solo gli ID utenti raccolti da GSR, ma anche liste di elettori di specifiche contee ottenute dai partiti o comitati PAC, usando le **Custom Audience** per raggiungerli online con messaggi su misura. Un ulteriore strumento furono i **Lookalike Audiences**: partendo da un pubblico di cui CA conosceva l'orientamento (es. persone profilate come _pro-Trump_), l'algoritmo di Facebook trovava altri utenti con caratteristiche analoghe su cui estendere la campagna[\[38\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=3%20The%20issue%20of%20data,GSR%20and%20Cambridge%20Analytica%20allegations)[\[39\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%2C%20involving%20various%20organisations%20including,overseas%20elections%20in%20Chapter%206).

**Evidenze di efficacia e limiti metodologici:** Misurare l'impatto concreto delle tattiche di CA √® complesso e fortemente dibattuto. La stessa CA vant√≤ alcuni risultati numerici nelle elezioni **Midterm USA 2014**: in una gara supportata dall'allora _John Bolton Super PAC_, l'uso di profili psicografici per micro-targeting pubblicitario in 5 gruppi di personalit√† port√≤ - a dire di SCL - a un incremento del **39%** nella _brand awareness_ degli argomenti chiave presso gli elettori esposti[\[4\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups). Un'altra campagna ("For America", 2014) mirata a mobilitare elettori conservatori online, sempre secondo un report interno SCL, avrebbe generato **1,5 milioni** di impression mirate traducendosi in un sorprendente **+30% di affluenza al voto** rispetto alle attese nei gruppi target[\[4\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups). Questi dati, difficili da verificare indipendentemente, suscitarono scetticismo: gli uplift erano calcolati rispetto a baseline non trasparenti e confusi con altri fattori (ad es. un incremento di turnout potrebbe derivare da cause esterne, non solo dalle ads). Studi accademici successivi hanno ridimensionato la "mitologia" CA. Ad esempio, un'analisi di Eitan Hersh (2019) al Senato USA concluse che **l'effetto persuasivo del micro-targeting psicometrico fu probabilmente modesto**, poich√© campagne precedenti (Obama 2012) avevano gi√† usato dati e modelli analoghi senza risultati miracolosi[\[45\]](https://www.judiciary.senate.gov/imo/media/doc/Professor%20Emma%20L.%20Briant%20Report%20on%20Cambrige%20Analytica.pdf#:~:text=,really%20championed%20by%20Obama%27s). Matthew Hindman not√≤ che il modello di Kogan spiegava una porzione di varianza comparabile ai modelli tradizionali su et√†, sesso e razza[\[5\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=In%20an%20email%20to%20me%2C,like%20race%2C%20age%2C%20and%20gender). Inoltre, **limiti metodologici** interni minavano la robustezza: i punteggi OCEAN dedotti dai like hanno margine d'errore e sono statici, mentre le preferenze degli elettori evolvono; l'algoritmo poteva quindi etichettare erroneamente soggetti (es. scambiare un giovane cinico per un indeciso persuadibile). Vi √® poi un **bias di selezione**: non tutti gli 87 milioni di utenti erano elettori indecisi - anzi molti erano gi√† schierati, riducendo l'utilit√† del convincerli. Anche Christopher Wylie, il whistleblower, ammise che CA non pot√© eseguire test controllati a livello macro su chi avesse votato cosa, quindi **non esiste prova diretta** che abbia "spostato" X voti decisivi. Infine, un report della Commissione UK evidenzi√≤ come l'intera operazione CA fosse **opaca e non replicabile**: la societ√† rifiut√≤ di soddisfare pienamente una richiesta di accesso ai dati (Subject Access Request) da parte di un cittadino, impedendo di sapere esattamente quali info possedeva su di lui e come le avesse usate[\[46\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=100,43)[\[47\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=101,119). Questo suggerisce una mancanza di rigore che getta dubbi sulla qualit√† del prodotto finale di CA e rende arduo valutarne scientificamente l'impatto.

**Esiti e implicazioni:** Nel 2018 lo scandalo Cambridge Analytica, emerso grazie a inchieste (_The Guardian_, _NY Times_) e all'azione del Parlamento UK, ha portato alla chiusura di CA/SCL e a una multa simbolica a Facebook (¬£500k dall'ICO UK) per aver fallito nel proteggere i dati[\[48\]](https://now.tufts.edu/2018/05/17/did-cambridge-analytica-sway-election#:~:text=Tufts%20political%20scientist%20Eitan%20Hersh,a%20Senate%20Judiciary%20Committee%20hearing)[\[49\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=conflicted%20with%20Mr%20Nix%E2%80%99s%20evidence%3B,as%20they%20were%20never%20enforced). L'eredit√† principale √® stata una presa di coscienza globale sui rischi di abuso dei dati personali a fini di **manipolazione di massa**. Gli strumenti raffinati da CA (profilazione psicologica + social media targeting) in s√© non erano completamente nuovi - ma CA ne spinse l'uso oltre i limiti etici, operando senza trasparenza n√© consenso informato degli interessati. L'episodio ha accelerato riforme: Facebook nel 2018-19 ha limitato ulteriormente le API e reso pi√π controllabili le inserzioni politiche, mentre in ambito normativo casi come questo hanno contribuito alla stesura di linee guida (es. GDPR in UE prevede gi√† dal 2018 il diritto di opposizione a profilazione e decisioni automatizzate, sebbene all'epoca non fosse invocato).

**Le stesse logiche oggi nei chatbot/LLM:** I principi del "**collezionare dati comportamentali per migliorare modelli predittivi**" non sono confinati al marketing politico. Con l'avvento di ChatGPT e simili (2022+), le aziende tech hanno applicato un paradigma analogo: raccogliere quante pi√π **interazioni utente** possibili per addestrare e perfezionare modelli di intelligenza artificiale generativa. In questa seconda parte analizziamo le politiche dei principali fornitori di LLM (OpenAI, Google, Anthropic, Microsoft, Meta, DeepSeek), focalizzandoci su come gestiscono i dati utenti (tipologie raccolte, finalit√†, tempi di conservazione, training, controlli, base legale) e quali meccanismi di **trasparenza e governance** offrono.

## Chatbot e LLM: dati raccolti, utilizzi e confronto tra piattaforme (2023-2025)

Le piattaforme di **AI generativa conversazionale** hanno modelli di business e utenza diversi (consumer vs enterprise), ma presentano tendenze comuni: **registrazione delle richieste utente (prompt)**, monitoraggio delle conversazioni (log) e sfruttamento di tali dati per **migliorare i modelli tramite training continuo**. Di seguito esponiamo per ciascun vendor chiave lo stato attuale (al 2025) su raccolta dati, utilizzo per training, retention, revisione umana, controlli offerti e base legale, con un quadro comparativo riassuntivo.

### OpenAI - ChatGPT (GPT-3.5/GPT-4)

**Dati raccolti:** OpenAI raccoglie ogni input fornito dagli utenti ai suoi modelli, sia tramite l'**interfaccia ChatGPT** (conversazioni in chat, allegati caricati) sia via **API**. In dettaglio, le **chat** su chat.openai.com includono prompt dell'utente e risposte dell'AI; se l'utente fornisce file (es. nell'uso di plugin o la recente funzione di upload immagini/PDF in GPT-4), anche quei contenuti vengono acquisiti. Sono registrati inoltre **feedback espliciti** (i voti üëç/üëé sulle risposte, segnalazioni) e **metadata tecnici** (timestamp, indirizzo IP, tipo di dispositivo/browser) per motivi di sicurezza e monitoraggio di performance[\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out)[\[50\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20our%20services,content%20to%20train%20our%20models). OpenAI non raccoglie dati biometrici o di sensori perch√© il servizio √® testuale/visivo, ma l'uso di **Speech-to-text** (come nel nuovo ChatGPT vocal) implica che l'audio utente venga trascritto e processato (usando modelli come Whisper) - anch'esso √® considerato input collezionato. Oltre ai dati forniti direttamente dall'utente, OpenAI pu√≤ raccogliere **telemetria d'uso** (es. frequenza delle sessioni, lunghezza dei prompt, tassi di errore) e applica cookie/tracking sul sito ChatGPT come da sua Privacy Policy generica[\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out)[\[51\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=You%20can%20opt%20out%20of,used%20to%20train%20our%20models).

**Finalit√† e base legale:** secondo OpenAI, i dati utente vengono usati primariamente per _"fornire e migliorare i nostri modelli"_. Le finalit√† dichiarate includono: (a) erogazione del servizio e generazione delle risposte richieste (base contrattuale/di servizio), (b) **miglioramento continuo dell'accuratezza e sicurezza dei modelli** tramite l'addestramento su conversazioni reali (OpenAI parla di "esporre i modelli a problemi reali per renderli pi√π utili"[\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out); base legale indicata: consenso dell'utente o legittimo interesse, a seconda del regime), (c) prevenzione abusi e sicurezza (OpenAI monitora input per evitare usi illeciti o contenuti vietati; base: obbligo contrattuale e legittimo interesse), (d) analisi aggregate d'uso e ricerca (es. capire quali richieste sono difficili per il modello). OpenAI **non utilizza i dati degli utenti per fini di marketing/pubblicit√† personalizzata**[\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out) - una differenza rispetto ai social network classici: i prompt non servono a profilare l'utente per vendere inserzioni, ma a _profilare il modello_ stesso, per cos√¨ dire. Dal punto di vista GDPR, OpenAI (dopo le vicende col Garante italiano nel 2023) ha introdotto un modulo di consenso facoltativo per l'uso dei dati a fini di training, ma continua a sostenere una combinazione di basi giuridiche: l'esecuzione di un contratto per i servizi di risposta, e il legittimo interesse per il miglioramento del modello, offrendo comunque l'**opt-out** (diritto di opposizione) agli interessati[\[52\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models). Per gli utenti API e enterprise, la base √® contrattuale (fornitura del servizio come **processor** per conto del cliente, senza usare i dati oltre quanto richiesto).

**Uso ai fini di training modelli:** _Default_ per **utenti consumer** (ChatGPT free e Plus) - i contenuti delle chat **vengono utilizzati per addestrare e migliorare i modelli** di OpenAI, **salvo opt-out esplicito**[\[53\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=capabilities%20and%20safety,it%2C%20unless%20you%20opt%20out)[\[51\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=You%20can%20opt%20out%20of,used%20to%20train%20our%20models). OpenAI spiega che ChatGPT "migliora addestrandosi sulle conversazioni che le persone hanno con esso"[\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out). Questa forma di _training_ comprende sia l'uso nei set di _fine-tuning_ supervisionato/RLHF (ad es. gli specialisti OpenAI esaminano campioni di chat per creare dati di addestramento con il feedback umano, oppure usano valutazioni üëç/üëé degli utenti come segnale di reward nel Reinforcement Learning) sia l'impiego diretto nei _dataset_ per versioni future del modello. Nel **marzo 2023** OpenAI ha cambiato le policy per l'API, annunciando che di default _non_ avrebbe pi√π usato i dati delle chiamate API dei clienti per addestrare modelli, a meno di opt-in[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform). Questo per rassicurare aziende e sviluppatori. Dunque, per **servizi business/API**, l'impostazione √® opposta: **nessun uso dei dati utente per training**, a meno che l'organizzazione scelga volontariamente di condividerli (ad es. inviando apposta esempi di prompt via _Playground_ con flag di opt-in)[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform). A settembre 2023 OpenAI ha lanciato **ChatGPT Enterprise**, chiarendo che tutte le conversazioni degli utenti enterprise "non entreranno mai nel training dei nostri modelli" di default[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform)[\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data). Da aprile 2023 inoltre l'interfaccia consumer ChatGPT ha introdotto un toggle nelle impostazioni ("**Do not use my data to improve model**" / "Disabilita la cronologia & training"), dando anche ai singoli la possibilit√† di escludere le proprie nuove chat dal training generale[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models). Se l'utente attiva questa opzione, le chat non vengono trattenute per addestramento (restano per 30 giorni in forma cifrata solo a fini di sicurezza)[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models). Va notato che eventuali **feedback** espliciti forniti (ad es. scrivere commenti nel form di segnalazione o mettere üëç/üëé) **possono essere usati per training** anche se la cronologia √® disabilitata, perch√© si considera il feedback come consenso puntuale a usare quell'interazione a fini di miglioramento[\[55\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Even%20if%20you%E2%80%99ve%20opted%20out,used%20to%20train%20our%20models).

**Conservazione dei dati (retention):** Le policy di retention di OpenAI distinguono per servizio e impostazioni:  
\- Per conversazioni ChatGPT _con_ cronologia attiva (default consumer): i dati sono conservati nei sistemi di OpenAI a tempo indeterminato, salvo diversa comunicazione. OpenAI non dichiara esplicitamente una scadenza - anzi, il fine √® accumulare un lungo storico per addestrare modelli futuri. In risposta a pressioni regolatorie, nel 2023 OpenAI ha dichiarato che avrebbe "retained certain data for as long as needed to provide and improve services"[\[56\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=We%20retain%20certain%20data%20from,become%20more%20efficient%20over%20time), senza dettagli quantitativi.  
\- Per conversazioni ChatGPT _con_ cronologia _disattivata_ (opt-out training): OpenAI afferma che tali dati "non saranno usati per addestrare modelli" e verranno **conservati solo per 30 giorni** allo scopo di monitoraggio abusi, dopodich√© saranno cancellati[\[57\]](https://openai.com/index/response-to-nyt-data-demands/#:~:text=How%20we%27re%20responding%20to%20The,Retention%20API%3A%20If%20a). Questa finestra di ~30 giorni serve a eventualmente ispezionare contenuti se emergono problemi (ad es. un utente che genera molte richieste illegali).  
\- Per utilizzi **API standard**: analogamente, _log_ di richieste e risposte vengono tenuti per max **30 giorni** per motivi di sicurezza/abuso, dopodich√© eliminati[\[57\]](https://openai.com/index/response-to-nyt-data-demands/#:~:text=How%20we%27re%20responding%20to%20The,Retention%20API%3A%20If%20a). √à stata una modifica introdotta a fine marzo 2023: prima, i dati API potevano essere conservati pi√π a lungo. Inoltre, OpenAI offre a clienti enterprise un'opzione di "**Zero Data Retention**", ossia non conservare affatto i contenuti delle richieste (se il cliente opta per questa modalit√†, le richieste vengono elaborate e immediatamente scartate, mantenendo solo metriche aggregate)[\[58\]](https://medium.com/@jeffkessie50/openais-zero-data-retention-policy-916ff04a3599#:~:text=OpenAI%27s%20Zero%20Data%20Retention%20Policy,Enterprise%20customers%20can)[\[59\]](https://help.openai.com/en/articles/8983130-what-if-i-want-to-keep-my-history-on-but-disable-model-training#:~:text=the%20model%20for%20everyone%20,you%20opt%20out%2C%20new).  
\- Per **ChatGPT Enterprise**: OpenAI fornisce controllo agli amministratori sull'intervallo di conservazione delle conversazioni dei loro utenti aziendali (fino a poter scegliere _zero retention_). Nella documentazione Enterprise (agg. giugno 2025) si afferma: "You control how long your data is retained (ChatGPT Enterprise)"[\[60\]](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law). Ci√≤ implica che un'azienda pu√≤ impostare, ad esempio, auto-cancellazione di chat dopo X giorni. Se non configurato diversamente, i dati enterprise dovrebbero seguire la retention di default contrattuale, che √® 30 giorni negli endpoint API e potenzialmente pi√π estesa per l'interfaccia enterprise se l'utente lo consente (ma sempre esclusa dal training).  
\- **Eccezioni legali**: come per tutti, eventuali dati soggetti a obblighi di legge (es. ordine di conservazione da autorit√†, o dati necessari per dispute legali) possono essere mantenuti oltre i termini sopra.

In sintesi, l'utente comune di ChatGPT che non tocca le impostazioni accetta una conservazione indefinita a fini di training. L'utente consapevole pu√≤ ridurre l'impatto disattivando la cronologia (riducendo a 30 gg retention e niente training). Le aziende e sviluppatori invece hanno by design retention breve e nessun training, con opzione di stringere ulteriormente a zero log.

**Revisione umana dei dati:** OpenAI impiega revisione manuale su una _piccola percentuale_ di conversazioni, sia per migliorare il modello (ad es. etichettare conversazioni difficili per far apprendere risposte migliori) sia per moderazione (analisti che controllano se il modello ha violato le policy). La documentazione spiega che **gli addetti OpenAI possono accedere e visionare** un campione di contenuti utente, con strumenti per offuscare informazioni personali ove possibile[\[56\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=We%20retain%20certain%20data%20from,become%20more%20efficient%20over%20time). Ad esempio, per addestrare GPT-4 con il metodo RLHF, OpenAI ha fatto esaminare e valutare migliaia di chat a contractor umani. Oggi, sebbene con GPT-4 sia in fase "di produzione", questo processo continua: OpenAI nella sua _Privacy Policy_ ammette che "potremmo analizzare i contenuti condivisi con noi per assicurarci che rispettino le nostre politiche di sicurezza e per migliorare i modelli" (tradotto, c'√® uno staff che legge alcuni prompt e risposte reali). I dati destinati a training vengono **pseudonimizzati**: OpenAI dichiara di _rimuovere o aggregare quante pi√π informazioni personali possibile_ prima di usare i dati nelle dataset di addestramento[\[56\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=We%20retain%20certain%20data%20from,become%20more%20efficient%20over%20time), per ridurre il rischio di ricostruire identit√†. Ad esempio, nomi, email, numeri potrebbero essere mascherati automaticamente. Non √® per√≤ una garanzia assoluta di anonimato, come hanno notato vari enti (il Garante italiano ha chiesto di implementare meglio queste misure). La revisione umana OpenAI avviene anche per i **feedback**: se un utente segnala un output come inappropriato, un membro del team di sicurezza potr√† leggere quell'intera conversazione[\[55\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Even%20if%20you%E2%80%99ve%20opted%20out,used%20to%20train%20our%20models). Importante: per le **utenze API/enterprise**, OpenAI afferma di _non_ far intervenire umani sulle richieste contenute, a meno che il cliente stesso le condivida via feedback. Dunque i dati aziendali rimangono riservati (anche perch√© spesso contengono segreti industriali); in ChatGPT Enterprise √® previsto che i log delle conversazioni non siano visibili al personale OpenAI n√© usati per training, salvo situazioni di abuso eccezionali.

**Controlli e opzioni per l'utente:** OpenAI ha progressivamente ampliato gli strumenti di controllo:  
\- **Impostazioni ChatGPT:** ogni utente pu√≤ disattivare la _chat history & training_ con un semplice toggle[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models). Da UI web o app mobile, questa opzione assicura maggiore riservatezza. Inoltre, in ChatGPT √® presente la modalit√† "**Temporary Chat**" (chat effimera) in cui si apre una sessione che non viene salvata nella cronologia per nulla - simile all'uso con history off ma per singola chat[\[61\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20ChatGPT%2C%20you,used%20to%20train%20our%20models).  
\- **Privacy Portal:** OpenAI ha istituito un portale (privacy.openai.com) dove gli utenti registrati possono gestire i consensi e fare richieste sui propri dati (accesso, cancellazione). In particolare possono inviare una richiesta per **non includere TUTTI i propri contenuti passati nel training** ("do not train on my content" dal privacy portal)[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models). Ci√≤ offre tutela anche retroattiva: ad esempio se uso ChatGPT da mesi ma ora non voglio che le mie vecchie chat vengano in futuro usate, posso presentare opposizione e OpenAI dovr√† escluderle.  
\- **Cancellazione account/chat:** l'utente pu√≤ eliminare singole conversazioni dall'interfaccia, e questo dovrebbe rimuoverle permanentemente dai server di OpenAI (salvo backup entro 30 gg)[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models). Pu√≤ anche richiedere la cancellazione completa del proprio account, che OpenAI evader√† (con l'avvertenza che i dati gi√† usati per training non possono essere "sottratti" ai modelli gi√† addestrati).  
\- **OpenAI Enterprise Admin:** per i clienti business, OpenAI offre una console admin dove si possono attivare/disattivare funzionalit√† (es. consentire o meno plug-in ai dipendenti), settare la retention (es. "retain chats 90 days" oppure "non conservare affatto")[\[60\]](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law)[\[62\]](https://openai.com/enterprise-privacy/#:~:text=%2A%20Enterprise,ChatGPT%20Enterprise%20and%20API), e gestire l'accesso tramite SSO SAML. √à data la possibilit√† all'admin di **disabilitare l'invio di feedback** a OpenAI a livello organizzativo[\[63\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Note), utile se un'azienda vuole assicurarsi che i dipendenti non condividano involontariamente dati sensibili cliccando su "feedback".

**Aggiornamenti recenti:** - **Aprile 2023:** in seguito a uno stop temporaneo di ChatGPT in Italia per questioni privacy, OpenAI ha aggiornato termini e privacy policy, introducendo il citato toggle per la cronologia e un'informativa pi√π chiara (in 13 lingue) sulle pratiche di dati.  
\- **Agosto 2023:** lancio ChatGPT Enterprise con certificazione **SOC 2** e rassicurazioni contrattuali sul trattamento dati (dati criptati in transito e a riposo, niente training, opzioni retention configurabili)[\[64\]](https://openai.com/enterprise-privacy/#:~:text=Our%20commitments%20provide%20you%20with,support%20for%20your%20compliance%20needs)[\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data). Contestualmente, annuncio di **ChatGPT Business** (piani a pagamento per team pi√π piccoli) con le stesse garanzie.  
\- **Dicembre 2023:** OpenAI ha implementato la funzione di **cancellazione mirata**: un utente pu√≤ chiedere via modulo web la rimozione di conversazioni specifiche dal suo storico permanente (oltre alla semplice cancellazione client-side, che gi√† c'era).  
\- **Giugno 2025:** aggiornamento "Enterprise Privacy" con ulteriori dettagli su **connettori** e **GPTs personalizzati**: confermato che anche se un'azienda collega ChatGPT ai propri database tramite plugin o connector, i dati estratti non finiscono in nessun training globale[\[65\]](https://openai.com/enterprise-privacy/#:~:text=more%20about%20GPTs%20%E2%81%A0). Anche i modelli personalizzati ("Custom GPTs") addestrati su dati aziendali restano segregati solo per quell'azienda[\[66\]](https://openai.com/enterprise-privacy/#:~:text=%2A%20Fine,and%20available%20features).  
\- **Ottobre 2025:** (ipotizzato) con l'entrata in vigore parziale dell'AI Act UE, OpenAI dovr√† fornire all'utente info su dati di addestramento. In attesa, OpenAI ha aderito a impegni volontari di **trasparenza** (ai Government US/EU) e sta sviluppando strumenti di watermarking delle output. Sul fronte dati, si segnala anche l'**azione legale** avviata da alcuni autori contro OpenAI (USA) per aver usato i loro testi come training senza permesso: questo solleva il tema dei **dataset di pre-training** originari, che includevano web scrape (CommonCrawl etc.). OpenAI ora permette via _robots.txt_ di escludere siti dal prossimo training (ha annunciato che il suo GPT-4 Turbo 2024 rispetter√† i file robots dei siti web per non includerli nei dati).

### Google - Bard / Gemini (AI generativa di Google)

**Dati raccolti:** Google raccoglie un ampio spettro di dati attraverso i suoi servizi di AI conversazionale, che nel 2023 si sono evoluti da _Google Bard_ a una suite unificata sotto il nome di **Gemini**. La **Privacy Hub di Google Gemini** dettaglia le categorie:  
\- **Contenuti forniti dall'utente:** tutto ci√≤ che l'utente _dice o inserisce_ in interazione con l'AI. Ci√≤ include il **prompt testuale** o vocale, gli eventuali file caricati (immagini, documenti per farli analizzare dal modello), i dati condivisi tramite funzioni di _"connected apps"_ (es. se l'utente chiede a Gemini di leggere una pagina web o un'email, il contenuto di quella pagina/email viene acquisito)[\[67\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Apps)[\[68\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,settings%2C%20device%20type%20and%20settings). Se l'utente usa l'AI in modalit√† vocal/video (es. _Gemini Live_), le **trascrizioni e registrazioni audio/video** di quell'interazione vengono raccolte[\[69\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,or%20%E2%80%9CSaved%20Info%E2%80%9Din%20some%20locales).  
\- **Contenuti generati dall'AI:** Google registra anche le **risposte generate** dal modello (testo, codice, immagini create, audio, video)[\[68\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,settings%2C%20device%20type%20and%20settings). Quindi l'intera conversazione (prompt + output) √® memorizzata. Anche _riassunti di chat_ o _link pubblici_ creati con Gemini sono considerati contenuti generati e tracciati.  
\- **Dati di app e dispositivo:** ogni interazione con Gemini √® accompagnata da dati di **telemetria**. Google colleziona informazioni sulle **app collegate** e servizi Google integrati (ad es. se l'utente ha attivato la "modalit√† Extensions" in Bard che consente all'AI di cercare su Google o estrarre dati da YouTube, il sistema registra quali servizi sono stati consultati). Raccoglie poi dettagli del **dispositivo** (modello, OS, versione app), browser e impostazioni locali, nonch√© identificatori unici e l'**indirizzo IP**[\[70\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=public%20links%20%2C%20citations%2C%20chat,metrics%2C%20crash%20and%20debug%20information)[\[71\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,collected%20through%20supplemental%20Gemini%20Apps). Inoltre, vengono loggate metriche di interazione: tempi di risposta, eventuali crash, clic sui suggerimenti.  
\- **Permessi e contesto device (mobile):** se l'utente usa l'app mobile di Gemini (o Google app con assistente), Google pu√≤ raccogliere dati supplementari: la rubrica/contatti (se funzionalit√† "help you keep in touch" attiva), log di chiamate e messaggi (per rispondere a prompt tipo "quando mi ha chiamato X?"), elenco di app installate, contenuto schermo (se abilita "overlay" per porre domande sullo schermo corrente)[\[72\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,with%20public%20Gemini%20Apps%20content)[\[71\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,collected%20through%20supplemental%20Gemini%20Apps). Queste sono informazioni potenzialmente molto sensibili, gestite con permessi Android/iOS, ma se concesse diventano input al modello e sono trattate come dati utente raccolti.  
\- **Informazioni di posizione approssimata:** Google rileva una stima di posizione (basata su IP o geolocalizzazione account) per contestualizzare risposte e per registrare da dove proviene l'attivit√† (ad es. per applicare policy regionali)[\[73\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,to%20Gemini%2C%20subscription%20related%20information).  
\- **Feedback espliciti e Gems:** quando l'utente fornisce un **feedback** (es. un rating üëç/üëé su Bard) o definisce istruzioni personalizzate (le "Custom instructions" di Bard/Gemini) o _Gems_ (agenti salvati dall'utente), questi dati vengono memorizzati[\[74\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,or%20%E2%80%9CSaved%20Info%E2%80%9Din%20some%20locales).  
\- **Dati supplementari da funzioni opt-in:** se l'utente opta per alcune feature sperimentali (es. condividere conversazioni pubblicamente via link, o usare "Canvas" per creare app visuali con l'AI), Google potrebbe raccogliere dati aggiuntivi specifici (es. il contenuto generato su una Canvas app)[\[75\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=use%20Gemini%20overlay%20to%20ask,co%2Fprivacypolicy%2Flocation)[\[76\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Audio%20Features).

In pratica, **tutto ci√≤ che passa per l'AI di Google viene loggato**: prompt, output, interazioni collegate (ricerche effettuate dall'AI per conto dell'utente), contesto applicativo. Google pu√≤ anche incrociare queste info con la cronologia utente su altri servizi: la Privacy Hub menziona l'uso di dati da _Search_ o _YouTube history_ come contesto che Gemini pu√≤ prendere se l'utente lo permette[\[77\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=public%20links%20%2C%20citations%2C%20chat,such%20as).

**Finalit√† d'uso:** Google applica la sua **Privacy Policy generale** anche a Gemini, enumerando varie finalit√†:  
\- **Erogazione del servizio**: usare i dati per fornire le funzionalit√† richieste (es. generare la risposta, ricordare le preferenze dell'utente nelle chat)[\[78\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Google%20uses%20this%20data%2C%20as,in%20our%20Privacy%20Policy%2C%20to).  
\- **Mantenimento e miglioramento**: impiegare i dati raccolti per addestrare e affinare i modelli di AI, migliorare la qualit√† delle risposte e la robustezza. Google esplicita che questi utilizzi "si estendono ai modelli di IA generativa che alimentano i nostri servizi"[\[79\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,and%20the%20public) - segno chiaro che i prompt e chat degli utenti servono a _trainare_ i modelli (parte di "maintain and improve service").  
\- **Sviluppo di nuovi servizi**: dati usati in forma aggregata per creare funzionalit√† future, prototipi.  
\- **Personalizzazione servizi**: se l'utente ha optato per la personalizzazione, i prompt possono influenzare suggerimenti futuri (ad es. se chiedo spesso ricette a Gemini, potrebbe personalizzare i risultati di Google Search verso ricette).  
\- **Comunicazioni con l'utente**: es. invio di avvisi su nuove feature.  
\- **Misurazione performance**: capire metriche di successo delle risposte, utilizzo (questo rientra nell'analitica).  
\- **Protezione di Google, utenti e pubblico**: significa usare i dati per moderazione, prevenire abusi, rimuovere contenuti illeciti, rispettare normative (ad es. filtri su hate speech).

Nella Privacy Hub viene sottolineato che _revisori umani_ visionano parte dei dati "per questi scopi"[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve). Google inoltre _non user√†_ questi dati per pubblicit√† personalizzata (lo ha affermato altrove per Bard: i prompt non alimentano il profilo ads dell'utente). La base legale: in EU, Google presumibilmente fa riferimento al **consenso** dell'utente (quando attiva Bard/Gemini, c'√® un consenso per "Attivit√† Gemini" che copre l'uso a scopi di miglioramento) e al contratto (servizio richiesto). Per gli utenti business (Google Workspace) la base √® il contratto/dati del cliente trattati come _processor_.

**Uso per training e miglioramento modelli:** di default, **Google utilizza i dati utente delle interazioni generative per addestrare e perfezionare i suoi modelli AI**. Questo avviene su due livelli:  
1\. **Miglioramento del modello generativo stesso (fine-tuning)** - le conversazioni e feedback vengono integrati nel ciclo di training futuro. Google ha esplicitamente avvisato che "le conversazioni con Bard potrebbero essere analizzate da revisori umani e servire a migliorare i modelli"[\[80\]](https://news.ycombinator.com/item?id=38186828#:~:text=Google%20Bard%20introduces%20,learning%20models). Il Privacy Hub nota che se l'utente disattiva l'attivit√†, _future conversazioni non verranno usate per migliorare i modelli_ n√© inviate a revisori[\[80\]](https://news.ycombinator.com/item?id=38186828#:~:text=Google%20Bard%20introduces%20,learning%20models). Ma per impostazione predefinita, ogni prompt e risposta pu√≤ essere prelevato (dopo un certo periodo) per entrare nel dataset di addestramento di Gemini e modelli successivi.  
2\. **Addestramento di sistemi di moderazione e funzioni di sicurezza** - come OpenAI, anche Google pu√≤ usare i dati raccolti per allenare modelli di filtro (es. riconoscimento di richieste proibite) o per migliorare integrit√†.

Google afferma di creare per questi scopi dataset _anonymized_. Secondo un'analisi indipendente, i dati delle chat che Google seleziona per training vengono **conservati fino a 3 anni** insieme a metadati (lingua, device) ma scollegati dall'account utente[\[81\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Human%20Reviews%20of%20User%20Gemini,Data)[\[82\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20retains%20user%20data%20reviewed,learning%20models%2C%20enhancing). In pratica, Google campiona conversazioni, rimuove identificativi diretti e le archivia in un corpus di training su cui fanno tuning del modello generativo (Gemini e affini). Quindi s√¨, a meno di opt-out, se chiediamo a Bard "Consigliami un film per bambini", quella chat - opportunamente anonimizzata - potrebbe contribuire a far s√¨ che in futuro il modello risponda meglio a richieste simili.

**Differenze consumer vs enterprise:** per **utenti consumer (account Google personali)**, l'uso dei dati per training √® opt-out (cio√® attivo salvo disattivazione), mentre per **clienti Google Workspace (azienda/scuola)** Google ha preso l'impegno opposto: _nessun dato del cliente verr√† usato per addestrare modelli generativi al di fuori del suo dominio_. In un documento sulle AI in Workspace si legge: "**Your content is not human reviewed or used for generative AI model training outside your domain, without permission**"[\[83\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). Ci√≤ significa che se un'azienda usa le funzioni AI (ad es. "Help me write" in Gmail con Gemini), i prompt e testi generati **restano entro il tenant** e Google non li utilizza per migliorare il modello base destinato al pubblico[\[83\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission)[\[84\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20DOES%20NOT%20Gemini%20DOES,your%20existing%20data%20protection%20controls). Questo ricalca l'approccio di Microsoft con Copilot (no training cross-tenant). Dunque Google distingue due flussi:  
\- _Ambito consumer:_ dati usati per training modelli generali (Gemini), a meno di opt-out.  
\- _Ambito enterprise:_ dati isolati, niente training globale. (Google potrebbe in futuro offrire opzioni di _opt-in_ per le aziende che vogliono contribuire, ma ad oggi non risulta).

**Conservazione e durata dei dati:** Il trattamento dei dati generativi di Google √® strettamente legato alle impostazioni scelte dall'utente:  
\- **Utenti con "Attivit√† Gemini" attiva (default):** le conversazioni vengono salvate nell'account Google dell'utente, in una sezione dedicata (accessibile tramite myactivity.google.com, voce "Gemini Apps Activity"). Il periodo di conservazione predefinito √® **18 mesi**[\[85\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Gemini%20Data%20Collection%20and%20Storage), coerente col default di altre attivit√† web Google. L'utente pu√≤ scegliere di ridurre a **3 mesi** oppure estendere a **36 mesi** tramite le impostazioni di auto-eliminazione[\[85\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Gemini%20Data%20Collection%20and%20Storage). Questo significa che, ad esempio, Bard conserver√† lo storico chat di un utente per un anno e mezzo per sua consultazione e per migliorare i risultati personalizzati. Dopo il periodo scelto, le voci pi√π vecchie vengono eliminate dall'account (ma come vedremo, Google potrebbe tenerne copie anonime altrove).  
\- **Utenti con Attivit√† disattivata (opt-out):** se l'utente disabilita la memorizzazione (cosa possibile fin da subito per Bard dall'estate 2023), **le conversazioni non vengono salvate nel proprio account oltre 72 ore**[\[86\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20stores%20user%20data%20from,Apps%20Activity%20settings%20but%20remains). In dettaglio, Google ha chiarito che anche con attivit√† off, le chat vengono comunque conservate per un breve periodo (fino a 3 giorni) "to facilitate service delivery and process feedback"[\[86\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20stores%20user%20data%20from,Apps%20Activity%20settings%20but%20remains), dopodich√© eliminate. Questo buffer serve a permettere operazioni tecniche (generare la risposta, far funzionare la chat) e dare tempo per segnalare abusi. Tali chat a breve termine _non compaiono_ nella sezione My Activity (che infatti √® disattivata), ma esistono temporaneamente sui server.  
\- **Dati revisionati da umani:** come detto, le conversazioni che Google seleziona per revisione e training sono conservate fino a **36 mesi** in forma pseudonima[\[81\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Human%20Reviews%20of%20User%20Gemini,Data). Dunque, potenzialmente, Google pu√≤ avere fino a 3 anni di backlog di chat anonime su cui addestrare nuovi modelli.  
\- **Enterprise (Workspace):** nel contesto aziendale, Google dichiara che per funzioni come "Help me write" in Gmail **non conserva i prompt e output dopo la sessione**: "Gemini in Workspace Apps - prompts and responses not retained after session ends"[\[87\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20in%20Workspace). Ovvero, se un dipendente chiede in Docs "Riassumi questo documento", quell'interazione √® effimera, sparisce una volta generato il risultato. Mentre per la **Gemini app in versione Enterprise** (cio√® se un'azienda abilita l'app stand-alone gemini.google.com per i propri utenti), l'admin pu√≤ scegliere se salvare le chat e per quanto (3, 18 o 36 mesi, default 18)[\[88\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=The%20Gemini%20app%20enables%20admins,The%20default%20is%2018%20months), analogamente al consumer. Ma pu√≤ anche disabilitare la cronologia (conversation history off), in tal caso le chat vengono memorizzate 72 ore max, come per consumer opt-out[\[88\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=The%20Gemini%20app%20enables%20admins,The%20default%20is%2018%20months).  
\- **Altri dati**: dati come log tecnici e diagnostici (IP, device info) possono essere conservati separatamente secondo le norme interne di Google (in genere log anonimi per X mesi). Google inoltre ha regole globali: in generale, se cancelli un dato dal tuo account, i sistemi online lo rimuovono abbastanza presto, ma copie di backup possono persistere per qualche tempo.

In sintesi: l'utente ha controllo fino a un certo punto. Se lascia attivo, Google tiene le chat 18 mesi (o 36 se scorda di ridurre). Se disattiva, Google di fatto butta le chat dopo 72h (mantenendo per√≤ quell'uso per rispondere e per eventuali controlli di policy nel frattempo).

**Revisione umana e sicurezza:** Google specifica che **personale umano (incluso personale di fornitori terzi contrattati)** **esamina una parte dei dati raccolti** per gli scopi elencati[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve). In particolare, team di **analisti/annotatori** controllano conversazioni estratte per valutare la qualit√† delle risposte, identificare errori o bias del modello e segnare contenuti problematici. Prima del lancio pubblico di Bard, ad esempio, Google ha fatto esaminare campioni di output ai suoi _rater_. Ora, su base continua, c'√® un pipeline: certe chat (ad es. lunghe conversazioni o quelle con thumbs down dall'utente) vengono inviate a un **tool di labeling** dove i revisori le leggono e marcano attributi (come "contiene disinformazione", "risposta utile?" ecc.). Google afferma: "per favore, non inserire informazioni confidenziali che non vorresti che un revisore umano veda o che Google usi per migliorare i nostri servizi"[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve). Questo √® un avviso molto diretto: vuol dire che qualunque cosa scriviamo potrebbe essere letta dallo staff. Proprio questa frase appare nel disclaimer di Bard e Gemini. Per i **dati enterprise** invece Google promette: "il tuo contenuto non √® soggetto a revisione umana fuori dalla tua organizzazione senza permesso"[\[83\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). Ossia, Google non far√† leggere a dipendenti i prompt di utenti Workspace, salvo che l'azienda stessa li condivida (o forse in forma aggregata per debug, ma non sistematico).

Google ha implementato anche **filtri automatici**: prima che Bard risponda, i prompt passano in modelli di moderazione che li classificano (contro hate speech, richieste illegali, dati personali ecc.), decidendo se bloccare o non rispondere. Idem per le risposte: c'√® un controllo di policy sulle output. Eventuali contenuti che violano (es. espressioni estreme) possono essere segnalati e revisionati a posteriori dal team di trust & safety.

**Controlli utente e impostazioni:** Google fornisce vari controlli:  
\- Nel **Google Account** > sezione **Dati e Privacy**, c'√® la voce **"Attivit√† delle App Gemini"** dove l'utente pu√≤: vedere ed eventualmente eliminare la cronologia delle domande fatte all'AI, impostare l'**auto-cancellazione** dopo 3, 18 o 36 mesi[\[89\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=You%20can%20visit%20your%20Google,your%20data%20and%20experience%2C%20like), e scegliere se **consentire l'uso dei dati per migliorare Google AI**. Questa ultima opzione √® cruciale: se disattivata, i prompt non vengono utilizzati per training (in teoria). Tuttavia, come specificato, disattivando l'attivit√† si impedisce l'uso per _miglioramento modelli_ e _revisione_, ma Google avverte che _comunque i tuoi prompt potranno essere usati per risponderti e per mantenere Gemini sicuro, anche con aiuto di revisori_[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). C'√® una distinzione sottile: _Gemini Apps Activity_ off blocca l'uso _per addestrare_ modelli (i dati non entrano nel dataset anonimo), ma Google potr√† ancora farli controllare per moderazione e tenerli 72h. Quindi un utente non pu√≤ impedire totalmente che i suoi contenuti siano letti da umani, poich√© la moderazione rientra nel _legittimo interesse/sicurezza_ (a meno di non usare affatto il servizio).  
\- **Eliminazione manuale:** l'utente pu√≤ cancellare singole interazioni (dall'interfaccia Bard c'√® la funzione "Cancella conversazione"). Inoltre pu√≤ utilizzare Google Takeout per esportare eventuali conversazioni e poi rimuoverle.  
\- **Personalizzazione:** se non vuole i prompt usati per personalizzare altri servizi, pu√≤ disattivare "Attivit√† Web e App", ecc. Google informa che disabilitare l'attivit√† Gemini non influenza altre attivit√† generali (es. se _Web & App Activity_ resta on, ricerche o posizioni potrebbero essere salvate altrove)[\[90\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=If%20you%20change%20your%20Gemini,you%20use%20other%20Google%20services).  
\- **Account Workspace admin:** per clienti enterprise, Google offre admin console dove si pu√≤ **abilitare/disabilitare le funzioni AI** per l'organizzazione, scegliere se permettere la "Gemini app" standalone o solo features integrate (tipo attivare/disattivare "Help me write"). Se attiva la Gemini app, l'admin pu√≤ decidere se attivare **conversation history** (con default retention) oppure disattivarla (nessun salvataggio oltre 72h)[\[91\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Prompts%20and%20responses)[\[88\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=The%20Gemini%20app%20enables%20admins,The%20default%20is%2018%20months). L'admin pu√≤ inoltre, tramite Vault (strumento di conformit√†), **impostare policy di retention** per i contenuti generati inseriti in altri prodotti (es. se un'email viene scritta da AI e salvata in bozza, quella bozza √® soggetta alle stesse retention delle email normali)[\[92\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Inserted%20or%20generated%20content%20in,Docs%2C%20Gmail%2C%20etc).  
\- **Modulo di opposizione (UE):** per i dati pubblici usati nel pretraining, Google (come Meta e OpenAI) offre modulistica GDPR per opporsi all'uso di propri dati personali presenti nel web scraping. Non direttamente legato ai prompt dell'utente, ma parte della governance complessiva.

**Aggiornamenti recenti e impegni:** - **Dicembre 2022**: Google pubblica AI Principles (post-ChatGPT mania) includendo la privacy come principio.  
\- **Febbraio 2023**: lancio limitato di Bard (basato su LaMDA), con informativa dedicata e opt-out di salvataggio (gli utenti potevano gi√† scegliere di non far salvare le conversazioni in account).  
\- **Luglio 2023**: Google attiva di default Bard per utenti over 18 in 230 paesi (non UE). Introduce le **Bard Extensions** (collegamento a Gmail, Docs, etc.), accompagnato da disclaimers su dati: i contenuti presi dalle app utente per rispondere restano entro la sessione e non usati altrove.  
\- **Settembre 2023**: inizia lancio di _Bard in EU_ con ritardo (dovuto a discussioni con regolatori). Google implementa il pulsante "Export" per facilitare portare le risposte generative in Gmail o Docs.  
\- **Ottobre 2023**: annuncia l'evoluzione in **Gemini** (nuovo modello multimodale) e l'integrazione delle funzionalit√† (Bard col modello Gemini). Lancia inoltre **NotebookLM** come prototipo di AI per riassunti di documenti personali - anch'esso coperto da privacy simile (non memorizza prompt dopo sessione in Workspace Edu).  
\- **Novembre 2023**: pubblica la **Workspace AI Privacy Hub** aggiornata (con le tabelle illustrate sopra)[\[93\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Last%20updated%3A%C2%A0November%204%2C%202025)[\[94\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20data%20retention), chiarendo formalmente gli impegni per clienti business.  
\- **Aprile 2025**: Google Cloud conferma in una nota che non user√† mai dati dei clienti per addestrare modelli senza consenso[\[95\]](https://services.google.com/fh/files/misc/genai_privacy_google_cloud_202308.pdf#:~:text=,customer%27s%20prior%20permission%20or) (impegno gi√† noto).  
\- **Novembre 2025**: adegua privacy policy e termini via il _Transparency Center_, includendo sezioni sull'uso dei dati per AI (richiesto da Digital Services Act UE e in vista AI Act).

### Anthropic - Claude

**Dati raccolti:** **Claude** (Anthropic) √® accessibile via API e attraverso un'interfaccia chat (claude.ai) in versioni Free, Pro e Max. I **dati utente raccolti** includono: i **prompt** inseriti (testo e eventuali allegati; ad es. codice inviato per analisi), l'intera **conversazione** (dialogo cumulativo), i **metadati di utilizzo** (orari, indirizzo IP, info browser) e qualsiasi **feedback esplicito** inviato (reazioni, segnalazioni). La _Privacy Policy_ di Anthropic (agg. 5 settembre 2023) definisce "**chat and coding session data**" come il contenuto che possono usare per migliorare i modelli[\[96\]](https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training#:~:text=Center%20privacy,If). Inoltre raccoglie i dati di account (email, telefono per 2FA) e dati di pagamento se utente Pro/Max. Non risultano funzionalit√† di integrazione con app esterne per ora, quindi i dati sono principalmente quelli forniti dall'utente nella chat stessa. A livello tecnico, Anthropic conserva anche **log delle richieste API** simili a OpenAI, con id e timestamp.

**Finalit√† dichiarate:** Anthropic, come da Privacy Policy, usa i dati utente per: erogare il servizio (generare risposte con Claude), migliorare e addestrare i suoi modelli AI, prevenire abusi e problemi di sicurezza (moderazione), comunicare con gli utenti (assistenza, avvisi di policy), e rispettare obblighi legali. In UE, nel suo privacy center afferma che la base legale per i normali utenti consumer √® il **consenso** per usare i dati nel training (consenso che dal 2025 chiede esplicitamente), mentre per i clienti commerciali la base √® contrattuale e i dati non vengono usati oltre l'esecuzione del servizio. Anthropic non fa pubblicit√†, quindi non usa dati a scopi di marketing.

**Uso per training modelli - evoluzione delle policy:** inizialmente Anthropic era tra i pi√π attenti: fino al 2023 dichiarava che **non utilizzava le conversazioni degli utenti per addestrare i modelli di base**. Infatti, testate hanno riportato che "Claude era uno dei pochi chatbots principali a **non** addestrarsi automaticamente sulle chat degli utenti"[\[97\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Prior%20to%20Anthropic%20updating%20its,user%20chooses%20to%20opt%20out). Questo scenario √® cambiato nel **fine 2023-2024**: con la necessit√† di migliorare Claude e competere, Anthropic ha rivisto la policy.

- **Fino a met√† 2023:** per utenti Claude (ad esempio su Poe o Slack) i dati non venivano usati per training by default; venivano per√≤ conservati per moderazione per 30 giorni. L'utente _poteva_ acconsentire a condividere conversazioni per ricerca, ma era opt-in.
- **Settembre 2023:** emergono rumor di un cambio - Wired pubblica a fine mese che Anthropic aggiorner√† la privacy policy per cominciare a usare le chat come training data a meno di opt-out[\[98\]\[99\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models). La policy era prevista entrare in vigore il 28/9/23 ma poi l'hanno posticipata all'**8 ottobre 2025** per dare pi√π tempo agli utenti di scegliere[\[100\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20developers%20hope%20to%20make,of%20their%20chatbot%20over%20time). (La data 2025 appare in Wired - in realt√† il grosso del cambio fu annunciato fine agosto 2025).
- **28 agosto 2025:** Anthropic annuncia formalmente gli aggiornamenti per utenti _consumer_ (Free, Pro, Max)[\[101\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Today%2C%20we%27re%20rolling%20out%20updates,be%20done%20at%20any%20time). Introduce una finestra decisionale: l'utente deve scegliere se "allow usage of your data to improve Claude" oppure no[\[102\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=You%E2%80%99re%20always%20in%20control%20of,window%20like%20the%20one%20below). Il default √® di fatto **opt-in al training** (il toggle in app √® pre-selezionato su ON)[\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy), quindi se l'utente accetta i nuovi termini senza toccarlo, viene incluso[\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy). Chi rifiuta (opt-out) potr√† continuare a usare Claude ma con i dati esclusi dal training.
- **A partire da ottobre 2025:** tutte le nuove conversazioni degli utenti consumer che hanno acconsentito verranno utilizzate per **addestrare le nuove versioni di Claude**[\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=). Quelle di chi ha negato, no. Importante: Anthropic precisa che **solo le chat nuove o riprese** dopo l'opt-in contano - le vecchie chat pregresse non verranno retroattivamente usate a meno che l'utente le riapra (riaprendole, diventano "attive" e quindi "idonee" al training da quel punto in poi)[\[105\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20new%20models). Dunque c'√® un focus su dati freschi.

Per i **clienti aziendali (Claude for Work, Claude Pro per organizzazioni, API, partner cloud)**, Anthropic **non utilizza** i dati per training a meno di accordi specifici[\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI)[\[107\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20do%20not%20apply,under%20our%20Commercial%20Terms%2C%20including). Lo ribadisce: "questi cambiamenti non si applicano ai servizi sotto i Commercial Terms, inclusi Claude for Work (Team ed Enterprise), API, Amazon Bedrock, Google Vertex, Claude Gov/Ed"[\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI)[\[107\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20do%20not%20apply,under%20our%20Commercial%20Terms%2C%20including). In altre parole, come OpenAI e Google, anche Anthropic rispetta l'isolamento dei clienti business.

**Durata di conservazione:** fino al 2023, Anthropic affermava di eliminare i dati delle conversazioni entro **90 giorni** o al massimo **1 anno** nei log, mantenendo 30 giorni per moderazione attiva. Ma nel 2025 con la nuova policy c'√® un grosso cambiamento:  
\- Se l'utente **consente l'uso per training**, Anthropic estende la retention delle conversazioni a **5 anni**[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention). L'idea √® che avendo uno storico lungo aiutano il ciclo di sviluppo modelli (Anthropic motiva che i cicli di training durano 1-2 anni e serve consistenza nei dati raccolti[\[108\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,the%20data%20retention%20period)). Dunque chat e codice di utenti opt-in potranno restare nei sistemi fino al 2030!  
\- Se l'utente **non consente (opt-out)**, rimane la retention breve precedente: i dati della chat saranno conservati solo **30 giorni** (per motivi di sicurezza/abuso) e poi cancellati[\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period)[\[13\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,training%20during%20the%20signup%20process).  
\- In entrambi i casi, l'utente ha sempre la possibilit√† di **cancellare manualmente singole conversazioni** dall'UI Claude.ai; se lo fa, Anthropic garantisce che quelle conversazioni _non saranno usate nel training futuro_[\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period). (Anche se potrebbero averle gi√† viste per moderazione, ma vengono escluse dai dataset).  
\- I **feedback inviati** (es. segnalazioni su output) se utente √® opt-in saranno conservati 5 anni come le chat, se opt-out suppongo 30gg come resto.

Questa conservazione lunga (5 anni) √® notevole e ha suscitato critiche (c'√® stata discussione su Reddit e post su Medium evidenziando "Anthropic now retains your data for 5 years if you opt in"[\[110\]](https://www.reddit.com/r/ClaudeAI/comments/1nd73si/anthropics_new_privacy_policy_is_systematically/#:~:text=Anthropic%27s%20New%20Privacy%20Policy%20is,Claude%20can%20potentially%20access)). Anthropic dice che i 5 anni valgono solo per i dati raccolti con permesso dal momento del cambio in poi, e servono per migliorare classifcatori di abuso di lungo periodo[\[111\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=The%20extended%20retention%20period%20also,keep%20Claude%20safe%20for%20everyone).

**Revisione umana:** come gli altri, Anthropic utilizza annotatori umani per migliorare Claude. Ad esempio, la RLHF di Claude √® stata condotta con principi di "Constitutional AI" (dove piuttosto che feedback umani su ogni risposta, usano un insieme di regole e poi intervento umano su casi limite). Comunque, la Privacy Policy ammette che **personale di Anthropic pu√≤ visualizzare i contenuti dell'utente** per debug e miglioramento (specialmente se segnalati come problematici). Nel _Claude Privacy Center FAQ_ c'era scritto: "_we may use some chat data to improve models, including entire conversation with any content_"[\[96\]](https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training#:~:text=Center%20privacy,If), suggerendo che intere chat possano essere lette dagli ingegneri. Con la nuova policy di _opt-in_, solo le chat di chi ha aderito verranno passate al team di modello. Quelle di opt-out dovrebbero essere toccate solo per moderazione se emergono violazioni (p.e. un sistema automatico segnala una conversazione, allora un reviewer la vede per decidere provvedimenti).

Anthropic afferma di applicare filtri automatici per privacy: come OpenAI, usano tecniche per rilevare e offuscare PII prima di utilizzare i dati per training. Inoltre, i dati utente vengono visti solo da personale autorizzato (Anthropic √® SOC 2 compliant). Un caso particolare: **Claude Pro "Constitutional" mode** - in futuro prevedono modalit√† dove l'utente pu√≤ scegliere se contribuire o no.

**Controlli offerti:**  
\- **Interfaccia utente (claude.ai):** con l'aggiornamento, appare un pop-up per utenti esistenti dove poter impostare il toggle "Help improve Claude" (S√¨/No)[\[112\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=for%20existing%20Claude%20app%20users). Questo toggle √® poi sempre accessibile in **Privacy Settings** dell'account, quindi l'utente pu√≤ cambiare idea in ogni momento[\[113\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=the%20signup%20process.%20,five%20years%20if%20you%20allow). Cambiando l'impostazione in Off, le nuove chat non saranno pi√π usate per training.  
\- **Cancellazione chat/account:** l'utente pu√≤ eliminare manualmente le chat e pu√≤ anche cancellare l'intero account (tramite supporto). Anthropic conferma che se uno elimina l'account o disattiva l'opzione training, escluderanno quei dati da futuri training e li rimuoveranno quando non pi√π necessari[\[114\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=If%20you%20change%20your%20setting,our%20data%20retention%20practices%20here)[\[115\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=You%20can%20always%20update%20your,Your%20data%20will%20still%20be).  
\- **Opt-out prima del cambio:** Anthropic aveva mandato email agli utenti (e mostrato banner) con scadenza 8 ottobre 2025: chi non accettava i nuovi termini e non faceva scelta, credo veniva sospeso dal servizio finch√© non sceglieva (Wired dice "dovrai scegliere per continuare a usare Claude"[\[116\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Starting%20today%2C%20we%E2%80%99re%20rolling%20out,Privacy%20Settings%20at%20any%20time)). Quindi hanno cercato di ottenere un consenso esplicito da tutti.  
\- **Clienti enterprise/API:** questi hanno contratti dedicati. Per API, Anthropic gi√† di default non trattiene i contenuti (oltre log minimi). Inoltre, **Claude for Enterprise** (lanciato luglio 2023) dichiara: "we do not train Claude on your conversations and content" come punto vendita[\[117\]](https://amstlegal.com/anthropics-claude-ai-updated-terms-explained/#:~:text=Terms,changes%20%26%20protect%20your%20business). Quindi per business c'√® garanzia scritta.  
\- **Strumenti di privacy UE:** Anthropic essendo meno mainstream non ha ancora un portale self-service, ma indica di poter contattare <privacy@anthropic.com> per richieste dati.  
\- **Claude Instant (modello locale):** da citare che Anthropic ha anche modelli deployabili local, ma il servizio pubblico √® cloud.

**Aggiornamenti degni di nota:**  
\- **Marzo 2023:** Anthropic rilascia Claude via API beta; ottiene investimenti da Google. Privacy stance iniziale: "non tratterremo dati cliente oltre 30gg e non li useremo per training".  
\- **Luglio 2023:** Lancio Claude 2, con pi√π enfasi su controllo utente e con pacchetto **Claude Pro**. In quell'occasione, implementano _chat history_ (prima su Slack integr., ora chat anthro).  
\- **Agosto 2023:** Emendano ToS per chiarire rimozione di dati su richiesta e supporto GDPR.  
\- **Agosto 2025:** Annuncio via blog e email di **nuova Privacy Policy** effettiva dal 8/10/25 con _opt-out opt-in switch_ e retention 5 anni[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention).  
\- **Settembre 2025:** Wired e altri diffondono la notizia e come fare opt-out[\[98\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models)[\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy).  
\- **Ottobre 2025:** roll-out completato; Anthropic afferma che la maggior parte degli utenti ha optato-in (non abbiamo dati pubblici, ma √® plausibile visto toggle preattivato).  
\- **Dicembre 2025 (futuro):** con potenziali regolamentazioni, Anthropic potrebbe dover offrire pi√π trasparenza (dataset originari etc.). Intanto, come impegno volontario, a luglio 2023 ha firmato con OpenAI e altri Casa Bianca commitment per sviluppare watermark etc.

### Microsoft - Copilot (servizi AI di Microsoft)

**Dati raccolti:** Microsoft fornisce funzionalit√† di AI generativa principalmente in due categorie:  
\- **Copilot per Microsoft 365** (integrato in Office: Word, Excel, PowerPoint, Outlook, Teams, etc.) e **Copilot Chat** (Business Chat che attinge a Microsoft Graph).  
\- **Azure OpenAI** (servizio cloud dove i dati dei clienti vengono processati da modelli OpenAI isolatamente).

Qui ci concentriamo su **Microsoft 365 Copilot**, lanciato nel 2023 per utenti business. I dati coinvolti:  
\- **Prompt dell'utente**: la richiesta che l'utente pone a Copilot, spesso in linguaggio naturale (es. "Riassumi le mail non lette di oggi" in Outlook). Questo prompt viene trasmesso dal client (es. Outlook) ai backend di Microsoft.  
\- **Contenuti aziendali richiamati**: Copilot accede ai **dati dell'organizzazione** (per cui l'utente ha permessi) tramite Microsoft Graph: documenti SharePoint/OneDrive, messaggi di posta, chat Teams, calendario, contatti, ecc.[\[118\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=data,accurate%2C%20relevant%2C%20and%20contextual%20responses)[\[119\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20only%20surfaces,you%20give%20to%20users%20outside). Questi contenuti (ad es. il testo di una mail rilevante per rispondere al prompt) vengono estratti e passati come **contesto** al modello GPT-4 che genera la risposta[\[120\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20you%20enter%20prompts%20using,Data%20stored%20about%20user%20interactions). Dunque, M365 Copilot di fatto "legge" i dati aziendali ma **non li memorizza permanentemente fuori** - li usa on the fly per produrre l'output.  
\- **Risposta generata**: il testo (o tabella, immagine) generato da Copilot come output viene potenzialmente salvato se l'utente lo **inserisce** in un documento o email. In tal caso, diventa parte dei normali dati aziendali (salvati in SharePoint, Exchange etc.). Microsoft registra inoltre la risposta per mostrarla nella history utente.  
\- **Log di interazione**: Microsoft **conserva traccia** delle interazioni con Copilot (prompt + risposta) in quella che chiama "Copilot activity history"[\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot). Questo consente all'utente di rileggere le ultime richieste in Business Chat e per debug interno. I log includono anche riferimenti (citations) ai dati aziendali usati.  
\- **Dati tecnici**: come sempre, ID utente, tenant ID, timestamp, performance metrics e possibili errori vengono registrati. Microsoft segnala che i prompt/responses sono considerati "customer data" a tutti gli effetti e li tratta come tali (cifratura etc.)[\[122\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20a%20user%20interacts%20with,in%20Microsoft%20365%20Copilot%20Chat)[\[123\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot).

**Finalit√† e base legale:** Microsoft posiziona Copilot come **servizio enterprise cloud**, quindi tratta i dati utente come _dati del cliente_ sotto l'**Addendum sulla Protezione Dati** (DPA). Base legale: esecuzione del contratto col cliente (organizzazione) per fornire il servizio richiesto. Microsoft afferma chiaramente: _non usa i dati dei clienti per addestrare o migliorare i modelli di base_[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important). Il loro scopo √® fornire output utili e rispettare privacy e sicurezza corporate. Possono usare i dati aggregati (telemetria) per migliorare il servizio in generale, ma dicono di farlo senza usare il contenuto. Unica eccezione: **feedback** facoltativo di utenti, che se inviato pu√≤ essere usato per migliorare Copilot (ma comunque non per addestrare GPT-4, solo per affinare magari il sistema o valutare feature)[\[63\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Note). In sostanza, Microsoft agisce da **Responsabile del trattamento (processor)** che non tocca i dati oltre quanto necessario per erogare la funzionalit√† al cliente.

**Uso per training dei modelli:** **No**, per design. Microsoft ha negoziato con OpenAI che i modelli GPT-4 forniti tramite Azure OpenAI per Copilot **non facciano learning sui dati elaborati** - n√© immediatamente (nessun fine-tuning online), n√© a posteriori. Microsoft stessa sottolinea: "**Prompts, responses, and data accessed through Microsoft Graph _aren't used to train foundation LLMs_**, including those usati da Copilot"[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important). Questo √® un impegno contrattuale: scritto nel Microsoft Product Terms che i dati del cliente non alimentano modelli base. Quindi diversamente dai servizi consumer, qui l'output dell'AI per un cliente non migliora l'AI per altri clienti.

Inoltre, Microsoft afferma di aver **disabilitato il monitoring con revisione umana** che era presente di default in Azure OpenAI: "while abuse monitoring (with human review) is available in Azure OpenAI, Microsoft 365 Copilot services have **opted out** of it"[\[124\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=While%20abuse%20monitoring%2C%20which%20includes,section%20later%20in%20this%20article). Dunque nemmeno moderazione manuale: se un utente in azienda scrive insulti a Copilot, non finir√† sotto gli occhi di OpenAI staff. (Va notato per√≤ che modelli come GPT-4 hanno filtri integrati, quindi risponderanno "non posso fare X" a richieste vietate, ma quell'evento non viene inoltrato a persona).

**Conservazione e controlli enterprise:**  
\- **Boundary di servizio e residenza dati:** Microsoft 365 Copilot √® integrato nell'ecosistema M365, quindi i dati trattati rientrano nell'**Enterprise Trust**. Copilot risiede su infrastruttura **Azure in regione**: per clienti UE, √® dichiarato conforme all'EU Data Boundary, elaborando i prompt preferibilmente in datacenter UE (salvo necessit√† di fallback in altre regioni, comunque con garanzie)[\[125\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20the,EU%20Data%20Boundary). A marzo 2024, Microsoft ha aggiunto Copilot tra i servizi coperti dagli impegni di residenza dati e Multi-Geo[\[126\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20data,residency). Quindi, un'azienda pu√≤ stare certa che i suoi prompt e risposte rimangono in quell'ambito geografico come da contratto.  
\- **Conservazione storici e eliminazione:** i **prompts e responses** di Copilot Chat non sono conservati a lungo termine dall'app: vengono per√≤ registrati in "Copilot activity history" disponibile per l'utente, e anche esportabili via Content Search e Teams Export API[\[127\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles)[\[128\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=For%20Microsoft%20Teams%20chats%20with,to%20view%20the%20stored%20data). Se un'azienda vuole, pu√≤ **configurare policy di retention** su questi record con Microsoft Purview, ad esempio per cancellare le interazioni dopo X giorni o conservarle per eDiscovery[\[129\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=LLMs%2C%20including%20those%20used%20by,Microsoft%20365%20Copilot). L'utente stesso ha un controllo: pu√≤ andare sul portale account Microsoft e cancellare il proprio storico Copilot[\[130\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Deleting%20the%20history%20of%20user,interactions%20with%20Microsoft%20365%20Copilot) (funzionalit√† "Delete your Copilot activity history"). Microsoft memorizza i log di interazioni come parte di Exchange Online mailboxes (pare li equipari a altri dati utente). In mancanza di policy custom, tali dati potrebbero conservarsi a tempo indefinito come parte account, ma presumibilmente soggetti a stesse regole di retention di altri file (es. se l'azienda ha default 5 anni su mailbox, potrebbe applicarsi).  
\- **Controlli amministratore:** Microsoft 365 admin pu√≤ **abilitare/disabilitare** Copilot per l'organizzazione o per utenti specifici tramite licenze. Pu√≤ gestire le **settings** su cosa Copilot pu√≤ accedere: in realt√† Copilot rispetta le permission esistenti su file e chat, quindi non vi sono settaggi granulari: se uno non ha accesso a un documento, Copilot non glielo riveler√† (fa parte del design: "Copilot surfaces data a cui l'utente ha almeno permesso di lettura, altrimenti no"[\[131\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20only%20surfaces,shared%20channels%20in%20Microsoft%20Teams)). Il tenant admin pu√≤ inoltre controllare l'**invio di feedback a Microsoft**: c'√® una policy "Allow Recommender System Feedback" centralizzabile (come citato, Microsoft permette feedback utente e potenzialmente li usa in aggregate per migliorare). L'admin pu√≤ disattivare questa telemetria se preferisce niente condivisione.  
\- **Moderazione contenuti:** Microsoft afferma di aver integrato filtri per bloccare output inappropriati (usano i filtri OpenAI e regole proprie di Microsoft). Se un utente tenta di generare, ad es., uno scenario offensivo, Copilot rifiuter√†. Microsoft non raccoglie questi casi per training (opt-out da monitoring). L'admin ha anche la possibilit√† di definire **injection di prompt di sistema personalizzati** (non ancora disponibile ma promesso) per aggiungere regole del tipo "non menzionare questi argomenti‚Ä¶".  
\- **Trasparenza per utenti finali:** ogni risposta di Copilot fornisce **citazioni** con link alle fonti interne usate (documenti o mail)[\[132\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20a%20user%20interacts%20with,24%20meetings%20in%20Microsoft), cos√¨ l'utente sa da dove viene il dato. Ci√≤ √® parte di governance (evitare "allucinazioni" incontrollate).  
\- **Contrattualistica:** Copilot √® coperto dagli stessi impegni di M365: **garanzie di privacy (DPA), sicurezza** (crittografia AES-256 at rest, TLS1.2 in transito) e conformit√† (ha Cloud Security Alliance CAIQ, ISO27001, etc.). Microsoft ha anche un programma di **responsible AI** interno e fa auditing periodici delle funzionalit√† Copilot per equit√† e privacy.

**Riassunto:** Microsoft adotta un approccio **walled garden** per i dati di Copilot: i prompt restano del cliente, non diventano parte di un modello pubblico. Ci√≤ elimina molti rischi (es. fuga di IP in risposte ad altri, come avvenuto con ChatGPT). Ovviamente Microsoft potrebbe utilizzare _metadati anonimi_ (tipo "il 30% degli utenti chiede riassunti di meeting"), ma non il contenuto specifico. Questo √® stato ribadito in risposte pubbliche: ad es. quando gir√≤ voce su un "Copilot setting: use my data to train", Microsoft chiar√¨ trattarsi di un misunderstanding, confermando _no training on customer data_[\[133\]](https://learn.microsoft.com/en-us/answers/questions/5298757/does-microsofts-copilot-pro-for-office-365-use-you#:~:text=,LLMs%20to%20your%20organizational).

**Aggiornamenti recenti:**  
\- **Mar 2023:** annuncio Microsoft 365 Copilot; in anteprima a pochi clienti, enfatizzando "privacy by design" (fu detto: Copilot non ha memoria al di fuori del contesto utente).  
\- **Lug 2023:** Microsoft risponde a un articolo erroneo che ipotizzava training su dati M365, negandolo e rassicurando sul fronte privacy[\[134\]](https://www.thurrott.com/a-i/microsoft-copilot-a-i/313765/microsoft-says-it-is-not-training-copilot-ai-on-your-microsoft-365-data#:~:text=Microsoft%20Says%20it%20is%20Not,Which%20seems%20unnecessary).  
\- **Set 2023:** M365 Copilot pubblic preview; Microsoft pubblica _Product Terms_ aggiornati includendo clausole di **no use for training**.  
\- **Nov 2023:** Copilot GA (disponibilit√† generale). Lancio anche di **Copilot for Teams chat**. Viene resa disponibile la **history** delle interazioni (visibile in Teams e Office apps), con pulsante per cancellare la history nel Microsoft Account.  
\- **Mar 2024:** come detto, includono Copilot nell'EU Data Boundary ufficialmente[\[126\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20data,residency).  
\- **Ott 2024:** iniziano piani per **Copilot per utenti consumer** (ad es. Windows Copilot integrato in Windows 11). Qui la dinamica differisce, ma in Italia/UE √® disattivato per ora. In futuro se avremo Copilot su account Microsoft personali, bisogner√† vedere se usano dati per migliorare modello consumer. Microsoft ha detto al momento che Windows Copilot √® basato su Bing Chat (che ha sue policy), quindi i dati consumer comunque vanno su Bing (che li usa per training in forma aggregata probabilmente).

### Meta - LLM e AI assistant (Meta AI, Llama)

**Dati raccolti:** Meta (Facebook) √® entrata nell'AI generativa con vari prodotti nel 2023: modelli open-source (Llama 2) e soprattutto **Meta AI assistant** integrato in Messenger, Instagram, WhatsApp e occhiali Ray-Ban (con personaggi AI tipo Snoop Dogg, etc.). Meta sta anche costruendo modelli multimodali ("Emu" per immagini). La gestione dati di Meta AI ha due componenti: (a) i **dati di addestramento iniziale** dei modelli, molti dei quali provenienti dal web (incluse piattaforme Meta), e (b) i **dati generati dall'uso** dei chatbot.

Per il punto (a), Meta ha annunciato che inizier√† a usare **contenuti pubblici** di Facebook e Instagram (post, commenti visibili a tutti) per addestrare i suoi LLM[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,). In USA lo fa gi√† da anni "silenziosamente"[\[136\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=While%20Meta%20has%20been%20training,data%20to%20train%20AI%20models); in EU, a causa di GDPR, aveva messo pausa nel 2024[\[137\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=a%20clear%20legal%20basis%20for,data%20to%20train%20AI%20models), ma da aprile 2025 ha iniziato con notifiche agli utenti[\[138\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,messages%2C%20nor%20public%20data%20from)[\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly). Meta dice che _non_ utilizzer√† i dati di **messaggi privati** (Messenger, WhatsApp, DM IG) n√© i contenuti non pubblici degli utenti per trainare modelli[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models). Inoltre esclude i dati di minori (<18) in EU anche se pubblici[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,). Questo √® cruciale: se scrivo un post pubblico "Buongiorno!", potrebbe finire nel dataset di training di Llama; se scrivo un messaggio privato a un amico, Meta afferma di non usarlo nel dataset.

Per il punto (b), quando si chatta con **Meta AI** su Messenger/IG, i dati raccolti includono: l'**input utente** (messaggio di chat, foto inviata al bot), la **risposta generata** dall'assistente, e eventuali _interazioni_ (feedback, se utente mette like al messaggio del bot, ecc.). Meta in app ha avvisato: "Le tue interazioni con Meta AI potranno essere utilizzate per addestrare e migliorare i modelli AI" (notifica in-app). Quindi presumibilmente, a meno di opt-out, i messaggi che scriviamo al bot e le risposte vengono loggati e usati dal team AI per tarare i modelli. **Temporaneamente**, per l'UE, a fine 2023 Meta ha lanciato Meta AI con dataset limitato e disse che al momento non avrebbe usato quelle chat per training finch√© non chiarito col DPC (infatti la news dice "limited version launched in EU after delay"[\[141\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=interactions%20with%20Meta%20AI%20will,used%20to%20train%20its%20models)). Ma dall'aprile 2025, con l'annuncio, sembra che useranno anche le **interazioni degli utenti con Meta AI** come dati di training[\[142\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=regulatory%20pressure%20due%20to%20data,used%20to%20train%20its%20models).

**Finalit√† e base legale:** Meta sostiene che per usare dati di utenti europei nei training deve avere una base legale solida. Nel dicembre 2024 il Board dei Garanti europei (EDPB) ha dato un'opinione per cui la base del **legittimo interesse** pu√≤ essere accettata per usare dati pubblici negli addestramenti[\[143\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Last%20year%2C%20we%20delayed%20training,AI%20to%20people%20in%20Europe), cosa che Meta ha interpretato come via libera (lo affermano: l'EDPB ha confermato che il nostro approccio originale era conforme)[\[143\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Last%20year%2C%20we%20delayed%20training,AI%20to%20people%20in%20Europe). Quindi Meta procede su tale base, offrendo per√≤ il diritto di **opposizione** (GDPR Art.21) agli utenti. Il Garante Irlandese ha richiesto le notifiche e modulo di opt-out. Per i dati di interazioni con Meta AI, probabilmente Meta li gestisce sotto base contrattuale (fornitura servizio assistente) e legittimo interesse (migliorare i modelli). Hanno comunque integrato nelle impostazioni privacy un **"AI usage" opt-out form**. Ad esempio, un utente UE pu√≤ compilare un modulo per opporsi all'uso dei suoi dati (pubblici e personali) nei training AI di Meta[\[144\]](https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/#:~:text=Reddit%20www,not%20be%20available%20to%20you). Questo modulo sul Privacy Center consente di "object to processing of your data for AI model training". Va segnalato entro 30 giorni dall'avviso (scadenza iniziale era 26 maggio 2025 per chi c'era, ma modulo resta disponibile)[\[145\]](https://www.iamexpat.de/expat-info/germany-news/how-european-users-can-opt-out-meta-using-their-data-train-ai#:~:text=How%20European%20users%20can%20opt,to%20train%20its%20AI%20system).

**Uso per training:**  
\- **Web e post pubblici:** Meta ora (dal 2025) **usa i post/commenti pubblici degli utenti EU nei dataset di training** dei LLM (ad es. Llama 3). Chi vuole pu√≤ fare opt-out: Meta "honorer√† tutti i moduli di opposizione ricevuti"[\[146\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=notifications%20to%20explain%20that%20Meta,), escludendo i contenuti di quegli utenti. Da notare: se Tizio ha un post pubblico e oppone, non lo useranno; se Caio commenta quel post e Caio non ha opposto, forse prendono il commento di Caio.  
\- **Interazioni con Meta AI (Assistant):** Meta dice che inizialmente quelle chat sarebbero state usate anch'esse per training modelli. Dall'ott 2023 al mar 2024 erano limitate, ma dal 2025 scrivono "le interazioni con Meta AI saranno usate per addestrare i modelli"[\[142\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=regulatory%20pressure%20due%20to%20data,used%20to%20train%20its%20models). Quindi se chatto col bot su Instagram, quella conversazione pu√≤ finire nel dataset per migliorare l'assistente. Non √® chiaro se c'√® opt-out per questo specifico (forse il modulo generale copre tutto).  
\- **Dati privati:** Meta ribadisce che _non user√† chat private per training_[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,). Lo stesso CEO Zuckerberg dichiar√≤ "non useremo i messaggi WhatsApp per addestrare l'AI" per rassicurare.  
\- **Dati di minorenni:** esclusi per training.  
\- **Altre fonti:** Meta gi√† addestra sui dati pubblici esterni (es. Wikipedia, CommonCrawl) e su licenze (ha accordi con Shutterstock per immagini ad esempio).  
In sintesi, Meta ora segue Google/OpenAI: i dati utente (pubblici e conversazionali) vengono sfruttati per _allenare_ i modelli generativi e rendere l'AI pi√π "socialmente competente".

**Conservazione e protezione:** Meta non ha fornito tempi precisi di retention per i prompt/risposte utente con AI. Essendo integrato in Messenger/IG, quelle chat con AI potrebbero essere trattate come normali messaggi (conservati finch√© l'utente non li elimina, e comunque salvati su server finch√© account attivo). Meta tende a conservare i dati _fintanto che utili_, a meno di richiesta di cancellazione. Le conversazioni con bot potrebbero essere soggette a moderazione (Meta filtra contenuti per policy).

**Revisione umana:** Sicuramente Meta impiega testers umani. Il rischio di privacy √® che se uno confida segreti al bot, degli annotatori possano leggerli. Meta ha evitato di incoraggiare ci√≤, infatti ha messo quell'avviso chiaro "non inserire info confidenziali" quando usi Meta AI (simile a Google e OpenAI). Probabile che Meta abbia un programma di review come gli altri: una percentuale di chat col bot viene analizzata per capire errori, e utilizzata per RLHF. Da notare: gi√† nel 2019 fu scoperto che Facebook aveva fatto trascrivere conversazioni audio di utenti (consenzienti) a contractor per migliorare l'AI di Portal - quindi non √® nuovo a pratica di data labeling umano. Ora con AI su larga scala, certamente proseguiranno.

**Controlli utente:**  
\- **Notifiche e modulo opt-out EU:** come detto, ogni utente EU sta ricevendo una notifica su FB/IG con spiegazione e link a un **form** per opporsi[\[138\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,messages%2C%20nor%20public%20data%20from)[\[146\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=notifications%20to%20explain%20that%20Meta,). Il form sul centro assistenza chiede di confermare account e preferenze (distingue: vuoi opporti all'uso di post, o di immagini, etc). Meta afferma che chi invia l'opposizione verr√† escluso _"at any time"_ (non c'√® scadenza).  
\- **Privacy Center:** c'√® una sezione "Generative AI Data" in cui si spiega e si d√† il link al modulo. Meta non offre un toggle one-click (essendo su base legale LI, serve che l'utente dichiari opposizione).  
\- **Eliminazione dati:** l'utente pu√≤ eliminare i propri post dal social, impedendo cos√¨ quell'uso. Le conversazioni col bot pu√≤ cancellarle come chat normali. Ma se gi√† usate nel training, restano nel modello (non retroattivo). Almeno l'opposizione dovrebbe farli escludere da futuri training iterativi.  
\- **Controlli per minori:** per default sotto 18 esclusi, quindi nulla da fare.  
\- **Trasparenza e audit:** Meta per ora pubblica solo _AI system cards_ per modelli (Llama2 card dice fonti: 32% data from Meta's own products incl. public FB/IG). Potrebbe in futuro dare opzione "don't use my public info" integrata nell'account.

**Aggiornamenti/regolamentazione:**  
\- **Mid 2023:** Garante irlandese costringe Meta a chiarire base legittimo interesse per ads; Meta annuncia che per AI attender√† linee guida.  
\- **Sep 2024:** Meta riprende training con dati pubblici UK, affermando di avere base legale (implied).  
\- **Oct 2024:** Llama 2 release, addestrato su "data publicly available, plus Meta's own public content".  
\- **Nov 2024:** EDPB opinion in Dec 2024 che d√† ok condizionato.  
\- **Mar 2025:** Meta annuncia in EU limited rollout di Meta AI (non ancora training su user data locale).  
\- **Apr 2025:** Meta inizia notifiche opt-out e avvia training con dati EU[\[147\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=from%20its%20EU%20user%20base,as%20well)[\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly).  
\- **Mag 2025:** segnalazioni che Meta sta facendo re-opt-out alcuni che avevano gi√† optato (c'√® un caso segnalato da un watchdog che dice Meta starebbe chiedendo di rifare il modulo, contestato)[\[148\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=article).  
\- **Future:** Con AI Act EU, se modello generativo usa dati personali, possibili ulteriori obblighi di valutazione d'impatto. Meta cercher√† di essere trasparente per evitare multe come su ads.

### DeepSeek - chatbot AI (case study di privacy rischiosa)

**Identikit e dati raccolti:** **DeepSeek** √® un'app di chatbot AI cinese, emersa nel 2023, disponibile via web e mobile. Ha modelli GPT-simili e funzioni di ricerca. Sul fronte dati, DeepSeek √® notevole perch√© **colleziona quasi tutto** ci√≤ che pu√≤ degli utenti:  
\- **Dati account:** email, telefono, nome utente, password (per creare account)[\[149\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=When%20you%20create%20an%20account%2C,all%20of%20the%20following%20information).  
\- **Input utente (prompt e file):** qualsiasi testo o audio immesso nella chat viene raccolto e memorizzato[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies)[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D). Questo include _intere conversazioni_, file caricati, immagini date in input. La Privacy Policy esplicita: "**we may collect your text or audio input, prompt, uploaded files, feedback, chat history**"[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies). In breve, **l'intera cronologia chat e allegati** √® nei server.  
\- **Output generati:** anche le risposte del bot ("Outputs") possono essere conservate (spesso includono info dell'utente rielaborate, quindi considerate parte dei dati utente).  
\- **Dati di dispositivo e rete:** l'app raccoglie **modello dispositivo, OS, indirizzo IP, lingua, identificatore device, log di sistema, dati di crash e performance**[\[152\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,in%20experience%20and%20for%20security)[\[153\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20with%20all%20digital%20platforms%E2%80%94from,analyze%20how%20you%20use%20our). Inoltre monitora le **azioni** in-app (quali funzioni usi, quante query fai)[\[154\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,We%20will%20obtain). Utilizza cookie e tecnologie simili per tracciare l'uso (sul web e mobile)[\[155\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,see%20our%20Cookies%20Policy). La Wired ha scoperto che carica anche librerie di tracking cinesi (Baidu Tongji analytics, e invia dati base a ByteDance)[\[156\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=A%20WIRED%20review%20of%20the,owner%20ByteDance%20%E2%80%9Cand%20its%20intermediaries).  
\- **Dati di input taciti:** un aspetto inquietante, DeepSeek pu√≤ raccogliere "**keystroke patterns or rhythms**"[\[153\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20with%20all%20digital%20platforms%E2%80%94from,analyze%20how%20you%20use%20our), ovvero la dinamica di digitazione, e altri parametri di comportamento (forse per fingerprinting).  
\- **Dati da terze parti:** se si effettua login con Google/Apple, prende i dati profilo base[\[157\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=%2A%20Log,publicly%20available%20information%20via%20the). Inoltre, dichiara che pu√≤ ricevere dati da "advertisers" su identif. pubblicitari e cookie ID per linkare attivit√† fuori dal servizio[\[158\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20final%20category%20of%20information,%E2%80%9D) (questo suggerisce che integrano maybe librerie marketing).

In pratica, DeepSeek raccoglie non solo i **contenuti** delle comunicazioni, ma anche un quadro **tecnico completo** dell'utente e di come interagisce.

**Finalit√† d'uso:** la Privacy Policy elenca:  
\- Fornire e mantenere il servizio (es. memorizzare la chat per continuit√† conversazionale)[\[159\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=How%20We%20Use%20Your%20Information).  
\- **Addestrare e migliorare modelli**: esplicitamente, "to train and improve our technology, e.g. our ML models"[\[160\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=user%20support.%20,the%20Platform%2C%20such%20as%20by) - compreso monitorare interazioni e analizzare come la gente usa il servizio. Questo passaggio indica chiaramente che DeepSeek utilizza i prompt degli utenti come _dati di addestramento_ per potenziare le sue AI[\[160\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=user%20support.%20,the%20Platform%2C%20such%20as%20by). La policy di Wired conferma: "the company's privacy policy suggests it may harness user prompts in developing new models"[\[161\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=Crucially%2C%20though%2C%20the%20company%E2%80%99s%20privacy,our%20technology%2C%E2%80%9D%20its%20policies%20say).  
\- Comunicazioni con l'utente (email di servizio, ecc.).  
\- Sicurezza e prevenzione frodi: guardano i dati per individuare abusi, spam, hacking[\[162\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Platform%2C%20provide%20customer%20support%20to,to%20protect%20health%20or%20life).  
\- **Conformit√† legale:** forniranno dati alle autorit√† se richiesto e se necessario per proteggere la salute/vita di qualcuno[\[163\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,to%20protect%20health%20or%20life). Quest'ultima √® standard, ma nel contesto cinese significa che i dati possono essere condivisi con le forze dell'ordine locali se ritenuto opportuno.

La base legale varia: per utenti in UE, come da loro tabella, usano _performance contrattuale_ per dati base, _legittimi interessi_ per migliorare il servizio e sicurezza[\[164\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Purpose%20of%20processing%20Personal%20information,DeepSeek%20and%20provide%20user%20support)[\[165\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=such%20as%20our%20machine%20learning,training%20and%20improving%20our%20technology), _consenso_ per cose come cookie e eventuali richieste (anche la creazione account se minorenne 14-18). Per√≤, essendo in Cina, di fatto la compliance GDPR √® sulla carta (non hanno rappresentante in UE noto).

**Uso per training modelli:** **S√¨, by default e senza opt-out**. Tutto ci√≤ che digiti pu√≤ essere usato per addestrare i modelli DeepSeek o affini. Non c'√® indicazione di esclusione se non smettere di usarlo. La policy dice: _"We use your info to train and improve our models"_[\[160\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=user%20support.%20,the%20Platform%2C%20such%20as%20by) e nelle FAQ su Privacy Safe Social di un ricercatore: "They will review, improve, develop the service by monitoring interactions across devices and by training our technology"[\[161\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=Crucially%2C%20though%2C%20the%20company%E2%80%99s%20privacy,our%20technology%2C%E2%80%9D%20its%20policies%20say). Inoltre, non esistono piani business con esclusione: √® un prodotto consumer (anche se vendono API, ma non menzionano escluderle dal training). Quindi presumibile qualsiasi prompt inviato va a un dataset centrale.

**Retention e archiviazione:** DeepSeek dichiara di conservare i dati _finch√© necessari a fornire il servizio e per le altre finalit√†_[\[166\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,Keep%20Your%20Information). Inoltre, li tiene per obblighi legali e interessi legittimi (sviluppo e difesa legale)[\[167\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=We%20retain%20information%20for%20as,or%20defense%20of%20legal%20claims). Non fornisce un tempo fisso (es. X anni), dipende dal tipo: dice _finch√© hai un account manteniamo info account e input_[\[168\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=The%20retention%20periods%20will%20be,necessary%20to%20process%20the%20violation). Quindi, possedendo un account, presumibilmente l'intera chat history viene tenuta a tempo indefinito sul loro cloud (a meno che tu la cancelli manualmente). E se violi i termini, possono conservare i tuoi dati anche dopo per investigazioni[\[169\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=provide%20you%20with%20the%20Services%2C,necessary%20to%20process%20the%20violation). Dopo che uno chiude account, o se decidero che i dati non servono pi√π, li cancelleranno o anonimizzano. Ma non c'√® un vero limite definito.

Una questione critica √® **dove** vanno questi dati: DeepSeek informa apertamente che **"we store the information we collect in secure servers located in the People's Republic of China"**[\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D). Ci√≤ significa che tutti i prompt, file, chat, ecc. finiscono su server in Cina soggetti alle leggi cinesi. Ci√≤ comporta che, in base alla legge cinese sulla cybersicurezza e intelligence (es. National Intelligence Law 2017), le autorit√† possono richiedere accesso a quei dati per motivi di sicurezza nazionale. Dunque un utente USA/EU che pensasse di avere confidenzialit√†, in realt√† i suoi dati possono essere visti dal governo cinese se voluto (e l'utente non lo sapr√†).

**Revisione umana:** DeepSeek sicuramente fa controllare i dati: non fosse altro che per censura. Si √® notato infatti che il bot **censura contenuti critici verso la Cina** (es. non risponde su Tiananmen)[\[171\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20people%20clamor%20to%20test,to%20deflect%20US%20security%20concerns). Ci√≤ implica che i prompt degli utenti passano in un sistema di filtro e potenzialmente segnalazione. Essendo un'azienda cinese, se un utente scrive roba vietata, quell'informazione potrebbe essere isolata e trasmessa a chi di dovere (non confermato, ma plausibile). Sul miglioramento modelli: certamente i tecnici di DeepSeek esamineranno conversazioni utente in massa per rifinire l'AI. Non c'√® menzione di offuscamento PII, e dati spediti in Cina includono tutto, quindi i revisori possono vedere nomi o dati privati.

**Controlli utente:**  
\- **Cancellazione chat:** l'app permette di eliminare la cronologia chat (sull'interfaccia c'√® "Delete all chats" in impostazioni)[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D). Se uno usa quell'opzione, immagino rimuovano la visibilit√† di chat dall'account e dal db primario. Resta dubbio se la tengano in backup. Comunque, l'utente ha almeno quell'opzione per la propria console. Wired l'ha documentato passo passo[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D).  
\- **Cancellazione account:** bisogna scrivere a loro (<privacy@deepseek.com>) per richiedere cancellazione dati, secondo la policy. Non √® chiaro quanto efficacemente rispondano.  
\- **Opt-out training:** non previsto esplicitamente. Nella policy c'√® una frase: "Se ti rifiuti di permetterci di trattare i dati come sopra descritto, puoi fornire feedback via privacy email"[\[172\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=services%20with%20information%20in%20urgent,to%20protect%20health%20or%20life). Quindi l'unico modo √® scrivere e supplicare di non usare i tuoi dati, ma dubito.  
\- **Preferenze tracking:** su web possono chiedere cookie consent. Su mobile non si sa se rispettino i toggles di Android.  
\- **Trasparenza:** minima. A parte la privacy policy, l'utente comune non saprebbe tutti questi dettagli. (La policy c'√® in inglese sul sito per√≤).

**Rischi e valutazioni:**  
DeepSeek appare come **caso limite** di scarsa tutela: i dati finiscono in una giurisdizione con leggi intrusive, vengono ampiamente condivisi nel "gruppo corporativo" (Hangzhou HQ, eventuali affiliate)[\[173\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Our%20Corporate%20Group) e possono essere dati a governi senza resistenze[\[22\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so). La Wired ha paragonato: "probabilmente sta inviando pi√π dati in Cina di quanti TikTok non faccia"[\[174\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=already%20reported%20several%20examples%20of,to%20deflect%20US%20security%20concerns). Quindi molti esperti consigliano di **non inserire informazioni personali o sensibili** in quell'app (cosa valida per tutte le AI, ma qui ancora di pi√π)[\[175\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=the%20ways%20data%20can%20be,information%20to%20AI%20chat%20bots)[\[176\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=any%20sensitive%20or%20personal%20information,to%20AI%20chat%20bots). Servizi come Proton e ExpressVPN hanno pubblicato avvisi evidenziando i punti: tracciamento intenso, condivisione con ByteDance e Baidu, ecc.[\[177\]](https://proton.me/blog/deepseek#:~:text=Using%20DeepSeek%3F%20Here%27s%20why%20your,history%2C%20prompts%2C%20and%20audio%20input)[\[178\]](https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj#:~:text=Is%20DeepSeek%20safe%3F%20What%20happens,chat%20history%2C%20or%20other).

**Aggiornamenti:** - **Feb 2025:** Privacy Policy aggiornata (forse per includere EU specifico e legge cinese PIPL). - **Aug 2023:** Wired pubblica l'articolo-denuncia sui dati verso Cina[\[179\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=What%20DeepSeek%20Collects%20About%20You).  
\- **2024:** possibili ban o restrizioni? Ad es. se DeepSeek diventasse popolare, potrebbe attirare attenzione di governi occidentali (sul modello TikTok). Per ora √® poco noto al grande pubblico.

## Confronto sintetico tra piattaforme LLM principali

Per avere una visione d'insieme, la tabella seguente confronta i vari provider di chatbot/LLM rispetto ad alcuni parametri chiave di gestione dati:

_Tabella 1: Confronto pratiche dati e opzioni privacy dei fornitori LLM (agg. 2025)._  
_Fonti: Policy ufficiali e documentazione dei rispettivi servizi_[_\[180\]_](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20our%20services,content%20to%20train%20our%20models)[_\[181\]_](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies)[_\[107\]_](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20do%20not%20apply,under%20our%20Commercial%20Terms%2C%20including)[_\[15\]_](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important)[_\[135\]_](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[_\[150\]_](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies)_._

| **Vendor (Prodotto)** | **Ambito** | **Dati utente usati per training (default)** | **Opt‚Äëout/Opt‚Äëin** | **Conservazione dati utente** | **Revisione umana** | **Controlli e garanzie** | **Agg. policy** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **OpenAI (ChatGPT)** | Consumer (Web/App) | S√¨ (conversaz. e feedback)[\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out) | Opt‚Äëout disponibile (cronologia off)[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models) | ~Indefinita (se history on); 30 giorni se off[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models) | S√¨, su campioni (pseudonimiz.)[\[56\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=We%20retain%20certain%20data%20from,become%20more%20efficient%20over%20time) | Toggle "Do not train", Privacy Portal per opposizione, Chat effimera[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models)[\[61\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20ChatGPT%2C%20you,used%20to%20train%20our%20models) | Apr 2023 (Italia DPA), Jun 2025 (Enterprise)[\[182\]](https://openai.com/enterprise-privacy/#:~:text=Updated%3A%20June%204%2C%202025) |
| **OpenAI (API / Enterprise)** | Business/Dev | No (per default)[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform) | Opt‚Äëin facoltativo (feedback)[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform) | Log 30 giorni (standard); opzione _zero log_[\[57\]](https://openai.com/index/response-to-nyt-data-demands/#:~:text=How%20we%27re%20responding%20to%20The,Retention%20API%3A%20If%20a)[\[58\]](https://medium.com/@jeffkessie50/openais-zero-data-retention-policy-916ff04a3599#:~:text=OpenAI%27s%20Zero%20Data%20Retention%20Policy,Enterprise%20customers%20can) | No (opt‚Äëout human monitoring) | Data encryption, SOC2; admin controls retention[\[60\]](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law) | Mar 2023 (no-train API)[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform); Aug 2023 (ChatGPT Ent.) |
| **Google (Gemini/Bard)** | Consumer (Account Google) | S√¨ (prompt & conv.)[\[79\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,and%20the%20public) | Opt‚Äëout disponibile (disattiva "Attivit√† Gemini")[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager) | 18 mesi predef. (3-36 mesi configur.)[\[85\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Gemini%20Data%20Collection%20and%20Storage); ~72h se off[\[86\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20stores%20user%20data%20from,Apps%20Activity%20settings%20but%20remains) | S√¨, limitata (0.2% conv.)[\[80\]](https://news.ycombinator.com/item?id=38186828#:~:text=Google%20Bard%20introduces%20,learning%20models) | Toggle in MyActivity (UE: diritto opposizione); elimina cronologia chat[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager) | Jul 2023 (Bard EU); Nov 2025 (Privacy Hub Workspace)[\[93\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Last%20updated%3A%C2%A0November%204%2C%202025) |
| **Google (Workspace AI)** | Business (Gsuite) | No (rimane nel dominio)[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission) | N/A (opt‚Äëin richiesto se share) | Session only (no retention prompt)[\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F); conv. facolt. salvabili 3-36 mesi[\[88\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=The%20Gemini%20app%20enables%20admins,The%20default%20is%2018%20months) | No (nessuna review esterna)[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission) | Admin: abilita/disabilita AI; data in EU boundary[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission)[\[94\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20data%20retention) | Nov 2023 (Workspace Hub)[\[93\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Last%20updated%3A%C2%A0November%204%2C%202025) |
| **Anthropic (Claude)** | Consumer (Free/Pro) | _Dal Q4 2025:_ S√¨ (se utente consente)[\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=) | _Opt‚Äëout possibile_ (toggle "no training")[\[13\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,training%20during%20the%20signup%20process); default = opt‚Äëin (toggle on)[\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy) | 5 anni se training on[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention); 30 giorni se off[\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period) | S√¨ (su dati opt-in) | Privacy Settings per scelta training; cancella chat; no app EU ancora | Ott 2025 (nuovi Termini)[\[184\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Aug%2028%2C%202025%E2%97%8F2%20min%20read) |
| **Anthropic (Claude Ent/API)** | Business/API | No (mai usati x training)[\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI) | N/A (di default esclusi) | 30 giorni log (per abusi) | No  | Contract: data isolati; opzione self-host in cloud privato | Lug 2023 (Claude 2) |
| **Microsoft (365 Copilot)** | Enterprise (M365) | No[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important) | N/A (nessun training est.) | Conservato come dati M365 standard; controllabile via retention policy[\[185\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot)[\[127\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles) | No (opt‚Äëout da monit.)[\[124\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=While%20abuse%20monitoring%2C%20which%20includes,section%20later%20in%20this%20article) | DPA e Product Terms (no data use); EU Data Boundary; user delete history[\[186\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Deleting%20the%20history%20of%20user,interactions%20with%20Microsoft%20365%20Copilot) | Mar 2024 (Prod. Terms, EU boundary) |
| **Meta (AI Assistant)** | Consumer (Facebook, IG) | S√¨ (post/commenti pubblici; chat AI)[\[142\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=regulatory%20pressure%20due%20to%20data,used%20to%20train%20its%20models) | Opt‚Äëout modulo (UE) - diritto opposizione[\[138\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,messages%2C%20nor%20public%20data%20from)[\[146\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=notifications%20to%20explain%20that%20Meta,) | N/D (simile ad altri msg FB); posts finch√© online | S√¨ (per moderazione & RLHF) | Notifiche in-app; form "Generative AI" UE[\[138\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,messages%2C%20nor%20public%20data%20from); promesse: niente dati privati[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,) | Apr 2025 (EU roll-out AI)[\[187\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20to%20start%20training%20its,public%20content%20in%20the%20EU)[\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly) |
| **Meta (business data)** | Enterprise (Workplace) | No (esclusi training) | N/A | Norme interne (customer data isolati) | No  | DPA Meta, sub-processors list; | -   |
| **DeepSeek** | Consumer (Global) | S√¨ (tutto prompt/chat)[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies)[\[161\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=Crucially%2C%20though%2C%20the%20company%E2%80%99s%20privacy,our%20technology%2C%E2%80%9D%20its%20policies%20say) | No opt-out (implicit consent) | Finch√© account attivo (no scadenza definita)[\[168\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=The%20retention%20periods%20will%20be,necessary%20to%20process%20the%20violation) | S√¨ (accesso staff + poss. autorit√† cinesi)[\[22\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so) | Cancella chat manuale[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D); nessuna garanzia su trasferimenti (-> Cina)[\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D) | Feb 2025 (policy PIPL)[\[188\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,2025) |

_(Legenda: "Ambito" distingue se servizio consumer o business; TrainingUseDefault indica se i dati utente alimentano il training del modello per default; Opt-out/in indica se l'utente pu√≤ opporsi o deve acconsentire; Retention √® il periodo tipico di conservazione dei contenuti utente; HumanReview se avviene revisione manuale routine; Controls evidenzia gli strumenti di controllo dati forniti; Agg. policy d√† ultimo aggiornamento significativo)_

**Osservazioni dalla tabella:** OpenAI, Google e Meta nel contesto consumer adottano un approccio simile: **inclusione predefinita** dei dati utente nel ciclo di miglioramento dell'AI, mitigata dalla possibilit√† di opt-out (pi√π o meno nascosta: OpenAI con togglina in UI, Google nell'account, Meta via modulo). Al contrario, i servizi rivolti alle imprese (OpenAI Enterprise/API, Google Workspace, MS Copilot) sono impostati per **non riutilizzare i dati** dei clienti nel training, per rassicurare su privacy e conformit√†: questi di fatto trattano i dati come un cloud privato dell'azienda. Il caso Anthropic spicca per il cambio recente: da full opt-out sono passati a un sistema di _consenso obbligatorio_ (l'utente deve scegliere, con default verso training), il che l'ha allineata a OpenAI/Google (in termini pratici, molti utenti lasceranno attivo e i dati verranno usati). Sul fronte **retention**, vediamo variabilit√†: da log brevissimi (30 gg) per API di OpenAI e ChatGPT con history off, fino a periodi lunghi come 5 anni se l'utente acconsente (Anthropic) o indefiniti (OpenAI con history on). Google d√† controllo granulare ma di base tiene 18 mesi. Microsoft e Workspace mantengono i dati allineati alle normali retention enterprise (quindi potenzialmente per molti anni se l'azienda li archivia). Per **revisione umana**, tutti i consumer (tranne DeepSeek che √® un mondo a parte) indicano un certo grado di intervento umano su dati utente, generalmente _limitato e pseudonimizzato_. Microsoft e Workspace affermano di non farne affatto per i dati cliente. Su **trasparenza e controlli**, Microsoft appare la pi√π restrittiva/protettiva, seguita da OpenAI enterprise; Google e OpenAI consumer forniscono toggles ma richiedono consapevolezza da parte dell'utente; Meta offre il modulo di opt-out in UE ma manca un banale switch in impostazioni (il che potrebbe ridurre le opposizioni per attrito). DeepSeek, infine, rappresenta un _worst-case_: l'utente non ha praticamente voce in capitolo, e i dati finiscono in un contesto di minore tutela giuridica.

## Trasparenza, governance e protezioni: tendenze attuali

**Controlli per l'utente finale:** In risposta alle crescenti preoccupazioni, i provider di AI generativa hanno introdotto strumenti che permettono agli utenti di gestire in parte i propri dati. Ad esempio, quasi tutti ora offrono un modo per **disattivare la conservazione della cronologia** (ChatGPT, Bard/Gemini) o cancellare facilmente le conversazioni (Anthropic, Bing, etc.)[\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models)[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). I servizi includono disclaimer chiari del tipo _"non inserire informazioni sensibili"_, cercando cos√¨ di mitigare il rischio reputazionale. L'utente ha poi diritti esercitabili: in UE, il **diritto di opposizione** (usato da Meta, OpenAI) consente di escludersi dal training globale, mentre il **diritto di accesso/cancellazione** permette di ottenere o eliminare i dati grezzi (OpenAI e Google offrono esportazione e cancellazione account dal portale privacy). Tuttavia, questi controlli hanno limiti: disattivare la cronologia su ChatGPT non impedisce che quell'input sia memorizzato 30gg e potenzialmente analizzato per abuso[\[51\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=You%20can%20opt%20out%20of,used%20to%20train%20our%20models); analogamente, Google avvisa che anche con attivit√† off user√† i prompt per sicurezza[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). In sintesi, l'utente pu√≤ evitare che i propri dati finiscano nel _dataset di addestramento_, ma non pu√≤ impedire del tutto la raccolta e l'analisi temporanea se vuole usare il servizio.

**Strumenti per amministratori e aziende:** Per le organizzazioni, la governance dei dati AI √® diventata cruciale. I fornitori offrono **dashboard amministrative** dove abilitare/disabilitare funzioni AI e impostare policy. Ad esempio, **OpenAI Enterprise** consente di fissare la retention su 0-30 giorni e controllare l'accesso tramite SSO[\[60\]](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law), e aggiunge audit logs per tracciare l'uso interno. **Microsoft** integra Copilot con i suoi strumenti di compliance: un DPO aziendale pu√≤ ricercare tra gli storici Copilot tramite eDiscovery e imporre retention via Purview[\[189\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles). **Google Workspace** analogamente lascia agli admin decidere se salvare conversazioni e per quanto, e garantisce che i connettori con dati aziendali rispettino i permessi gi√† in essere[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission)[\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F). Questi controlli a livello enterprise assicurano che l'adozione di AI non comprometta gli obblighi di privacy e riservatezza che un'azienda ha verso i propri dati: i fornitori si impegnano contrattualmente (con clausole di trattamento dati e allegati sicurezza) a isolare i dati dei clienti e a sottoporvisi ad eventuali **audit** o ispezioni da parte di terzi (ad es. OpenAI dichiara di aver superato audit SOC 2[\[18\]](https://openai.com/enterprise-privacy/#:~:text=Comprehensive%20compliance)). Nei contratti enterprise figurano inoltre specifici **impegni di riservatezza**: Microsoft addirittura indennizza il cliente in caso i loro dati venissero usati per addestrare modelli pubblici contro le policy. In UE, questi accordi costituiscono _Clausole Contrattuali Standard_ per export dati (USA), e molti vendor offrono anche **scelta regione** (OpenAI ha data centers in Europa tramite Azure; Microsoft/EU Data Boundary; Google Cloud region EU) per soddisfare requisiti di **residenza dei dati**.

**Audit e verifiche indipendenti:** A livello di accountability, alcune iniziative emergono:  
\- Le autorit√† privacy (es. il Garante italiano, il CNIL francese, la ICO UK) stanno indagando sulle pratiche di raccolta dati di servizi come ChatGPT. Questi interventi forzano maggiore trasparenza (OpenAI ha dovuto rivelare i tempi di conservazione e basi legali ai garanti). In prospettiva, l'**AI Act UE** imporr√† ai fornitori di modelli generativi obblighi di documentazione sul training data e meccanismi per segnalare e rettificare dati personali nei dataset. Ci√≤ potrebbe introdurre audit esterni periodici.  
\- I vendor stanno producendo **schede di sistema** (System Cards) per modelli AI, dove descrivono in generale le fonti di dati usate e i passi per mitigare rischi[\[190\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=It%E2%80%99s%20important%20to%20note%20that,many%20of%20our%20industry%20counterparts)[\[143\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Last%20year%2C%20we%20delayed%20training,AI%20to%20people%20in%20Europe). Ad esempio, Meta ha pubblicato la card di Llama 2 indicando che ha filtrato i dati per rimuovere PII e che non ha incluso dati di siti privati come e-mail. Sono per√≤ sintesi auto-dichiarate, non verifiche indipendenti.  
\- Per clienti enterprise, in mancanza di normative specifiche, vale la due diligence: molte aziende richiedono ai vendor AI **questionari di sicurezza e privacy** (sul modello delle checklist ISO27001) e clausole severe. Abbiamo gi√† visto casi: ad esempio, Samsung nel 2023 viet√≤ ai dipendenti di usare ChatGPT dopo che del codice interno era trapelato nei prompt - poi valut√≤ di fornire un LLM interno o usare API con logging disabilitato. Questo porta i provider a offrire soluzioni dedicate (OpenAI ad esempio lavora su istanze GPT private per aziende sensibili).

**Impegni contrattuali e normativi:**  
OpenAI, Google, Microsoft e Anthropic hanno pubblicato termini aggiornati e spesso segmentati per tipo di utente. Nel caso di OpenAI, ad esempio, i **Terms of Use consumer** specificano che l'utente concede a OpenAI una licenza per usare i contenuti inviati per migliorare i servizi[\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out), ma ha strumenti per revocarla (opt-out). Nei **contratti enterprise**, invece, OpenAI afferma chiaramente di non avere diritti sui dati se non quelli limitati a fornire il servizio e adempimenti di legge[\[191\]](https://openai.com/enterprise-privacy/#:~:text=Who%20owns%20inputs%20and%20outputs%3F). Google ha incluso clausole nelle **Condizioni di Workspace** che i dati del cliente non verranno usati per altri scopi senza consenso (le privacy commitments aggiornate al 2023). Microsoft nel suo **Data Protection Addendum** ha una sezione generative AI che afferma la segregazione dei dati e la conformit√† al GDPR. Meta, non vendendo direttamente l'AI come servizio separato, rientra nei Termini generali di Facebook/Instagram, dove l'uso dei dati per AI rientra in "migliorare i servizi" - integrato dalle informative e moduli aggiunti per l'UE.

**Differenze geografiche:** Va notato che molti controlli sono stati lanciati in risposta a regolamentazioni o pressioni in specifiche aree: l'Europa ha spinto per il consenso/opt-out (Italia vs ChatGPT, UE vs Meta), mentre negli USA il focus √® minore (l√¨ ad es. ChatGPT non offre toggle training, √® attivo per default e l'utente deve scoprirlo nelle impostazioni; idem Bard). Ci√≤ crea un potenziale scenario frammentato: ad esempio, Meta consente opt-out solo agli europei per ora[\[144\]](https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/#:~:text=Reddit%20www,not%20be%20available%20to%20you); OpenAI stessa scrive che i diritti privacy variano a seconda delle leggi locali. Aziende come **DeepSeek** evidenziano un altro rischio: giurisdizioni con leggi meno tutelanti possono essere base per servizi globali - qui la governance dipende dalla consapevolezza utente e da eventuali blocchi (ExpressVPN consiglia di non usarlo, ma non c'√® un Garante che possa intervenire facilmente data la sede extra-UE).

**Sicurezza dei dati:** Oltre alla privacy, i provider mettono enfasi sulla **sicurezza**: criptaggio, controlli accesso, audit logging interno. OpenAI ad esempio ha implementato crittografia a riposo e monitoraggio degli accessi ai dati di conversazione (dicono che solo un numero limitato di personale pu√≤ accedere e ogni accesso √® registrato). Microsoft e Google avendo gi√† infrastrutture cloud mature, estendono quelle misure ai sottosistemi AI (nel doc Microsoft: "data is encrypted at rest (AES-256) and in transit (TLS 1.2+)"[\[192\]](https://openai.com/enterprise-privacy/#:~:text=with%20industry%20standards%20for%20security,and%20confidentiality)). Inoltre c'√® il tema **data leakage**: i modelli potrebbero rigenerare testi presi dai prompt di altri utenti (come successo a Samsung via ChatGPT). I provider lavorano su tecniche per mitigare (OpenAI riduce la probabilit√† che dati sensibili rimangano nei pesi, e ha threshold su lunghezze per non memorizzare numeri di carta, ecc.). Questo rientra nella governance tecnica.

**Conclusione:** Le logiche di profilazione e sfruttamento dati viste in Cambridge Analytica - raccolta massiva, elaborazione machine learning, uso sperimentale - oggi rivivono potenziate nell'ambito dell'AI generativa. La differenza chiave √® che nel caso CA l'obiettivo era influenzare gli utenti stessi (targeting comportamentale esterno), mentre nel caso dei chatbot l'obiettivo √® "influenzare" l'**intelligenza artificiale** addestrandola sui dati utente. In entrambi i casi, per√≤, c'√® uno **sbilanciamento informativo**: gli utenti forniscono inconsapevolmente dati (like, conversazioni) ignari di come verranno sfruttati a monte. Serve quindi una forte enfasi su **trasparenza** (chiare informative come quelle introdotte post-sanzioni) e su meccanismi di **governance** che diano controllo alle persone e alle organizzazioni sui propri dati. I passi compiuti finora - toggle, moduli, contratti - sono incoraggianti, ma richiedono vigilanza continua. L'esperienza Cambridge Analytica ha insegnato che usare i dati personali senza adeguati freni pu√≤ avere impatti sociali enormi; oggi, con i chatbot, in gioco c'√® anche la fiducia verso queste nuove tecnologie: garantire che rispettino la privacy e la volont√† degli utenti √® fondamentale per un'adozione responsabile e sostenibile dell'AI nella societ√†.

## Bibliografia annotata

- **ICO (2018)** - _"Investigation into the use of data analytics in political campaigns - Update"_, **Information Commissioner's Office (UK)**. Rapporto ufficiale dell'Authority UK che indaga il caso Cambridge Analytica[\[1\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=The%20total%20number%20of%20users,this%20with%20other%20sources%20of)[\[31\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=Facebook%20to%20be%20approximately%2087,with%20personalised%20advertising%20during%20the). Fonte primaria per numeri (87 milioni coinvolti) e descrizione dell'app GSR (dati estratti, OCEAN, condivisione con SCL)[\[23\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=%E2%80%A2%20Public%20profile%20data%2C%20including,to%20be%20approximately%2087%20million)[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the). Rilevante perch√© conferma da audit forense la pipeline tecnica e le violazioni di legge.
- **UK Parliament DCMS Committee (2019)** - _"Disinformation and 'fake news': Final Report"_, **House of Commons**[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter)[\[193\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=the%20company%20claimed%20that%20there,turnout%2C%20for%20the%20targeted%20groups). Inchiesta parlamentare sulle interferenze digitali. Contiene testimonianze di Wylie, Kogan, Nix e risultati (efficacia dichiarata: +30% turnout)[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter). Utile per comprendere retroscena e contraddizioni nelle versioni fornite dai protagonisti (es. Nix neg√≤ l'uso dei dati GSR, poi ritratt√≤[\[24\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=108,which%20he)[\[194\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=109,of%20events%2C%20in%20February%202018)) e per metriche di successo vantate da SCL[\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter).
- **Hindman (2018)** - _"This is how Cambridge Analytica's Facebook targeting model really worked"_, **Nieman Lab**[\[195\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=The%20researcher%20whose%20work%20is,Netflix%20uses%20to%20recommend%20movies)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed). Articolo di analisi che, grazie a informazioni dirette di Kogan, spiega il funzionamento statistico del modello CA (approccio tipo Netflix Prize con SVD)[\[195\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=The%20researcher%20whose%20work%20is,Netflix%20uses%20to%20recommend%20movies)[\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed). Evidenzia che le "psicografiche" erano in realt√† correlate a demografia, sgonfiando la narrativa mistica. Fonte secondaria ma autorevole e basata su fonte diretta.
- **Kosinski et al. (2015)** - _"Facebook as a research tool for the social sciences"_, **Cambridge University Press** (riassunto citato in DCMS Interim)[\[196\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=2014%2C%20where%20he%20re,to%20this%20approach%2C%20stating%20that)[\[197\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=orientation%E2%80%9D,to%20this%20approach%2C%20stating%20that). Articolo accademico cardine che dimostr√≤ come i "Like" predicessero personalit√† e altri tratti sensibilissimi (orientamento sessuale, idee politiche)[\[196\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=2014%2C%20where%20he%20re,to%20this%20approach%2C%20stating%20that). Importante per capire le basi psicometriche sfruttate poi da CA e per evidenziare i rischi etici gi√† segnalati allora[\[197\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=orientation%E2%80%9D,to%20this%20approach%2C%20stating%20that).
- **OpenAI Help Center (2025)** - _"How your data is used to improve model performance"_, **OpenAI**[\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out)[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform). Spiega la policy attuale di OpenAI sui dati: distinzione netta consumer vs API[\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform), possibilit√† di opt-out e portal privacy. Fonte primaria aggiornata (novembre 2025) con language chiaro sulle finalit√† e sui controlli offerti agli utenti.
- **OpenAI (2025)** - _"Enterprise privacy at OpenAI"_, **OpenAI Blog**[\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data)[\[198\]](https://openai.com/enterprise-privacy/#:~:text=Does%20OpenAI%20train%20its%20models,on%20my%20business%20data). Descrive gli impegni per clienti business: niente training su dati business, controllo retention, possesso di input/output dall'utente[\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data)[\[198\]](https://openai.com/enterprise-privacy/#:~:text=Does%20OpenAI%20train%20its%20models,on%20my%20business%20data). Rilevante per evidenziare come vendono la sicurezza dei dati enterprise e quali garanzie contrattuali (SOC 2, encryption) forniscono.
- **Google Support (2025)** - _"Gemini Apps Privacy Hub"_, **Google**[\[181\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies)[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). Documento comprensivo che elenca categorie di dati raccolti (prompt, file, device info)[\[67\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Apps)[\[68\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,settings%2C%20device%20type%20and%20settings) e spiega uso (improvement modelli, revisori umani)[\[181\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies) e controlli (MyActivity per disattivare retention)[\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager). Fonte primaria cruciale per le policy Google generative AI, con focus utente consumer.
- **Google Workspace Privacy (2025)** - _"Generative AI in Workspace Privacy Hub"_, **Google**[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission)[\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F). Specifica per contesto enterprise: afferma nero su bianco il principio "i contenuti rimangono nell'organizzazione, non usati per training esterno"[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission). Include tabella retention differenziata (session ephemeral vs conversazioni facoltative 3-36 mesi)[\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F). Serve per mostrare l'approccio pi√π cauto in ambito aziendale.
- **Anthropic Blog (2025)** - _"Updates to our Consumer Terms and Privacy Policy"_, **Anthropic**[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention)[\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=). Comunicato ufficiale del cambio di policy: rivela il passaggio a opt-in e l'estensione retention a 5 anni per dati condivisi[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention), nonch√© la non applicabilit√† ai clienti enterprise[\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI). Fonte diretta per comprendere la strategia di Anthropic di bilanciamento fra miglioramento modelli e feedback utenti.
- **Wired (2025)** - _"Anthropic Will Use Claude Chats for Training Data. Here's How to Opt Out"_, **WIRED**[\[98\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models)[\[199\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=The%20new%20privacy%20policy%20also,under%20the%2030%20day%20policy). Articolo divulgativo che conferma e commenta la mossa di Anthropic, notando che di default gli utenti sono inclusi (toggle preattivato)[\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy) e paragonando alle impostazioni di OpenAI e Google[\[97\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Prior%20to%20Anthropic%20updating%20its,user%20chooses%20to%20opt%20out). Utile per la prospettiva critica e per estrarre info come la tempistica (ritardo al 8 ottobre)[\[100\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20developers%20hope%20to%20make,of%20their%20chatbot%20over%20time).
- **Microsoft Learn (2023)** - _"Data, Privacy, and Security for Microsoft 365 Copilot"_, **Microsoft Docs**[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important)[\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot). Documentazione tecnica che dettaglia il funzionamento di Copilot: chiarisce che prompt e dati Graph non addestrano gli LLM[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important), e spiega come i dati sono trattati internamente (Copilot activity history)[\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot). Fonte primaria essenziale per illustrare il "no data mingling" di Microsoft e le opzioni di retention via Purview[\[185\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot).
- **TechCrunch (2025)** - _"Meta to start training its AI models on public content in the EU"_, **TechCrunch**[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly). Notizia che riporta l'annuncio di Meta (e.g. citazioni: notifiche agli utenti, modulo opt-out, esclusione di privati e minori)[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[200\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,and%20the%20distinct%20ways%20different). Serve come supporto giornalistico per consolidare le info sulle policy Meta, basandosi anche sul blog ufficiale Meta (linkato nell'articolo)[\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes).
- **Meta Newsroom (2025)** - _"Making AI Work Harder for Europeans"_, **Meta**[\[201\]\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes). Comunicazione ufficiale di Meta che delinea il piano UE: conferma uso di post/commenti pubblici per training[\[202\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Today%2C%20we%E2%80%99re%20announcing%20our%20plans,their%20cultures%2C%20languages%20and%20history)[\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes), rispetto di decisione EDPB, e reitera che niente messaggi privati verr√† toccato[\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes). Fonte primaria di Meta, importante per citare testualmente gli impegni (es. _"non usiamo i messaggi privati per i nostri modelli generativi"_).
- **PrivacyGuides Forum (2025)** - Discussione _"Meta AI training in EU"_ citante TechCrunch[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,)[\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models). Evidenzia estratti chiave dell'articolo TC (ad esempio la frase "Meta notes it doesn't use private messages or under-18 public data for training"[\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models)). Utile come check incrociato e per la dimensione comunit√† privacy.
- **DeepSeek Privacy Policy (2025)** - **DeepSeek Inc.**[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies)[\[168\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=The%20retention%20periods%20will%20be,necessary%20to%20process%20the%20violation). Testo integrale policy (EN) con dettagli sorprendenti: afferma esplicitamente la raccolta di prompt, file, chat history[\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies) e l'archiviazione su server in Cina[\[203\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Where%20We%20Store%20Your%20Information). Fornisce la base per affermazioni su cosa raccoglie e dove vanno i dati, indispensabile per la sezione DeepSeek.
- **Wired (2023)** - _"DeepSeek's Popular AI App Is Explicitly Sending US Data to China"_, **WIRED**[\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D)[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D). Inchiesta giornalistica che analizza la privacy policy e il traffico di DeepSeek: conferma che i server sono in Cina[\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D), e avvisa che chat e file utente vengono inviati integralmente[\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D). Anche rivela tracking verso Baidu e ByteDance[\[156\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=A%20WIRED%20review%20of%20the,owner%20ByteDance%20%E2%80%9Cand%20its%20intermediaries). Fonte di terza parte attendibile che d√† autorevolezza alle preoccupazioni su DeepSeek.
- **Proton (2023)** - Blog _"Using DeepSeek? Here's why your privacy is at stake"_, **Proton**. Articolo divulgativo che sintetizza i rischi di DeepSeek, menzionando punti simili a Wired (raccolta prompt e chat, niente opt-out). Non citato direttamente nel testo, ma influente per completare il quadro e verifica incrociata.
- **ExpressVPN (2023)** - _"Is DeepSeek safe? What happens to your data"_, **ExpressVPN Blog**[\[178\]](https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj#:~:text=Is%20DeepSeek%20safe%3F%20What%20happens,chat%20history%2C%20or%20other). Altro commentario che riprende i punti noti (dati inviati in Cina, ecc.) e consiglia di non usare app di dubbia provenienza per query sensibili. Serve come contesto e per mostrare che la comunit√† sicurezza prende sul serio questi aspetti.
- **Reddit GDPR (2023)** - Discussione _"DeepSeek privacy practices"_[\[204\]](https://www.reddit.com/r/gdpr/comments/1lk4baw/what_are_deepseeks_privacy_practices/#:~:text=DeepSeek%20collects%20three%20main%20categories,uploaded). Apporta conferma che DeepSeek raccoglie 3 categorie: dati account, chat ("prompts") e file, e che tutto √® soggetto a quell'informativa. Rileva come quell'approccio violi potenzialmente GDPR. Non citato nel testo finale, ma utilizzato in analisi per consolidare affermazioni.

[\[1\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=The%20total%20number%20of%20users,this%20with%20other%20sources%20of) [\[23\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=%E2%80%A2%20Public%20profile%20data%2C%20including,to%20be%20approximately%2087%20million) [\[26\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=In%20summary%2C%20the%20app%20accessed,implemented%20on%20their%20Facebook%20profile) [\[27\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=20%20%E2%80%A2%20Posts%20on%20the,again%2C%20subject) [\[28\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=to%20the%20settings%20they%20had,the%20app%2C%20was%20estimated%20by) [\[30\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=%E2%80%A2%20Photographs%20in%20which%20the,psychological%20patterns%20and%20build%20models) [\[31\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=Facebook%20to%20be%20approximately%2087,with%20personalised%20advertising%20during%20the) [\[32\]](https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf#:~:text=OCEAN%20model%20and%20pioneer%20the,as%20ethnicity%20and%20political%20affiliation) ico.org.uk

<https://ico.org.uk/media2/migrated/2259371/investigation-into-data-analytics-for-political-purposes-update.pdf>

[\[2\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=personality%20and%20other%20drivers%20that,second%20amendment%2C%20which%20guarantees%20the) [\[3\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=%E2%80%98Openness%E2%80%99%2C%20%E2%80%98Conscientiousness%E2%80%99%2C%20%E2%80%98Extraversion%E2%80%99%2C%20%E2%80%98Agreeableness%E2%80%99%20and,39) [\[4\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,turnout%2C%20for%20the%20targeted%20groups) [\[24\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=108,which%20he) [\[25\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%20breach%20originated%20at%20the,%E2%80%9D%2046) [\[29\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=103,The%20aim) [\[33\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Cambridge%20Psychometrics%20Centre%2C%20Michal%20Kosinski%2C,to%20this%20approach%2C%20stating%20that) [\[34\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=Nix%20told%20us%3A%20%E2%80%9CWe%20do,41) [\[35\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=We%20are%20trying%20to%20make,109) [\[36\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,be%20relevant%20to%20understanding%20their) [\[38\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=3%20The%20issue%20of%20data,GSR%20and%20Cambridge%20Analytica%20allegations) [\[39\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=data%2C%20involving%20various%20organisations%20including,overseas%20elections%20in%20Chapter%206) [\[40\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=order%20to%20match%20the%20right,might%20support%20and%20how%20to) [\[41\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=required%20under%20the%20contract%20to,electoral%20register%20in%20those%20states) [\[42\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=sharing%20of%20data%20in%20the,overseas%20elections%20in%20Chapter%206) [\[43\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=micro,uplift%20in%20voter) [\[44\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=92,Some%20of%20the) [\[46\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=100,43) [\[47\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=101,119) [\[49\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=conflicted%20with%20Mr%20Nix%E2%80%99s%20evidence%3B,as%20they%20were%20never%20enforced) [\[193\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=the%20company%20claimed%20that%20there,turnout%2C%20for%20the%20targeted%20groups) [\[194\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=109,of%20events%2C%20in%20February%202018) [\[196\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=2014%2C%20where%20he%20re,to%20this%20approach%2C%20stating%20that) [\[197\]](https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm#:~:text=orientation%E2%80%9D,to%20this%20approach%2C%20stating%20that) Disinformation and 'fake news': Interim Report - Digital, Culture, Media and Sport Committee - House of Commons

<https://publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/363/36306.htm>

[\[5\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=In%20an%20email%20to%20me%2C,like%20race%2C%20age%2C%20and%20gender) [\[6\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=Regarding%20one%20key%20public%20concern%2C,quite%20as%20it%20was%20billed) [\[37\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=movies%20medium) [\[195\]](https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/#:~:text=The%20researcher%20whose%20work%20is,Netflix%20uses%20to%20recommend%20movies) This is how Cambridge Analytica's Facebook targeting model really worked - according to the person who built it | Nieman Journalism Lab

<https://www.niemanlab.org/2018/03/this-is-how-cambridge-analyticas-facebook-targeting-model-really-worked-according-to-the-person-who-built-it/>

[\[7\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=improve%20over%20time,it%2C%20unless%20you%20opt%20out) [\[9\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models) [\[11\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=share%20your%20content%20with%20us%2C,it%2C%20unless%20you%20opt%20out) [\[14\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Services%20for%20businesses%2C%20such%20as,Enterprise%2C%20and%20our%20API%20Platform) [\[50\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20our%20services,content%20to%20train%20our%20models) [\[51\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=You%20can%20opt%20out%20of,used%20to%20train%20our%20models) [\[52\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=may%20use%20your%20content%20to,train%20our%20models) [\[53\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=capabilities%20and%20safety,it%2C%20unless%20you%20opt%20out) [\[55\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=Even%20if%20you%E2%80%99ve%20opted%20out,used%20to%20train%20our%20models) [\[56\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=We%20retain%20certain%20data%20from,become%20more%20efficient%20over%20time) [\[61\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20ChatGPT%2C%20you,used%20to%20train%20our%20models) [\[180\]](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance#:~:text=When%20you%20use%20our%20services,content%20to%20train%20our%20models) How your data is used to improve model performance | OpenAI Help Center

<https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance>

[\[8\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,Google%20to%20use%20to%20improve) [\[10\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Manage%20Gems%20in%20Gem%20Manager) [\[67\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,Apps) [\[68\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,settings%2C%20device%20type%20and%20settings) [\[69\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,or%20%E2%80%9CSaved%20Info%E2%80%9Din%20some%20locales) [\[70\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=public%20links%20%2C%20citations%2C%20chat,metrics%2C%20crash%20and%20debug%20information) [\[71\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,collected%20through%20supplemental%20Gemini%20Apps) [\[72\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,with%20public%20Gemini%20Apps%20content) [\[73\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,to%20Gemini%2C%20subscription%20related%20information) [\[74\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,or%20%E2%80%9CSaved%20Info%E2%80%9Din%20some%20locales) [\[75\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=use%20Gemini%20overlay%20to%20ask,co%2Fprivacypolicy%2Flocation) [\[76\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Audio%20Features) [\[77\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=public%20links%20%2C%20citations%2C%20chat,such%20as) [\[78\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Google%20uses%20this%20data%2C%20as,in%20our%20Privacy%20Policy%2C%20to) [\[79\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=,and%20the%20public) [\[89\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=You%20can%20visit%20your%20Google,your%20data%20and%20experience%2C%20like) [\[90\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=If%20you%20change%20your%20Gemini,you%20use%20other%20Google%20services) [\[181\]](https://support.google.com/gemini/answer/13594961?hl=en#:~:text=Human%20reviewers%20,learning%20technologies) Gemini Apps Privacy Hub - Gemini Apps Help

<https://support.google.com/gemini/answer/13594961?hl=en>

[\[12\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Extended%20data%20retention) [\[13\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,training%20during%20the%20signup%20process) [\[101\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Today%2C%20we%27re%20rolling%20out%20updates,be%20done%20at%20any%20time) [\[102\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=You%E2%80%99re%20always%20in%20control%20of,window%20like%20the%20one%20below) [\[104\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=) [\[106\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20apply%20to%20users,and%20Google%20Cloud%E2%80%99s%20Vertex%20AI) [\[107\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=These%20updates%20do%20not%20apply,under%20our%20Commercial%20Terms%2C%20including) [\[108\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=,the%20data%20retention%20period) [\[109\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=resumed%20chats%20and%20coding%20sessions%2C,day%20data%20retention%20period) [\[111\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=The%20extended%20retention%20period%20also,keep%20Claude%20safe%20for%20everyone) [\[112\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=for%20existing%20Claude%20app%20users) [\[113\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=the%20signup%20process.%20,five%20years%20if%20you%20allow) [\[114\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=If%20you%20change%20your%20setting,our%20data%20retention%20practices%20here) [\[115\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=You%20can%20always%20update%20your,Your%20data%20will%20still%20be) [\[116\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Starting%20today%2C%20we%E2%80%99re%20rolling%20out,Privacy%20Settings%20at%20any%20time) [\[184\]](https://www.anthropic.com/news/updates-to-our-consumer-terms#:~:text=Aug%2028%2C%202025%E2%97%8F2%20min%20read) Updates to Consumer Terms and Privacy Policy \\ Anthropic

<https://www.anthropic.com/news/updates-to-our-consumer-terms>

[\[15\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Important) [\[63\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Note) [\[118\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=data,accurate%2C%20relevant%2C%20and%20contextual%20responses) [\[119\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20only%20surfaces,you%20give%20to%20users%20outside) [\[120\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20you%20enter%20prompts%20using,Data%20stored%20about%20user%20interactions) [\[121\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Data%20stored%20about%20user%20interactions,with%20Microsoft%20365%20Copilot) [\[122\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20a%20user%20interacts%20with,in%20Microsoft%20365%20Copilot%20Chat) [\[123\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot) [\[124\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=While%20abuse%20monitoring%2C%20which%20includes,section%20later%20in%20this%20article) [\[125\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20the,EU%20Data%20Boundary) [\[126\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20and%20data,residency) [\[127\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles) [\[128\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=For%20Microsoft%20Teams%20chats%20with,to%20view%20the%20stored%20data) [\[129\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=LLMs%2C%20including%20those%20used%20by,Microsoft%20365%20Copilot) [\[130\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Deleting%20the%20history%20of%20user,interactions%20with%20Microsoft%20365%20Copilot) [\[131\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Microsoft%20365%20Copilot%20only%20surfaces,shared%20channels%20in%20Microsoft%20Teams) [\[132\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=When%20a%20user%20interacts%20with,24%20meetings%20in%20Microsoft) [\[185\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Teams%20support,used%20by%20Microsoft%20365%20Copilot) [\[186\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=Deleting%20the%20history%20of%20user,interactions%20with%20Microsoft%20365%20Copilot) [\[189\]](https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy#:~:text=To%20view%20and%20manage%20this,information%2C%20see%20the%20following%20articles) Data, Privacy, and Security for Microsoft 365 Copilot | Microsoft Learn

<https://learn.microsoft.com/en-us/copilot/microsoft-365/microsoft-365-copilot-privacy>

[\[16\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Beginning%20this%20week%2C%20people%20based,well%20as%20newly%20submitted%20ones) [\[17\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes) [\[143\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Last%20year%2C%20we%20delayed%20training,AI%20to%20people%20in%20Europe) [\[190\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=It%E2%80%99s%20important%20to%20note%20that,many%20of%20our%20industry%20counterparts) [\[201\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=As%20we%E2%80%99ve%20previously%20mentioned%2C%20we,being%20used%20for%20training%20purposes) [\[202\]](https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/#:~:text=Today%2C%20we%E2%80%99re%20announcing%20our%20plans,their%20cultures%2C%20languages%20and%20history) Making AI Work Harder for Europeans

<https://about.fb.com/news/2025/04/making-ai-work-harder-for-europeans/>

[\[18\]](https://openai.com/enterprise-privacy/#:~:text=Comprehensive%20compliance) [\[54\]](https://openai.com/enterprise-privacy/#:~:text=You%20own%20and%20control%20your,data) [\[60\]](https://openai.com/enterprise-privacy/#:~:text=,where%20allowed%20by%20law) [\[62\]](https://openai.com/enterprise-privacy/#:~:text=%2A%20Enterprise,ChatGPT%20Enterprise%20and%20API) [\[64\]](https://openai.com/enterprise-privacy/#:~:text=Our%20commitments%20provide%20you%20with,support%20for%20your%20compliance%20needs) [\[65\]](https://openai.com/enterprise-privacy/#:~:text=more%20about%20GPTs%20%E2%81%A0) [\[66\]](https://openai.com/enterprise-privacy/#:~:text=%2A%20Fine,and%20available%20features) [\[182\]](https://openai.com/enterprise-privacy/#:~:text=Updated%3A%20June%204%2C%202025) [\[191\]](https://openai.com/enterprise-privacy/#:~:text=Who%20owns%20inputs%20and%20outputs%3F) [\[192\]](https://openai.com/enterprise-privacy/#:~:text=with%20industry%20standards%20for%20security,and%20confidentiality) [\[198\]](https://openai.com/enterprise-privacy/#:~:text=Does%20OpenAI%20train%20its%20models,on%20my%20business%20data) Enterprise privacy at OpenAI | OpenAI

<https://openai.com/enterprise-privacy/>

[\[19\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission) [\[83\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=,outside%20your%20domain%20without%20permission) [\[84\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20DOES%20NOT%20Gemini%20DOES,your%20existing%20data%20protection%20controls) [\[87\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20in%20Workspace) [\[88\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=The%20Gemini%20app%20enables%20admins,The%20default%20is%2018%20months) [\[91\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Prompts%20and%20responses) [\[92\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Inserted%20or%20generated%20content%20in,Docs%2C%20Gmail%2C%20etc) [\[93\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Last%20updated%3A%C2%A0November%204%2C%202025) [\[94\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=Gemini%20data%20retention) [\[183\]](https://support.google.com/a/answer/15706919?hl=en#:~:text=How%20long%20are%20prompts%20saved%3F) Generative AI in Google Workspace Privacy Hub - Google Workspace Admin Help

<https://support.google.com/a/answer/15706919?hl=en>

[\[20\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D) [\[21\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D) [\[22\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=DeepSeek%E2%80%99s%20privacy%20policy%20also%20says,is%20required%20to%20do%20so) [\[151\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20first%20of%20these%20areas,%E2%80%9D) [\[153\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20with%20all%20digital%20platforms%E2%80%94from,analyze%20how%20you%20use%20our) [\[156\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=A%20WIRED%20review%20of%20the,owner%20ByteDance%20%E2%80%9Cand%20its%20intermediaries) [\[158\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=The%20final%20category%20of%20information,%E2%80%9D) [\[161\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=Crucially%2C%20though%2C%20the%20company%E2%80%99s%20privacy,our%20technology%2C%E2%80%9D%20its%20policies%20say) [\[170\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=To%20be%20clear%2C%20DeepSeek%20is,%E2%80%9D) [\[171\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=As%20people%20clamor%20to%20test,to%20deflect%20US%20security%20concerns) [\[174\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=already%20reported%20several%20examples%20of,to%20deflect%20US%20security%20concerns) [\[175\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=the%20ways%20data%20can%20be,information%20to%20AI%20chat%20bots) [\[176\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=any%20sensitive%20or%20personal%20information,to%20AI%20chat%20bots) [\[179\]](https://www.wired.com/story/deepseek-ai-china-privacy-data/#:~:text=What%20DeepSeek%20Collects%20About%20You) DeepSeek's Popular AI App Is Explicitly Sending US Data to China | WIRED

<https://www.wired.com/story/deepseek-ai-china-privacy-data/>

[\[45\]](https://www.judiciary.senate.gov/imo/media/doc/Professor%20Emma%20L.%20Briant%20Report%20on%20Cambrige%20Analytica.pdf#:~:text=,really%20championed%20by%20Obama%27s) \[PDF\] Evidence for the US Senate Judiciary Committee on Cambridge ...

<https://www.judiciary.senate.gov/imo/media/doc/Professor%20Emma%20L.%20Briant%20Report%20on%20Cambrige%20Analytica.pdf>

[\[48\]](https://now.tufts.edu/2018/05/17/did-cambridge-analytica-sway-election#:~:text=Tufts%20political%20scientist%20Eitan%20Hersh,a%20Senate%20Judiciary%20Committee%20hearing) Did Cambridge Analytica Sway the Election? - Tufts Now

<https://now.tufts.edu/2018/05/17/did-cambridge-analytica-sway-election>

[\[57\]](https://openai.com/index/response-to-nyt-data-demands/#:~:text=How%20we%27re%20responding%20to%20The,Retention%20API%3A%20If%20a) How we're responding to The New York Times' data ... - OpenAI

<https://openai.com/index/response-to-nyt-data-demands/>

[\[58\]](https://medium.com/@jeffkessie50/openais-zero-data-retention-policy-916ff04a3599#:~:text=OpenAI%27s%20Zero%20Data%20Retention%20Policy,Enterprise%20customers%20can) OpenAI's Zero Data Retention Policy | by J Kes | Sep, 2025 - Medium

<https://medium.com/@jeffkessie50/openais-zero-data-retention-policy-916ff04a3599>

[\[59\]](https://help.openai.com/en/articles/8983130-what-if-i-want-to-keep-my-history-on-but-disable-model-training#:~:text=the%20model%20for%20everyone%20,you%20opt%20out%2C%20new) What if I want to keep my history on but disable model training?

<https://help.openai.com/en/articles/8983130-what-if-i-want-to-keep-my-history-on-but-disable-model-training>

[\[80\]](https://news.ycombinator.com/item?id=38186828#:~:text=Google%20Bard%20introduces%20,learning%20models) Google Bard introduces "Human reviewers," sparking privacy ...

<https://news.ycombinator.com/item?id=38186828>

[\[81\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Human%20Reviews%20of%20User%20Gemini,Data) [\[82\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20retains%20user%20data%20reviewed,learning%20models%2C%20enhancing) [\[85\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Gemini%20Data%20Collection%20and%20Storage) [\[86\]](https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini#:~:text=Google%20stores%20user%20data%20from,Apps%20Activity%20settings%20but%20remains) Google Gemini - Is your Data Safe?

<https://heydata.eu/en/magazine/is-your-data-safe-with-google-gemini>

[\[95\]](https://services.google.com/fh/files/misc/genai_privacy_google_cloud_202308.pdf#:~:text=,customer%27s%20prior%20permission%20or) \[PDF\] Generative AI, Privacy, and Google Cloud

<https://services.google.com/fh/files/misc/genai_privacy_google_cloud_202308.pdf>

[\[96\]](https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training#:~:text=Center%20privacy,If) Is my data used for model training? - Anthropic Privacy Center

<https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training>

[\[97\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Prior%20to%20Anthropic%20updating%20its,user%20chooses%20to%20opt%20out) [\[98\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models) [\[99\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Previously%2C%20the%20company%20did%20not,to%20train%20future%20Anthropic%20models) [\[100\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20developers%20hope%20to%20make,of%20their%20chatbot%20over%20time) [\[103\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=%E2%80%9CAllow%20the%20use%20of%20your,into%20the%20new%20training%20policy) [\[105\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=Anthropic%E2%80%99s%20new%20models) [\[199\]](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/#:~:text=The%20new%20privacy%20policy%20also,under%20the%2030%20day%20policy) Anthropic Will Use Claude Chats for Training Data. Here's How to Opt Out | WIRED

<https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/>

[\[110\]](https://www.reddit.com/r/ClaudeAI/comments/1nd73si/anthropics_new_privacy_policy_is_systematically/#:~:text=Anthropic%27s%20New%20Privacy%20Policy%20is,Claude%20can%20potentially%20access) Anthropic's New Privacy Policy is Systematically Screwing Over Solo ...

<https://www.reddit.com/r/ClaudeAI/comments/1nd73si/anthropics_new_privacy_policy_is_systematically/>

[\[117\]](https://amstlegal.com/anthropics-claude-ai-updated-terms-explained/#:~:text=Terms,changes%20%26%20protect%20your%20business) Anthropic's Claude AI Updates - Impact on Privacy & Confidentiality

<https://amstlegal.com/anthropics-claude-ai-updated-terms-explained/>

[\[133\]](https://learn.microsoft.com/en-us/answers/questions/5298757/does-microsofts-copilot-pro-for-office-365-use-you#:~:text=,LLMs%20to%20your%20organizational) Does Microsoft's Copilot Pro for Office 365 use your data to train it's ...

<https://learn.microsoft.com/en-us/answers/questions/5298757/does-microsofts-copilot-pro-for-office-365-use-you>

[\[134\]](https://www.thurrott.com/a-i/microsoft-copilot-a-i/313765/microsoft-says-it-is-not-training-copilot-ai-on-your-microsoft-365-data#:~:text=Microsoft%20Says%20it%20is%20Not,Which%20seems%20unnecessary) Microsoft Says it is Not Training Copilot AI on Your Microsoft 365 ...

<https://www.thurrott.com/a-i/microsoft-copilot-a-i/313765/microsoft-says-it-is-not-training-copilot-ai-on-your-microsoft-365-data>

[\[135\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=submitted%20ones.%20,) [\[138\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,messages%2C%20nor%20public%20data%20from) [\[146\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=notifications%20to%20explain%20that%20Meta,) [\[148\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=article) [\[200\]](https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702#:~:text=,and%20the%20distinct%20ways%20different) Meta to start training its AI models on public content in the EU - News - Privacy Guides Community

<https://discuss.privacyguides.net/t/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/26702>

[\[136\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=While%20Meta%20has%20been%20training,data%20to%20train%20AI%20models) [\[137\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=a%20clear%20legal%20basis%20for,data%20to%20train%20AI%20models) [\[139\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Starting%20this%20week%2C%20users%20in,received%2C%20as%20well%20as%20newly) [\[140\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20notes%20that%20it%20doesn%E2%80%99t,EU%2C%20to%20train%20its%20models) [\[141\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=interactions%20with%20Meta%20AI%20will,used%20to%20train%20its%20models) [\[142\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=regulatory%20pressure%20due%20to%20data,used%20to%20train%20its%20models) [\[147\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=from%20its%20EU%20user%20base,as%20well) [\[187\]](https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1#:~:text=Meta%20to%20start%20training%20its,public%20content%20in%20the%20EU) Meta to start training its AI models on public content in the EU | TechCrunch

<https://techcrunch.com/2025/04/14/meta-to-start-training-its-ai-models-on-public-content-in-the-eu/?guccounter=1>

[\[144\]](https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/#:~:text=Reddit%20www,not%20be%20available%20to%20you) How to Opt-Out of Meta Using Your Data for AI Training - Reddit

<https://www.reddit.com/r/facebook/comments/1d5h15h/how_to_optout_of_meta_using_your_data_for_ai/>

[\[145\]](https://www.iamexpat.de/expat-info/germany-news/how-european-users-can-opt-out-meta-using-their-data-train-ai#:~:text=How%20European%20users%20can%20opt,to%20train%20its%20AI%20system) How European users can opt out of Meta using their data to train AI

<https://www.iamexpat.de/expat-info/germany-news/how-european-users-can-opt-out-meta-using-their-data-train-ai>

[\[149\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=When%20you%20create%20an%20account%2C,all%20of%20the%20following%20information) [\[150\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,or%20other%20policies) [\[152\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,in%20experience%20and%20for%20security) [\[154\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,We%20will%20obtain) [\[155\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,see%20our%20Cookies%20Policy) [\[157\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=%2A%20Log,publicly%20available%20information%20via%20the) [\[159\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=How%20We%20Use%20Your%20Information) [\[160\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=user%20support.%20,the%20Platform%2C%20such%20as%20by) [\[162\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Platform%2C%20provide%20customer%20support%20to,to%20protect%20health%20or%20life) [\[163\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,to%20protect%20health%20or%20life) [\[164\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Purpose%20of%20processing%20Personal%20information,DeepSeek%20and%20provide%20user%20support) [\[165\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=such%20as%20our%20machine%20learning,training%20and%20improving%20our%20technology) [\[166\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,Keep%20Your%20Information) [\[167\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=We%20retain%20information%20for%20as,or%20defense%20of%20legal%20claims) [\[168\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=The%20retention%20periods%20will%20be,necessary%20to%20process%20the%20violation) [\[169\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=provide%20you%20with%20the%20Services%2C,necessary%20to%20process%20the%20violation) [\[172\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=services%20with%20information%20in%20urgent,to%20protect%20health%20or%20life) [\[173\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Our%20Corporate%20Group) [\[188\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=,2025) [\[203\]](https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html#:~:text=Where%20We%20Store%20Your%20Information) DeepSeek Privacy Policy

<https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy-2025-02-14.html>

[\[177\]](https://proton.me/blog/deepseek#:~:text=Using%20DeepSeek%3F%20Here%27s%20why%20your,history%2C%20prompts%2C%20and%20audio%20input) Using DeepSeek? Here's why your privacy is at stake | Proton

<https://proton.me/blog/deepseek>

[\[178\]](https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj#:~:text=Is%20DeepSeek%20safe%3F%20What%20happens,chat%20history%2C%20or%20other) Is DeepSeek safe? What happens to your data when you use it

<https://www.expressvpn.com/blog/is-deepseek-safe/?srsltid=AfmBOopeQaJ2d7ImG_kzfSwgE5Od_Ql3iNpnUTM_AqtlOqaZCTIpeYvj>

[\[204\]](https://www.reddit.com/r/gdpr/comments/1lk4baw/what_are_deepseeks_privacy_practices/#:~:text=DeepSeek%20collects%20three%20main%20categories,uploaded) What are DeepSeek's privacy practices? : r/gdpr - Reddit

<https://www.reddit.com/r/gdpr/comments/1lk4baw/what_are_deepseeks_privacy_practices/>
