# AI in banking: between DPIA and AI Act

## Abstract

The banking sector is adopting AI in critical areas such as credit scoring, fraud, and customer service, but the regulatory landscape is rapidly changing. This article connects GDPR and the AI Act, clarifying when DPIA and FRIA are triggered and what operational obligations arise from them. The objective is to offer a practical roadmap for assessing risks, distinguishing high-risk cases, and establishing coherent governance.

## Introduction

The adoption of AI in banking is no longer experimental: it is already part of processes that affect rights, access to credit, and data protection. Before delving into the rules, an introductory framework is needed to connect the main use cases to the two regulatory pillars (GDPR and AI Act) and the required impact assessments. This article bridges technology and compliance, to guide project choices from the outset.

## What the rules say and how to proceed

### Key findings

The new European AI rules (Reg. EU 2024/1689, "AI Act") classify several uses of Artificial Intelligence (AI) in the banking sector as _"high-risk systems"_, imposing stringent requirements[[1]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[[2]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati). In particular, the use of AI for assessing the creditworthiness (credit scoring) of natural persons falls under Annex III and is therefore considered _high-risk_[[1]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Similarly, AI systems used in the process of **recruitment and personnel management** (e.g., candidate selection, hiring or promotion decisions) are explicitly listed among high-risk uses, given the potential impact on workers' rights and the risk of discrimination[[2]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati). Other typical use cases in banking - such as **chatbot or virtual assistant systems for customers**, **anti-money laundering (AML) and fraud detection** tools, **identity verification (KYC)** systems with biometric recognition, etc. - while not all falling into "high-risk" categories under the AI Act, nonetheless entail significant compliance obligations. In particular, the AI Act exempts AI systems used for detecting financial fraud or for prudential capital calculations from the _high-risk_ category[[3]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi); this means that, paradoxically, **fraud detection** or **AML suspicious transaction monitoring** algorithms are not subject to the high-risk requirements of the AI Act, despite having to comply with sector-specific legislation (e.g., anti-money laundering) and privacy regulations. In any case, the obligation for banks to carry out a case-by-case assessment remains: for example, a banking AI system that, while not expressly falling under Annex III, presents significant risks to rights, safety, or access to essential services could be prudently treated as _high-risk_ for internal purposes.

Primary regulatory sources outline a series of **mandatory checklists** and applicable good practices. On the privacy front, the GDPR mandates a **Data Protection Impact Assessment (DPIA)** for high-risk processing, and authorities (EDPB and Garante Privacy) have listed typical criteria and cases: for example, large-scale profiling or scoring, automated decisions with significant effects (such as granting a loan), systematic monitoring of users, use of sensitive data or innovative technologies like AI[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Many banking AI use cases meet these criteria (e.g., credit scoring = financial profiling with impact on rights; transaction monitoring = systematic control; facial recognition = biometric data) and _therefore require a prior DPIA_. In parallel, the AI Act introduces for _deployers_ of high-risk AI systems the obligation to conduct a **Fundamental Rights Impact Assessment (FRIA)** before putting them into use[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti). In the banking sector, this particularly concerns entities that use AI Act Annex III systems, for example, banks that use AI for credit scoring (Annex III §5(b)) or for decisions on life/health insurance policies[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti). Such a FRIA must consider the context of use, categories of data subjects involved, risks to rights (e.g., bias, exclusion), planned human oversight measures, and remedial actions[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d). It must be notified to the competent market surveillance authority with the relevant results[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica), thus becoming a formal compliance requirement before the system's use.

A key issue that has emerged is the **overlap and distinction** between DPIA (focused on GDPR privacy risks) and FRIA (focused on broader _ethical and fundamental rights_ impacts). In the case of AI systems that process personal data, the legislator provides that the FRIA _integrates_ with the DPIA already carried out[\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=4,d%27impatto%20sulla%20protezione%20dei%20dati), avoiding duplication: in practice, the DPIA covers data privacy and security, while the FRIA extends the analysis to discrimination, fair access to services, transparency towards data subjects, etc. For example, for a loan granting algorithm, the DPIA will assess the lawfulness and proportionality of data processing, minimization, security measures, and anonymization; the FRIA will add the assessment of risks of _algorithmic bias_, socio-economic impact (e.g., financial exclusion of vulnerable groups), and the adequacy of **human oversight** measures adopted. This last point - human supervision - recurs as a basic principle: both the GDPR (Art. 22) and the AI Act and sector guidelines insist on the need for AI _not to make completely autonomous decisions without the possibility of human intervention_. The **EBA 2020 Guidelines on loan origination and monitoring** have clarified that the use of automated models is permitted only within precise boundaries, including _transparency, traceability, non-discrimination, and constant supervision_[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). Similarly, the Bank of Italy has reiterated that AI can support credit risk assessment _but not replace human judgment_: the bank remains ultimately responsible for the correctness and legitimacy of decisions[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). **The indispensability of human control** is also enshrined at the level of international principles: there is broad convergence on the importance of maintaining significant human intervention ("human-in-the-loop") in automated decision-making processes, to ensure accountability and protection of rights[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:).

• “international convergence on principles, translation of these principles into). In banking practice, this translates, for example, into requiring that the most impactful decisions (e.g., credit refusal, suspicious transaction reporting) be reviewed or endorsed by competent personnel, and that the operators involved have _adequate AI training_ and the power to stop or correct the system[[14]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[[15]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario). Large institutions are indeed moving towards "hybrid" models where AI processes recommendations but humans have the final say, investing in staff **upskilling** programs to bridge the technical skills gap[[16]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent). This also addresses risk management needs: as observed by the EBA, many banks adopt a gradual and prudent approach to AI, introducing robust **control measures and "guardrails"** before extending the use of advanced models, especially generative ones, and testing the riskiest use cases only after gaining sufficient experience and confidence in the technology[[17]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences).

From a strictly regulatory perspective, _new operational obligations_ emerge: for example, requirements for **transparency towards users**. The AI Act regulation mandates (even for non-high-risk systems) that anyone interacting with an AI be informed that they are interacting with a machine and not a human being[[18]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). This means that in an automated contact center or banking chatbot, it must be clearly indicated to the customer that the service is provided by an AI system, and channels for _escalation to a human operator_ must be provided upon request. Similarly, if the AI generates content (e.g., a textual response in natural language to the customer), any disclaimers regarding the automatic origin and accuracy of the information must be provided. On the **non-discrimination and fairness** front, although Italian banking legislation contains only generic fairness clauses in credit granting, the new framework requires reinforced attention: the AI Act explicitly includes the prohibition of algorithms that introduce classifications based on sensitive characteristics (practices comparable to _social scoring_ are prohibited) and, as mentioned, includes credit among high-risk cases precisely because of possible _algorithmic biases_[[1]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%27Unione%20al). Privacy guarantors and consumer authorities will be able to challenge unfair practices if a model systematically denies services to protected categories. Therefore, banks must implement **AI governance** techniques: proactive model testing to detect disparities in treatment (bias audit), transparent documentation of variables used (feature importance), and effective complaint mechanisms for data subjects. A customer has the right to know if a decision regarding their financing was influenced by an algorithm and on what parameters[[19]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori), as well as to obtain human intervention and challenge the automated decision - as provided by GDPR Art. 22.

**Main Regulatory Ambiguities:** Despite regulatory progress, grey areas and interpretative issues persist. A first element of uncertainty concerns the _exact scope of high-risk categories_: some banking use cases do not clearly fall under Annex III. For example, the use of AI for **Anti-Money Laundering** (detection of suspicious transactions) is not listed among high-risk uses unless it is carried out directly by law enforcement authorities - and indeed the EU legislator has excluded systems used by FIUs from police scopes[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo). This raises doubts: an algorithm that _preventively blocks_ transactions or current accounts could impact fundamental rights (e.g., economic freedom) almost as much as a credit scoring system, but formally the AI Act does not cover it as high-risk. The choice seems motivated by the desire to foster anti-fraud innovation, but the boundary remains ambiguous: banks will have to decide whether to treat these "unclassified" systems with a conservative approach anyway (voluntarily applying requirements similar to high-risk ones) for prudence and accountability.

Further ambiguity concerns the **definition of "significant risk"** in the AI Act. The regulation indeed stipulates that systems listed in Annex III are _not_ to be considered high-risk if, "by way of derogation," they _do not pose a significant risk_ to health, safety, or fundamental rights[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III). This exemption clause is not easy to apply in practice: for example, a small AI solution used experimentally on a few clients, while technically falling into a category (say, credit scoring), could be argued by the provider to pose a negligible risk; however, the criteria for establishing this are not explicit, and there is a risk of divergent interpretations. Companies might be reluctant to "downgrade" a system from high-risk to non-high-risk, fearing ex-post challenges - thus, a precautionary approach is likely, but the lack of implementing guidelines in this regard is a gap that will require clarification (the Commission is delegated to issue acts to amend Annex III and provide criteria, but it remains to be seen how this will be managed).

A third area of uncertainty concerns the **methodology and governance of the FRIA**. As this is a new requirement, there are no consolidated standards yet on _how to conduct an ethical/fundamental impact assessment_. The AI Act stipulates that the _AI Office_ (a new European body) will provide a questionnaire template, also via an automated tool, to facilitate deployers[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato), but until this is developed, companies will have to manage by drawing inspiration from similar guidelines (e.g., those from the Garante for ethical assessments, or frameworks like the EU's Assessment List for Trustworthy AI). What expertise will they need to involve? Who will be the "notifying authority" for the FRIA in Italy for the banking sector - the Ministry of Economic Development, Banca d'Italia, or a new body? - This is not yet precisely defined, creating operational ambiguities. Furthermore, while for privacy DPIAs there is an obligation to consult the Garante only if high unmitigated risks remain, for the FRIA there appears to be a generalized obligation of pre-use notification[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica), but without a clear process on what happens if the authority deems the FRIA inadequate or the use too risky: can it block it? will a kind of "no-objection" be required? These will be issues to clarify at the implementation level.

Ambiguità anche nella **distinzione di ruoli e responsabilità** lungo la filiera AI. In molti casi le banche utilizzano soluzioni di IA fornite da vendor terzi o basate su modelli generativi pre-addestrati (es. un foundation model linguistico integrato nel chatbot). Il regolamento distingue **"fornitore"** (chi immette sul mercato il sistema AI) e **"utilizzatore (deployer)"** finale; nel caso bancario, però, una banca che sviluppi internamente un algoritmo per uso proprio potrebbe essere considerata sia fornitore che deployer, con obblighi cumulativi (inclusa la conformità tecnica e marcatura CE del sistema high-risk). Se invece acquista un servizio AI esterno, dovrà comunque garantire gli obblighi dei deployer (FRIA, registrazione nel database UE, sorveglianza d'uso) ma dipende dal fornitore per la documentazione tecnica conforme. È incerto come gestire contrattualmente questa condivisione di responsabilità: le banche dovranno pretendere dai vendor garanzie di conformità AI Act (es. **EU Declaration of Conformity** per sistemi high-risk) e accesso alle info sul modello per poter fare la FRIA, ma non è ancora pratica comune.

Un'altra area grigia è la **gestione della spiegabilità ed esercizio dei diritti GDPR** in presenza di modelli AI opachi (es. deep learning). Il GDPR dà all'interessato diritto ad avere spiegazioni significative sulla logica di decisioni automatizzate; tuttavia, le tecniche di explainable AI sono ancora emergenti e potrebbe non essere possibile fornire spiegazioni semplici di modelli complessi. Le banche dovranno bilanciare questo obbligo con la tutela del segreto industriale sui propri algoritmi. Non esiste ancora un consenso su quale livello di trasparenza sia "sufficiente" - ambito in cui sono attesi orientamenti dal EDPB o dal futuro AI Office.

Infine, permane incertezza su **come tradurre in prassi concrete i principi etici condivisi**. La letteratura e le autorità convergono su principi come _non-discriminazione, trasparenza, oversight umano_[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in), ma - come notato da Banca d'Italia - risulta meno agevole incorporarli in norme vincolanti e procedure operative efficaci[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti). Ad esempio, tutti concordano sull'evitare bias, ma definire metriche quantitative di fairness e soglie accettabili di disparità è complesso e lasciato all'autonomia delle imprese per ora. Similmente, è pacifico che debba esserci un intervento umano, ma quanta discrezionalità e in quale fase del processo è adeguato? Sono aspetti su cui si naviga ancora a vista, con approcci conservativi (ad es. richiedere sempre un doppio controllo umano indipendente per certe decisioni critiche) in attesa di prassi consolidate.

---

Ambiguity also exists in the **distinction of roles and responsibilities** along the AI supply chain. In many cases, banks use AI solutions provided by third-party vendors or based on pre-trained generative models (e.g., a linguistic foundation model integrated into a chatbot). The regulation distinguishes between **"provider"** (who places the AI system on the market) and the end **"user (deployer)"**; however, in the banking sector, a bank that internally develops an algorithm for its own use could be considered both a provider and a deployer, with cumulative obligations (including technical compliance and CE marking for high-risk systems). If, on the other hand, it purchases an external AI service, it will still have to ensure the deployer's obligations (FRIA, registration in the EU database, use surveillance) but depends on the provider for compliant technical documentation. It is uncertain how to contractually manage this sharing of responsibilities: banks will have to demand AI Act compliance guarantees from vendors (e.g., **EU Declaration of Conformity** for high-risk systems) and access to model information to be able to perform the FRIA, but this is not yet common practice.

Another grey area is the **management of explainability and the exercise of GDPR rights** in the presence of opaque AI models (e.g., deep learning). The GDPR grants data subjects the right to meaningful explanations of the logic behind automated decisions; however, explainable AI techniques are still emerging, and it may not be possible to provide simple explanations for complex models. Banks will have to balance this obligation with the protection of trade secrets regarding their algorithms. There is not yet a consensus on what level of transparency is "sufficient" – an area where guidance is expected from the EDPB or the future AI Office.

Finally, uncertainty remains regarding **how to translate shared ethical principles into concrete practices**. Literature and authorities converge on principles such as _non-discrimination, transparency, human oversight_[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in), but – as noted by the Bank of Italy – it is less straightforward to incorporate them into binding rules and effective operational procedures[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti). For example, everyone agrees on avoiding bias, but defining quantitative fairness metrics and acceptable thresholds of disparity is complex and currently left to the autonomy of businesses. Similarly, it is undisputed that human intervention must occur, but how much discretion and at what stage of the process is appropriate? These are aspects on which there is still uncertainty, with conservative approaches (e.g., always requiring an independent human double-check for certain critical decisions) pending consolidated practices.

**Operational Implications for the MVP:** The evidence outlined above directly informs the requirements of the prototype tool ("AI Act Navigator" and "FRIA/DPIA Evidence Builder"). In summary, the MVP will need to: **(1)** incorporate a _use case triage_ system based on targeted questions that allow identification of whether a use case falls into _high-risk AI Act_ categories or presents GDPR DPIA triggers, guiding the user (e.g., a Compliance officer) in the correct classifications. **(2)** It will need to implement a set of transparent **decision rules** (business rules): for example, if the user indicates that the AI system performs creditworthiness assessment of retail customers, the wizard will automatically flag it as _AI Act Annex III - high-risk_ and prepare the subsequent steps (e.g., list of requirements to be met, FRIA obligation, etc.)[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). If the use case involves processing of special categories of data or extensive profiling, the tool will need to suggest a DPIA obligation[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza). **(3)** The wizard's output will need to include a _dashboard of evidence and obligations_: e.g., a customized checklist with "Documents to prepare" (e.g., _AI system descriptive sheet_, _DPIA_, _FRIA_, _record of processing_, _contract with supplier_…), "Applicable requirements" (e.g., _Art. 10 AI Act - data governance, Art. 14 - human oversight_…), "Recommended actions" (e.g., _assess bias on dataset_, _human intervention foreseen before final decision_, _information notice to data subjects to be updated_). **(4)** The MVP will need to integrate **disclaimers and explanatory notes** in each critical section to manage regulatory ambiguities: for example, a note clarifying "_If your case does not fall exactly into the AI Act categories but presents potential risks, the most prudent approach is recommended - see 'Ambiguities' section_". Or, in case of doubt about the necessity of a DPIA: "_Based on the information provided, there is no explicit DPIA obligation under Art. 35 GDPR, but a documented assessment is nevertheless recommended given the extensive use of personal data (conservative option)_". **(5)** Crucial will be the **traceability and justification** of the recommendations: the "Evidence table" deliverable will provide the rationale (regulatory source) behind each wizard rule, increasing user confidence in the guidance provided. The MVP will therefore not only guide step-by-step (wizard) but also serve as a searchable _knowledge base_, with sections like "Why are we asking you this?" or "Why is this document required?" drawing on authoritative sources (Garante, EBA, legislation) gathered in the research[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario)[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti).

# Evidence Table

| **Theme** | **Evidence** | **Source** | **Implication for wizard** | **Confidence** |
| --- | --- | --- | --- | --- |
| **High-risk banking use cases (AI Act)** | The AI Act classifies AI systems used to assess individuals' creditworthiness (credit scoring) and for employment purposes (e.g., personnel selection) as _high-risk_. Such systems can significantly impact individuals' rights and living standards, risking the perpetuation of discrimination[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati). However, AI systems for detecting financial fraud or calculating capital requirements are excluded from Annex III (not considered high-risk)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi). | _EU Reg. 2024/1689 (AI Act)_, recitals 58 and 59[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi); _Paradigma, 2025_[\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano). | Immediately identify if a use case falls into Annex III categories (e.g., credit, HR) - if so, mark it as "High-Risk AI Act" and activate compliance assessment modules (FRIA, AI Act Art. 8-15 requirements). For excluded areas (e.g., anti-fraud), still indicate sectoral obligations but with a different AI Act regime. | **High** (clear regulatory text; confirmed by doctrine) |
| **DPIA - trigger in the banking sector** | The GDPR requires a DPIA for high-risk processing; WP29/EDPB guidelines list criteria: among them, large-scale profiling or scoring based on economic situation, automated decisions with legal effects (e.g., loan granting), systematic monitoring, use of sensitive data or new technologies (like AI)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). The Garante (Italian DPA) has specified 12 mandatory types, including: _"large-scale evaluative or scoring processing"_ and _"automated decisions significantly affecting the data subject (e.g., screening bank customers for loan granting)"_[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la%20valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). | _WP29 Guidelines No. 248/2017_ (EDPB)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza); _Garante Privacy, Order 467/2018_[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). | In the wizard questionnaire, include questions to detect if the use case involves financial profiling, automated decisions about customers, transaction monitoring, use of biometrics, etc. - if affirmative, trigger a "DPIA mandatory" alert and add the task "Perform DPIA" to the output. | **High** (official EDPB guidelines adopted by the Garante) |

| **FRIA - obligation and contents** | The AI Act (Art. 27) obliges _deployers_ of high-risk AI systems (public authorities and private entities providing public services, as well as those using systems listed in Annex III points 5(b) and (c)) to carry out a **Fundamental Rights Impact Assessment** before use[[6]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti). The FRIA must include: description of the use context and purpose of the system, duration and frequency of use, categories of impacted persons, specific risks to rights (taking into account information provided by the provider pursuant to Art. 13 of the AI Act), planned human oversight measures, and mitigation/management measures in case of problems (incl. complaint mechanisms)[[7]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[[8]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d). The outcome must be notified to the market surveillance authority, using the template that will be prepared (also via an automated tool by the AI Office)[[9]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica)[[22]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato). If the deployer has already carried out a GDPR DPIA covering some aspects, the FRIA can integrate that analysis without duplicating it[[10]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=4,d%27impatto%20sulla%20protezione%20dei%20dati). | _EU Reg. 2024/1689_, Art. 27[[6]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)[[7]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista); EU Council, Press Release 9/12/23[[26]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons). | The wizard must clearly explain when the FRIA is required (e.g., "High-Risk classified Use Case → FRIA mandatory before deployment"). It should guide the user in assembling the elements for the FRIA: e.g., ask to describe purpose and context, list of impacted stakeholders, etc., and produce a report outline. Furthermore, it must remind of the need for _notification to the authority_ and potentially provide a compliant output template (e.g., standard AI Office forms). It must also indicate that if a DPIA has been carried out, it should be updated/integrated rather than duplicated. | **High** (detailed legal provision) |

| **Human oversight - obligation and models** | _AI Act_, Art. 14[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in)[\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana); _EBA Risk Report 2024_[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences); _Paradigma 2025_ (cit. EBA GL)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). | The wizard must include fields/questions to verify the presence of oversight mechanisms: e.g., "Is human intervention foreseen before the final decision is applied to the client?"; "Does the assigned personnel have the authority to block or correct the AI's output?". Based on the answers, provide alerts if oversight is inadequate (non-compliance trigger Art. 14). Furthermore, in the _output specifications_ for each use case, add recommendations on sustainable control models (e.g., double human verification for critical decisions, specific training for AI operators, periodic audits of model results). | **High** (legal requirement + EBA best practice) |
|---|---|---|---|
| Sectoral regulations and the AI Act converge on the obligation to maintain **effective human oversight** over high-risk AI systems. Art. 14 of the AI Act requires that such systems be designed to be "overseeable" by persons, and that the persons responsible for control have the skills, training, and authority to intervene, including by interrupting the system if necessary[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario). The **EBA Guidelines** on credit prescribe that AI should not operate with full autonomy: the intermediary must be able to review and _potentially override_ the model's decisions (the _"human-in-the-loop"_ principle)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). For example, Art. 172(3) CRR already requires that IRB banks have the possibility of _human override_ of the results of internal rating models[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=%E2%80%A2%20CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving). EBA analyses show that EU banks are adopting graduated approaches where **human intervention** is guaranteed, especially in the initial phases of advanced AI adoption, to control risks[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants). | | |

| **Transparency towards individuals** | _AI Act_, Art. 52 (now 50)[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system); _GDPR_, Art. 13-14, 22; _Paradigma 2025_[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). | The wizard should ask if the AI interacts with customers or determines outputs directed at people. If yes, include among the mandatory outputs: "Prepare a specific information notice for users", "Insert visible disclaimers in the interface (e.g., 'Automated Virtual Assistant')". Furthermore, provide guidelines on how to draft explanations of decisions in understandable language. The tool could include an **information notice template** module to be filled with the details of the use case (e.g., type of algorithmic logic, data used) in compliance with GDPR. | **High** (clear GDPR + AI Act rules) |
| The AI Act also imposes transparency obligations for non-high-risk systems: for example, anyone using a **chatbot** or system that interacts with people must clearly inform the user that it is an automated system[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Furthermore, if content (text, image) is generated by AI, it must be declared to the end-user (to prevent deception). In the banking sector, GDPR Art. 13-14 and 22 already require communicating to data subjects the existence of automated decisions and providing meaningful information about the logic used and the envisaged consequences[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). National legislation (e.g., TUB Art. 124-bis) reiterates that if credit is granted using automated tools, the customer must be clearly informed and has the right to adequate explanations. | | | |
| **Non-discrimination and fairness** | _Bank of Italy QEF 721/2022_[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove); _AI Act_ Recital 58[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al); _Paradigma 2025_[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano). | The wizard must include checkpoints dedicated to fairness: e.g., ask if the dataset has been checked for biases (representative imbalances), if the model uses potentially discriminatory attributes (direct or indirect). As an output, for sensitive use cases (credit, HR), recommend performing a _"bias audit"_ and documenting the results in the FRIA. Furthermore, indicate the conservative option of excluding non-pertinent variables or those potentially proxy for protected categories from the model. Provide ref | |
| The principle of non-discrimination is not regulated in detail in financial regulations, but it is a central focus of the AI Act and authorities. The Bank of Italy notes that banking transparency provisions contain few explicit references to _equal treatment_, and the use of AI-ML techniques calls for new attention on this front[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove). The AI Act prohibits social scoring systems and categorization based on sensitive data, and in its recitals highlights the risk that credit or recruiting models may _perpetuate historical biases_ (e.g., disadvantaging women, minorities)[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al). Real cases confirm the danger: overly opaque HR algorithms or credit algorithms based on data correlated with ethnicity/area can produce disparities. The Italian Data Protection Authority has pointed out that credit profiling must avoid processing sensitive data or proxies thereof (e.g., residential postcode) without adequate safeguards[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi). | | | |

references to guidelines (e.g., Technical Appendix on fairness metrics) in the wizard's bibliography for further reading. | **Medium** (general principle clear, but uniform metrics are lacking) |
| **Regulatory Overlap and Conservative Approaches** | The EBA found that many AI Act requirements (e.g., data governance, robustness, oversight) are partly covered by existing financial regulations, although there are no _explicit derogations_ in the AI Act for the banking sector[[36]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of). This means that banks and intermediaries will have to comply with both frameworks: for example, an IRB credit model must follow CRR/EBA rules _and_ meet AI Act requirements (technical documentation, testing, etc.). This dual track can create burdens, but also opportunities for integration. For example, periodic model controls required by Banca d'Italia/EBA (validations, backtesting) can also serve as continuous monitoring measures under the AI Act. In case of interpretative doubt regarding borderline categories, banks are adopting a prudential approach, often voluntarily applying the most rigorous measures. A recommended practice is to consider **combined FRIA and DPIA** as part of a single _AI risk assessment_ process, involving different functions (Compliance, DPO, Risk Management) to cover all aspects. | _EBA Chair Letter 2025_[[36]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of); _Banca d'Italia QEF_[[37]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono)[[13]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in). | The wizard should provide the user with a framework of _"regulatory intersection"_ - for example, a summary section stating: "Your case requires: AI Act compliance (requirements X, Y, Z) **and also** adherence to banking rules ABC (e.g., EBA guidelines x)". Suggest an integrated approach: output could recommend unifying DPIA+FRIA into a single corporate document/procedure. Furthermore, in the explanations, the tool will highlight where obligations coincide (e.g., data quality is both an AI Act Art. 10 requirement and a good practice for internal models) to avoid duplication. In case of uncertainty (use case not explicitly regulated), the wizard will adopt the "conservative approach suggested" flag and include extra precautionary measures. | **High** (authoritative EBA reconnaissance; convergence with Banca d'Italia practices) |

| **Examples and Precedents** | **Media** (useful empirical evidence, but indirect references) |
|---|---|
| Italian regulatory sandboxes have already experimented with innovative banking AI solutions: e.g., the "Kalaway" project for a credit risk scoring platform for SMEs and "O-KYC" for adequate customer due diligence with DLT[[38]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi)[[39]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi). Final reports show that such solutions are considered implementable outside the sandbox _provided all applicable regulations are complied with_[[40]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione)[[41]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=ICCREA%20Banca%20e%20Banca%20Monte,una%20futura%20commercializzazione%20della%20soluzione). Furthermore, interventions by the Privacy Guarantor (such as the case of the generative chatbot sanctioned in 2023) show that the use of AI must be based on clear legal grounds and transparency: in provision no. 755/2024, the Guarantor fined a generative AI provider for having trained the model on personal data without a valid basis and without adequately informing the data subjects[[42]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione)[[43]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata)[[43]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello). These examples indicate both the feasibility of new technologies in banking and the _pitfalls to avoid_ (e.g., unnotified data breaches, incomplete information). | _Bank of Italy Sandbox - report_[[40]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione); _Privacy Guarantor, Prov. 755/2024_[[42]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata)[[43]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello). |
| In the wizard, synthetic _case studies_ can be integrated: for example, a "Lessons from the Sandbox" card reminding the user to verify the legal basis for AI training (consent/contract to use customer data?) and to prepare notifications in case of AI data breaches. The evidence builder can also include as positive "evidence" the fact that the Bank of Italy validated model X for credit scoring in the sandbox, implying that similar use cases are allowed if compliant with EBA guidelines on credit granting[[44]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=intermediari%20finanziari%20per%20offrire%20loro,durata%20massima%20di%2018%20mesi). This increases user confidence in pursuing certain projects, but accompanies it with warnings (compliance by design). | |

# MVP Specifications "AI Compliance Navigator"

**Minimum Input Fields (max 20):** To power the triage process, the tool will require key information from the user about the AI use case under consideration. The fields will include:
- **Use Case Description** (free text): brief text about the scenario (e.g., "Customer support chatbot that suggests financial products");
- **Primary Application Domain**: dropdown menu (Credit, Customer Service, AML/Fraud, Human Resources, KYC/Onboarding, Other) - for mapping against Annex III;
- **AI Purpose and Function**: e.g., _scoring/assessment_, _automated decision-making_, _decision support_, _content generation_, _monitoring/anomaly detection_, etc.;
- **Impact on Individuals**: type of decision influenced (e.g., _service grant or denial_, _customer risk classification_, _candidate evaluation_, _no direct impact_);
- **Involvement of Personal Data?** (yes/no) - a discriminator for DPIA;
- **Type of Data Processed**: checklist (financial data, identification data, demographic data, biometric data, sensitive data under Art. 9 GDPR, anonymized data, etc.);
- **Scale of Processing**: indicative number of data subjects (e.g., <1000, thousands, millions) and geographical origin (EU/non-EU) - to assess "large scale" and transfers;
- **Model Logic**: multiple choice (Deterministic algorithms/fixed rules; Traditional Machine Learning; Deep Learning/NN; Generative AI/foundation model) - to assess explainability and technical requirements;
- **Data Source and Quality**: origin of training/input data (internal bank data, third-party data, open data, web scraping) and presence of possible known biases (notes field);
- **Company Role**: selection (In-house model developer; User of third-party provided model; Both) - to determine provider vs. deployer obligations;
- **Involvement of Third-Party Providers**: name/description of any vendor or pre-trained model (e.g., use of external GPT API) - to prepare contractual clauses and compliance requirements;
- **Presence of Automated Decision-Making**: (yes/no) - if the AI output is applied without direct human intervention (e.g., automatic request denial) - trigger for Art. 22 GDPR and oversight;
- **Planned Oversight Measures**: menu or checkbox (Human review of all critical outputs; Human intervention upon request; No human intervention; Other) - to assess compliance with Art. 14 AI Act;
- **Transparency towards Data Subjects**: checkbox (Dedicated privacy notice prepared; Not yet; Not applicable - e.g., internal-only tool);
- **Known/Self-Assessed Risks**: free text field or multiple selection (Possible discriminatory biases; Risk of errors/false positives; Impact on privacy; Other) - to trigger targeted advice and prepare FRIA;
- **Specific Regulatory Scope**: selection if applicable (e.g., _Consumer Credit_, _Mortgage Credit_, _Investments/MiFID profiling_, _PSD2 Payments_, _HR - Equality_, _AML_…) - to link to additional sectoral regulations;
- **Existence of Previous DPIA**: (yes/no) and if yes, brief reference - to integrate with FRIA;
- **Project Stage**: (Idea; Development; Internal Pilot; Active Production) - to guide recommendations (e.g., if still in development, consider controlled sandbox/testing).

_(NB: The total number of fields is optimized, and some items may be combined in the interface to stay within ~20 effective inputs, e.g., a single "Data Types & Affected Categories" section covering both sensitive data and vulnerable users.)_

**Main Logical Rules/Triggers:** Based on the inputs above, the MVP will apply "if-then" rules to determine obligations and outputs:
- **High-Risk AI Act Classification:** if _Domain_ = Credit **or** if _Purpose_ = creditworthiness assessment **→** flag "Possible AI Act Annex III (5)(b) - high-risk system (credit scoring)"[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). If _Domain_ = HR/Recruiting **→** flag "High-risk (hiring/employment)". These flags will trigger: FRIA obligation, indication of AI Act requirements (compliance, registration, etc.), and, if applicable, EU database notification[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)[\[45\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=usati%20nel%20settore%20elencati%20nell%27allegato%C2%A0III%2C,una%20valutazione%20dell%27impatto%20sui%20diritti).
- **Mandatory DPIA:** if _Personal data involvement_ = yes **and** (Large-scale automated profiling **or** Decision without human intervention with legal effects **or** Use of sensitive/biometric data **or** Systematic monitoring of customer behavior)[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento) **→** suggest "DPIA required pursuant to Art. 35 GDPR" with a list of reasons (highlighting the specific criteria met).
- **Consent Request Art. 22 GDPR:** if _Automated decision_ = yes **and** significantly impacts the data subject (credit outcome, hiring) **→** warning: "Verify legal basis for automated decision: explicit consent of the data subject _or_ legal derogation" (e.g., necessary for contract performance).
- **Transparency and Right to Explanation:** if _Automated decision_ = yes **→** output to include: "Inform the data subject of the system's logic (GDPR 13-15) and provide a channel for requesting human intervention"[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori).
- **Insufficient Oversight:** if _Oversight measures_ = "No human intervention" **or** _Purpose_ = automatic decision _and_ oversight = limited **→** flag of _potential non-compliance_: recommend inclusion of human control (e.g., "Human review is recommended: currently non-compliant with Art. 14 AI Act")[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in).
- **Bias/Fairness:** if _Domain_ ∈ {Credit, HR, AML} **or** user has selected "discriminatory bias" risk **→** include "Fairness Assessment" section in the report: suggest algorithmic audit, removal of proxy variables, etc., recalling the principle of non-discrimination[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al).
- **GenAI/Foundation model:** if _Model logic_ = Generative **→** recommend extra measures: e.g., output validation, prompt filtering, disclosure of generated content (transparency obligation)[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system), intellectual property control over training data, etc.
- **Vendor management:** if _Role_ = third-party user **→** display checklist: "Request from vendor: AI safety sheet, technical documentation (Annex IV AI Act), clauses on updates, audit rights, etc.".
- **Sandbox/regulatory testing:** if _Project stage_ = Pilot/Ideation **→** suggest evaluating entry into a sandbox (citing initiatives such as that of Banca d'Italia) as an option.
- **Non-applicability of AI Act:** if _Domain_ = "Other" _and_ no relevant risk **→** minimal output: e.g., "The use case does not appear to fall under specific AI Act obligations beyond general transparency requirements - GDPR rules still apply if personal data is involved".

**MVP-generated output:** The output will consist of several structured elements, designed as the requested deliverables A-G:
\- **Personalized Executive Summary:** an 8-10 point summary of findings for the specific use case entered. Ex: _"The proposed system falls under the AI Act's high-risk category (Credit): you will need to obtain CE marking and conduct a FRIA before use_[_\[6\]_](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)_. It involves large-scale profiling of financial data: a GDPR DPIA is required_[_\[4\]_](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)_. Possible regulatory ambiguities have emerged (e.g., automated decision based on credit profiling): a conservative approach with final human intervention and enhanced customer information is recommended…"_. This summary will also include 3-5 key implications for MVP design (e.g., _"integrate bias testing module"_, _"provide disclaimer to the end-user"_).
\- **Personalized Evidence Table:** a table similar to the general one but focused on the specific case, with rows on relevant topics. For example, for a credit scoring case, rows will appear for "Credit = High-risk (source: AI Act)", "Profiling = DPIA (source: GDPR)", "Bias risk (source: AI Act/EBA considerations)", each with implications for the project. This table draws from the general evidence database but filters for applicable ones.
\- **Requirements & Actions Checklist:** a bulleted list of activities to be carried out for compliance: e.g., _Draft AI Act Technical Document (Annex IV)_, _Perform accuracy and robustness tests_ (Art. 15), _Register the system in the EU High-Risk register_ (Art. 60) if applicable, _Conduct training for assigned personnel (oversight)_, _Update privacy policy for transparency_, etc. Each item will be marked as "Mandatory" or "Recommended" depending on the source (hard law vs. best practice).
\- **Functional MVP Specifications:** if relevant, the output will include design recommendations for the **technical implementation** of the AI itself from a compliance perspective (e.g.: _"Log all model decisions for audit (Art. 12 AI Act record-keeping)"_, _"Implement alerts if input is out of distribution (drift monitoring)"_, _"User interface: provide decision explanation field for the customer"_). These specifications help IT and Data Science build the system "compliant by design".
\- **"Use Case Sheet" Template:** the tool will generate a concise document (1-2 pages) for each of the 5 target use cases (if relevant) - for example, if the user has classified the case as "Credit", the pre-filled sheet for _AI in Credit Scoring_ will be attached with sections: Description, Specific Risks, Regulatory Requirements, Oversight Solutions, Practical Examples. This corresponds to the requested use case sheet deliverable, adapted with the provided data.
\- **Residual Questions and To-Dos:** a list of max 5-10 open questions or checks to be performed before go-live, tailored to the case. Ex.: _"Verify with the legal department whether the contractual basis Article 6(1)(b) GDPR is applicable for this automated processing"_; _"Consult DPO on the need for prior consultation with the Supervisory Authority in case of residual DPIA risks"_; _"Deepen suitable fairness metrics (e.g., disparate impact) and repeat bias tests with an expanded dataset"_.
\- **Annotated Bibliography:** list of relevant normative sources and guidelines cited, with links and brief descriptions, so the user can delve deeper (for example: _"EU Regulation 2024/1689 (AI Act) - Articles 6, 14, 27: defines high-risk and impact assessment obligations"_, _"Garante Provision 467/2018 - List of processing operations requiring DPIA in Italy (includes credit scoring)"_, _"EBA Guidelines on Loan Origination 2020 - paras. 74-96: use of automated models in credit with transparency and human control requirements"_, etc.).

**Disclaimers (MVP):** The tool will display clear warnings to manage expectations and limit liabilities. Upon startup, a **general disclaimer** will clarify that: _"This tool provides compliance support based on public regulatory sources, but does not constitute legal advice nor automatically ensure 100% compliance. The user is responsible for final decisions and the implementation of proposed measures"_. Furthermore, each sensitive section will have **explanatory notes**. Example: in the output section, a disclaimer will state that _"The recommendations provided (e.g., need for DPIA) derive from the information entered; ensure that you have provided them accurately. In case of doubt, consult the DPO or the competent Authority."_[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). For borderline cases, the wizard might include a disclaimer such as: _"Conservative approach: given the non-definitive regulatory framework on this point, we suggest adopting all measures as if they were mandatory, out of prudence."_ At the bottom of the final report, a final disclaimer will recall the date and version of the sources (e.g., _"Regulations updated to December 2025"_), warning that future regulatory changes (e.g., upcoming guidelines from the Commission or EBA) may require a revision of the indications.

**Out-of-scope (MVP limitations):** The areas not covered by the tool will also be explicitly stated. For example: _"It does not address scenarios of AI exclusively for military or national security purposes (excluded from the AI Act)", "It does not evaluate intellectual property or patent aspects of the algorithm", "It does not cover detailed compliance with sectoral regulations not directly related to the use of AI (e.g., MiFID pre-contractual transparency requirements if AI is used in investment advice, which will need to be evaluated separately)"_. Furthermore, it will be stated that the tool _does not replace technical security tests or mathematical validation_ of the model: it suggests performing them but does not execute them. Finally, any judgment on the commercial or ethical appropriateness of an AI project will be out of scope: the MVP is limited to mapping obligations and gaps, leaving the final decision to proceed or not to the company functions.

# Use Case Cards (5 target cases)

## **Use Case 1: Credit Assessment (Credit scoring)**

**Description:** Use of AI algorithms to assess customer solvency and support/determine credit granting decisions (e.g., automatic rating of consumer loan applications or SME credit lines). It may include machine learning models trained on historical credit data to predict default probabilities. It often replaces or integrates traditional credit scoring systems (e.g., CRIF score) with more complex models (random forest, neural networks).

**Regulatory relevance:** _High-risk AI Act_. Credit is explicitly listed in Annex III: _"AI systems intended to be used to evaluate the creditworthiness of natural persons"_ are high-risk[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al), entailing an obligation to comply with the requirements of Arts. 8-15 AI Act (risk management, data quality, technical documentation, transparency, human oversight, accuracy). Furthermore, the bank as a _user (deployer)_ will have to perform the **FRIA** before putting the system into use[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti), evaluating impacts on rights (primarily the risk of financial discrimination). The system's inclusion in the EU register of high-risk systems (via the reference authority, unless exempted) must be foreseen. _Note:_ if the bank develops the algorithm internally and uses it only for itself, it also accumulates the role of _provider_ under the AI Act, having to carry out the conformity assessment (likely through self-certification with internal control for Annex III point 5(b)) and issue the EU Declaration of Conformity[\[46\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,coinvolgimento%20di%20un%20organismo%20notificato)[\[47\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=match%20at%20L7793%20alto%20rischio,dati%20dell%27UE%20di%20cui%20all%27articolo%C2%A071).

**Privacy & DPIA:** Almost certainly required. Credit scoring processes personal data (financial, behavioral, socio-demographic), often on a large scale, and significantly impacts data subjects (credit acceptance/rejection = legal effect). It therefore falls under at least two DPIA criteria (profiling + automated decision-making)[[4]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza)[[25]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). The Italian Data Protection Authority explicitly includes _"customer screening with central credit register data to decide on financing"_ as an example that _requires a DPIA_[[5]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Therefore, a DPIA must be conducted in accordance with Art. 35 GDPR, assessing the necessity and proportionality of the data used, risks to rights (e.g., error, exclusion of deserving individuals), and security measures. Particular attention should be paid to: potential use of _sensitive_ data (direct or indirect) - e.g., if the model infers income from postal code/residence, risking geographical disparities (potential indirect discrimination based on ethnicity/socio-economic status). GDPR Art. 22 is relevant: a purely automated credit refusal decision requires an adequate contractual basis or consent. In the EU, banks usually still include a final human intervention to avoid the strict application of Art. 22, or they justify profiling as necessary for the performance of a contract (credit disbursement) - a possible approach but not without debate[[34]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi). **Transparency** towards the customer is crucial: at least the general elements of the scoring logic must be communicated (which categories of data are influential: e.g., payment history, debt-to-income ratio, etc.) and the right to obtain explanations and challenge the decision must be guaranteed. Operationally, this often translates into the establishment of an _"AI complaints office"_ or internal procedures for manually reviewing rejected applications upon customer request.

**Additional obligations/controls:** Sector strictly regulated by banking transparency rules (TUB, Bank of Italy Transparency provisions) and the _EBA Guidelines on Loan Origination and Monitoring_ (EBA/GL/2020/06). The latter require that when using statistical/AI models for credit granting, the bank ensures: data quality and relevance, **absence of illegitimate bias** (prohibition of discrimination based on gender, ethnicity, etc.), _documentation_ of methodologies, and human oversight of final decisions[[11]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). Furthermore, they impose _reliable and understandable_ creditworthiness assessments: AI must be _"explainable"_ to staff and, in part, to the customer. Consumer credit legislation (Directive 2008/48/EC and subsequent) provides for the obligation to explain to the applicant the reasons for any refusal, especially if based on automated processes or databases (e.g., credit bureau) - which aligns with the GDPR. From a **prudential modeling** perspective, if AI credit scoring is also used for calculating provisions or regulatory ratings (AIRB), it must comply with CRR and EBA guidelines on internal models: this entails requirements for independent validation, periodic _stress tests_, and evidence that the model does not age (monitoring of performance _drift_). Fortunately, as highlighted by the EBA, such model risk management practices partly align with the AI Act's obligations on robustness and accuracy[[48]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving). Specific attention: if the system uses data from **non-traditional external sources** (e.g., social media, telco data for "alternative" credit scoring), its lawfulness (data subject's consent or public source) and quality/relevance must be verified; furthermore, the use of unconventional data could increase discrimination risks and must be considered in the FRIA.

**Oversight and Monitoring Measures:** Implement the _"four eyes principle"_ on decisions: if the algorithm yields a "reject" outcome, ideally a human officer reviews it (at least on a sample basis to verify that the model does not unfairly penalize). Implement alert thresholds: e.g., if the model rejects more than X% of a certain category (e.g., residents in an area), evaluate the causes. Establish an internal AI committee for credit, involving compliance, risk, and data science, which periodically reviews results (actual vs. predicted default rates, customer complaints received for unfair rejection, etc.) and approves any model or policy revisions. Provide **training** to credit analysts on AI functionality, so they can explain to customers and manage exceptional cases. Regarding post-deployment monitoring: log all decisions with scores and main factors, to allow for ex-post audits (required by AI Act art.12 record-keeping). The AI Act will also require implementing measures to ensure _accuracy, robustness, and cybersecurity_ (art.15): in credit scoring, this means testing the model on historical data and in simulation to ensure that predictions are sufficiently accurate and stable, and that the model cannot be easily deceived (for example, by false data). In case of model updates (retraining), validation must be redone and technical documentation updated, notifying substantial changes in the EU register if necessary.

**Practical Examples:** Several institutions have experimented with AI in credit scoring. One case was brought to the Italian regulatory Sandbox: Kalaway S.r.l.'s platform, in collaboration with Banca Patavina, for _early warning_ and automatic assessment of businesses[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi)[\[49\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=finalizzato%20a%20fornire%20alle%20banche,gestione%20del%20rischio%20di%20credito). The experimentation showed benefits in effectiveness and risk profiling capability in line with EBA guidelines, but highlighted the need to comply with all applicable regulations before commercialization[\[50\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=La%20sperimentazione%20%E2%80%93%20nell%27ambito%20della,operare%20al%20di%20fuori%20dell%27ambiente). This confirms that such use cases are permissible if well-governed. On the international front, the **Apple Card 2019** case is noteworthy, where a credit algorithm was accused of gender discrimination (lower credit limits for women under equal conditions) - US authorities investigated, and although they do not have AI regulations equivalent to the AI Act, the case raised global attention on biases in credit models. This highlights the need for banks to test the model's output ex ante on different demographic sub-groups (gender, ethnicity if available, age) to ensure there are no unjustified disparities (_fair lending test_). In the European context, such checks become an integral part of FRIA and the _ethical AI_ practices also promoted by the EBA.

## **Use Case 2: Contact Centers and Virtual Assistants (Customer service AI)**

**Description:** Use of artificial intelligence systems (e.g., text chatbots, intelligent IVR voicebots) to manage customer assistance in banking. These systems can answer FAQs, provide information on balances and transactions, assist in performing simple operations (e.g., PIN reset), or even propose products (loans, investments) in a conversational manner. They can be based on NLP (Natural Language Processing) and, more recently, on generative models like GPT trained on banking knowledge bases. In some contact centers, AI acts as the first level, with the possibility of escalation to a human operator in case of complex requests or dissatisfaction.

**Regulatory Relevance:** _Not typically high-risk according to the AI Act_, unless the bot performs functions falling under critical categories (rare for a generic assistant). However, the AI Act imposes specific _transparency obligations_: a system intended to interact with people must declare itself as such[[18]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Therefore, the customer must be clearly informed that they are speaking with a virtual, not human, agent. Furthermore, if the AI contact center incorporates **emotion recognition** functionalities (analysis of voice tone to infer the customer's mood) - sometimes proposed for call routing based on sentiment - the AI Act classifies it as a high-risk practice (in contexts such as employment, emotion recognition is even prohibited)[[51]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=For%20some%20uses%20of%20AI%2C,of%20predictive%20policing%20for%20individuals). In a non-employment customer service context, it is not expressly prohibited, but if used, it must be managed with extreme caution and by informing the user that emotional characteristics are being analyzed[[18]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Therefore, the context primarily mandates compliance regarding transparency and respect for consumer rights.

**Privacy & DPIA:** Depends on the functionalities. If the chatbot processes personal data (almost certain, given it accesses customer account data upon request, etc.) and potentially performs profiling (e.g., interpreting requests to offer targeted products), it could trigger the need for a DPIA. Per se, a Q&A virtual assistant based on customer-provided data falls under _large-scale processing of data subjects' data and use of innovative technology (AI)_ - two criteria that, when combined, suggest a DPIA[[52]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=7,la%20concessione%20di%20un%20finanziamento)[[53]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=particolari%20misure%20di%20carattere%20organizzativo,compresi%20i%20trattamenti%20che%20prevedono). Furthermore, if there is also conversation monitoring or sentiment analysis (which is _communication surveillance + potentially sensitive data_, such as stress or emotions), a DPIA is strongly recommended. Aspects to evaluate in the DPIA: information security (dialogues contain confidential banking data, which must be protected with encryption and access controls); risk of errors or model _hallucinations_ (a generative model could provide financially damaging incorrect answers - e.g., wrong information on bank transfers - impacting the customer); consent management for call/chat recording (call centers usually inform about recording for quality purposes - if AI processes voice, it's additional processing to be covered). Important: if the bot makes decisions affecting contracts (uncommon: it usually executes customer instructions, not decides on their behalf), then Article 22 applies. Generally, the AI contact center assists but does not "decide," so GDPR Article 22 is not activated; however, the customer must always be able to request to speak with a human (this could almost be configured as an "analogous right" in terms of user experience, even if not legally based on Article 22). The DPIA should also cover compliance with **banking secrecy**: conversations with the AI contain confidential data; the algorithm and its maintainers must not violate confidentiality (e.g., if an external cloud service is used for NLP, is it an international data transfer? this needs to be evaluated).

**Additional Obligations/Controls:** Beyond the AI Act/GDPR, consumer regulations apply: the **Consumer Code** requires commercial practices to be fair and not misleading – the chatbot must provide truthful, up-to-date, and understandable information. If the bot proposes contracts (e.g., a loan), it could be considered a commercial communication or a contractual offer: it must therefore comply with pre-contractual transparency rules (Information Sheets, etc.). It is unlikely that a generative AI would be left to explain mortgage conditions without supervision, but if it were, the bank would remain responsible. In Italy, regulations such as Article 8-ter TUB (for banking customer assistance) imply that non-human response systems must not exacerbate the obligations to respond to complaints within defined timeframes: therefore, if the AI partially handles complaints, it must adhere to procedures. From an ICT perspective, Bank of Italy guidelines on security and continuity require that systems – including innovative front-end ones – have adequate continuity plans: an AI contact center must have fallbacks (e.g., if the bot doesn't understand or is offline, a human operator must take over or an apology message with an invitation to alternative channels must be provided). If the AI is "trained" with transcripts of past conversations, the legal basis for reusing that customer data must be evaluated – typically this is done anonymously/aggregated, otherwise it is necessary to include in the privacy policy that customer contact data may be used to improve AI services.

**Oversight and Monitoring Measures:** Even if this use case is not high-risk, _human-in-the-loop_ is still important for service quality. Best practice: arrange for **at least X% of conversations** (especially those concerning transactional operations) to be reviewed retrospectively by quality control personnel, to correct any incorrect answers provided by the AI and improve the system. Implement continuous monitoring metrics: rate of misunderstood requests (fallback to human), rate of negative feedback/low ratings from customers post-chat, common error types. These metrics should be reported in dashboards to the customer care manager and the _AI governance team_. Real-time oversight: if the user expresses frustration or enters keywords (e.g., "I want to speak to an operator"), the system must immediately transfer the chat/call to a human. Furthermore, specifically train second-level personnel on how to detect if the bot has made informational errors, in order to immediately rectify the information given to the customer. From a technical standpoint, for generative models, it is useful to include output _validation_: e.g., if the customer asks for an account balance, the bot must use the internal API and return the exact number, not "invent" it; for this, architectures like Retrieval-Augmented Generation (RAG) can be adopted, ensuring that the AI provides only answers based on reliable sources (banking database). The AI Act requires logging transparency: chats and recordings should be retained (after informing the customer) for potential audits – this already happens for traditional call centers (recordings for quality purposes).

**Practical Examples:** Many banks have launched chatbots: e.g., _Intesa Sanpaolo_ with its in-app virtual assistant, _UniCredit_ with its website chatbot, etc. An interesting case is **Widiba** (an Italian online bank) which in 2018 launched a voice virtual assistant ("Widdy") integrated with smart speakers, but withdrew it shortly after: it was found that customers preferred to interact via chat or app and there were privacy concerns (dialogues on devices like Alexa). This teaches the importance of testing the AI channel's adherence to user preferences and trust perception. On the regulatory front, the **Spanish Data Protection Agency (AEPD)** has published guidelines on chatbots indicating that they must clearly inform the user and that the user retains full GDPR rights (including knowing if they are speaking with AI). Furthermore, some media cases, e.g., _Bank of America_'s chatbot ("Erica"), have shown that the key to success is clearly defining the scope: Erica answers financial queries but does not give personalized investment advice (to avoid MiFID compliance risks). Therefore, restrict the AI's _scope_ and set boundaries (our AI contact center could be configured not to answer sensitive or legal topics, immediately directing to a human).

## **Use Case 3: Anti-Money Laundering and Transaction Monitoring**

**Description:** Use of AI systems (in particular machine learning, including deep learning) for analyzing financial transactions and identifying suspicious activities related to money laundering or fraud. Traditionally, banks use fixed rules and predefined scenarios (e.g., threshold amounts, certain known patterns) in AML systems. Advanced AI promises to detect even anomalous, non-predefined patterns ("_unknown unknowns_") through anomaly detection or classification algorithms trained on historical reporting data. For example, a model can learn a customer's normal operating profile and flag atypical deviations. Or it can correlate data from different sources (movements, risk profiles, OSINT info) to raise targeted alerts.

**Regulatory Relevance:** _Not classified as high-risk by the AI Act,_ because Annex III only covers AI uses in public _law enforcement_ activities. AML is in a hybrid area: the _AML Directive_ imposes controls on transactions and reporting of suspicions to the FIU (Financial Intelligence Unit) on obliged entities (banks), but the AI Act does not consider the bank as an enforcement authority. On the contrary, recital 59 specifies that financial intelligence units with administrative analysis tasks are not included in high-risk police uses[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%C2%A0a%C2%A0essere,L%27impatto%20dell%27utilizzo). Therefore, the use of AI for AML falls under "non high-risk" systems (unless used in direct collaboration with law enforcement for investigations, a non-standard scenario). This means no mandatory ex ante AI Act certification or mandatory FRIA _by law_. However, from an impact on rights perspective, an AML algorithm can temporarily freeze operations or report clients to the authority, with potentially serious effects (account freeze, investigations) - so _the bank should prudently conduct an impact assessment_ (voluntary FRIA or similar extended DPIA) to ensure that the system does not violate the rights of unaware individuals. Furthermore, the AI Act expressly prohibits the use of AI for _massive profiling in a police context_ and other invasive practices: a borderline AML system (that profiled all clients in a criminal key) must still respect the principles of _necessity and proportionality_.

**Privacy & DPIA:** It is very likely that a DPIA is necessary. Systematically monitoring the transactions of all customers in search of illicit activities is a classic example of _"systematic large-scale surveillance"_[\[54\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=profilazione,del%20volume%20dei%20dati), combined with behavioral profiling potentially involving sensitive data (e.g., transfers to religious organizations, medical expenses can reveal sensitive data). The French supervisory authority (CNIL) has previously indicated that anti-money laundering systems fall within the lists of treatments to be assessed. The Italian supervisory authority in its 12 categories mentions _"treatments for fraud prevention"_ and _"interconnection of consumption data with payment data"_[\[55\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=8,personali%20raccolti%20per%20finalit%C3%A0%20diverse)[\[56\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,ovvero%20della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di) - both relevant to AML - as cases to be examined. So, a DPIA is required, focused on: lawfulness (the bank has a legal obligation for AML, so the legal basis is compliance with a legal obligation - Art. 6(1)(c) GDPR, exempt from consent), minimization (use only necessary data for patterns: however, AML tends to _maximize_ data to discover correlations - finding a balance), storage limitation (unconfirmed alert data must be deleted after a period, etc.). The DPIA must also consider _false positives_: a high number of unjustified reports can damage the reputation and rights of clients (who may suffer service disruptions). A measure should therefore be put in place to mitigate this (e.g., reasonable thresholds for alarms, human verification before reporting to the FIU). From a privacy perspective, AML is in balance with anti-money laundering regulations: GDPR allows processing without consent and potential limitations of rights (e.g., the right to information can be limited so as not to alert the suspect, according to Art. 23 GDPR implemented by Legislative Decree 231/2007). Therefore, the client will not be told "we are profiling you for AML," and this is lawful by law - but internally the bank must carefully monitor access to this data and ensure that there is no excess (the logic of _"data protection by design"_ is complex here: it is necessary to track who sees the alerts, etc.).

**Additional Obligations/Controls:** Very strong sectoral regulation: Legislative Decree 231/07 (transposition of AMLD) requires banks to apply a **risk**-based approach (risk-based approach) in their anti-money laundering function. The use of advanced models is implicitly encouraged, provided the bank understands their results and incorporates them into its risk assessment. Supervisory authorities (UIF, Bank of Italy) expect that the use of IT tools does not replace the judgment of the AML compliance officer. There are obligations of _retention_ (keeping track of all reports and analyses for 10 years) and _confidentiality_ (not disclosing to the reported client). An AI algorithm that automatically reports to the UIF should be calibrated to minimize egregious errors – because the UIF can then request explanations. It must therefore be possible to explain (at least internally) why the model flagged X -> this is critical: many ML models are not easily explainable; the bank may need an _explanation_ module for each alert (e.g., highlighting the anomalous transaction sequence that led to the high score). From an _auditing_ perspective, the internal control body and auditors will inspect the AML process: it is necessary to document the AI methodology, the validation performed (to ensure it does not "miss" any known risk types), and the results. If the AI system replaces parameters of the _Bank of Italy Provisions_ on simplified vs. enhanced due diligence, etc., it is necessary to verify compliance with the regulatory framework: AI can help automatically assign client risk classes (low/medium/high risk) based on transactional data as well as client type, but the methodology should be validated and approved by the AML Officer and subject to continuous updates.

**Oversight and Monitoring Measures:** Recommended approach: _AI as assistant, not decider_ in AML. Therefore: the algorithm generates _internal alerts_ ("alerts"), but a **human AML analyst reviews them all** and decides which ones to forward as an official Suspicious Transaction Report (STR) to the UIF. This already happens with static scenarios (the analyst filters many false positives generated by rules); with AI, the volume and nature of alerts might change, but the principle of dual control remains fundamental (and is effectively required by AML regulations). Furthermore, _monitoring of the model's effectiveness_ is needed: metrics such as % of AI alerts that become actual STRs sent (precision), % of actual STRs originated by AI vs. other channels (recall), and comparisons with previous years – if AI drastically reduces or excessively increases STRs, it must be investigated. Oversight by the _AML Compliance Officer_: this person must be able to understand the system, have the possibility to intervene on parameters (e.g., raise sensitivity threshold if it is flagging too much, or add a specific control if regulations change – if it's a black box model, foresee mechanisms to integrate it with additional rules). Implement a periodic process (at least annually) for model review with stakeholders: IT, Compliance, Data Scientists, and also legal representatives to assess whether the model is meeting requirements and if new risks emerge (e.g., the model might "learn" to ignore certain common but potentially legitimate transactions – risk of _blind spots_).

**Practical examples:** Many banks are evaluating AI in AML: e.g., _ING_ has declared the use of ML to detect suspicious transactions, improving the signal; _Swedbank_, after a scandal of non-detection, has invested in AI. In Italy, the **Bank of Italy** launched a machine learning project (_"Gianos"_) in 2021, applied to aggregated anti-money laundering reports data to help the UIF identify hidden phenomena – a sign that authorities also see AI as useful, but with great caution. A concrete case: _HSBC_ used AI to cross-reference international transaction data and social networks in the famous _"FXogle"_ project, discovering patterns of currency fraud; however, this raised privacy issues (use of non-financial personal data). From a regulatory perspective, an interesting fact: in the United Kingdom, the regulator (FCA) organized _TechSprints_ on AML, highlighting that black box models are problematic for auditors and preferring interpretable models (_white box AI_). Therefore, a possible _shift_ is to favor more explainable algorithms (e.g., decision trees, Bayesian networks) over pure deep learning, at least as long as regulations require explanations. For the Italian sandbox, in the first window, _Vidyasoft_ tested "Hands-Free SCA" for payment fraud using AI[\[57\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20un%20servizio%20evoluto,durata%20massima%20di%2018%20mesi) – a slightly different area (Strong Customer Authentication with AI exemptions), but related: the final sandbox report (second window) is not public, but the general lesson is that AI can operate if integrated with controls like those mentioned above.

## **Use Case 4: Recruitment and HR Management with AI**

**Description:** Application of AI systems in the selection, hiring, and personnel management process within banks. It includes tools for _automatic CV screening_ (e.g., software that filters candidates based on requirements and scores, or that uses NLP to analyze cover letters), _video-interview analysis_ systems (algorithms that evaluate candidates' facial expressions, tone, and language in video interviews), up to AI that proposes a shortlist of candidates or even provides a suitability score. Other applications in HR include: tools for employee performance evaluation, predictive churn analysis (who might leave the company), shift optimization. Here, we focus on recruitment and internal mobility, as it is mentioned as a target area.

**Regulatory Relevance:** _High-risk AI Act_. Annex III §4 includes _"AI systems intended to be used for the recruitment or selection of natural persons, for making decisions on hiring, promotion, termination, task allocation, or evaluation in a work-related context"_[\[58\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%C2%A0i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei). Clearly, therefore, an ATS (Applicant Tracking System) with AI falls within this scope. Consequences: AI Act requirements are fully applicable (training data to be checked for quality and bias, technical documentation on the model, security measures, etc.). The ATS software provider will have to ensure the CE certification of the system as compliant. The bank, as a user, will have to carry out the FRIA (Fundamental Rights Impact Assessment) before using such systems on candidates/workers[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti), given that deployers of Annex III point 4 systems are presumably included (as employers). Furthermore, the AI Act emphasizes the risks of discrimination in the employment context: the recitals note how such systems can replicate historical discrimination (against women, age groups, ethnicities) and impact life opportunities[\[59\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=lavoratori,I%C2%A0sistemi%20di%20IA). Therefore, the FRIA will particularly have to assess fairness and impact on labor rights (e.g., right to equal opportunities, non-discrimination in the selection phase, dignity). We also point out that some AI HR practices may intersect with prohibitions: _e.g._, the AI Act **prohibits** the use of emotion recognition systems in the workplace[\[60\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=will%20be%20banned%20from%20the,of%20predictive%20policing%20for%20individuals) - this concerns, for example, software that analyzes emotions during tests or in the workplace (this is _banned_, so the bank could not lawfully use a system that monitors employees' emotions to evaluate them). Another prohibited practice: _generalized social scoring_ - not common in HR, but it means avoiding systems that profile a worker's "reliability" based on non-work-related parameters (e.g., private life).

**Privacy & DPIA:** Mandatory in 99% of cases. Processing candidate or employee data with automated evaluation systems falls under profiling and monitoring. GDPR, Art. 88, allows Member States to provide safeguards for processing in the employment context: Legislative Decree 101/2018 in Italy refers to Art. 113 of the Privacy Code, which entrusts the Garante (Data Protection Authority) with defining measures. The Italian Garante and the EDPB have clarified that automated decisions in an HR context are particularly sensitive: normally, _consent_ or another legal basis is required, and they must not violate the Workers' Statute (Art. 4 of Law 300/1970 on controls). A DPIA will therefore evaluate: legal basis (e.g., candidate's consent to automatic screening, or legitimate interest? Consent risks not being "free" in an employment context; generally, a legitimate interest basis balanced with safeguards is preferred); necessity - the bank must demonstrate that AI improves objectivity and efficiency without infringing rights; risks of errors (erroneously rejecting the ideal candidate) and how to remedy them (e.g., still having human recruiters review a sample of rejected CVs for control). Important: if there is _pure automated decision-making_ concerning a candidate (e.g., "the system excludes X without human intervention"), this triggers Art. 22 GDPR - typically, employers in the EU avoid purely automated decisions, including an HR professional in the final decision of whom to invite for an interview, precisely for compliance. If, for argument's sake, the bank wanted a fully automated decision (highly unlikely due to reputational and regulatory risks), it would need to obtain explicit consent from the interested candidates - a complex scenario (the candidate might feel obliged to give it). Furthermore, the right to obtain human intervention (a recruiting manager reconsidering the application upon request) should be guaranteed. From the data perspective: CVs and cover letters often contain _sensitive personal data_ (photos = biometric data if face recognition is used; text can reveal ethnic origin, age, family status, which are sensitive aspects in HR due to anti-discrimination regulations; also any health or criminal data if the candidate mentions them). An AI reading them must ignore irrelevant information, and the bank must configure the system not to take protected attributes into account. This may involve _bias mitigation_ techniques, for example, having anonymous CVs analyzed (removing name, gender, age). The DPIA must be coordinated with labor regulations: Art. 4 of the Workers' Statute prohibits unauthorized remote monitoring of employees - if AI were used to monitor performance (e.g., email analysis, chat, productivity), a trade union agreement or authorization would be required. In recruiting, this does not apply to candidates (they are not yet employees), but it does apply to internal mobility (an employee applying for another position - their data used by AI must be protected as employee data).

**Additional obligations/controls:** Anti-discrimination rules in recruitment: in the EU and Italy, direct or indirect discrimination based on sex, age, ethnicity, religion, disability, orientation, etc., is prohibited. An AI recruiting system is subject to such laws (Legislative Decree 216/2003 in IT implementing the equality directive). If it emerged that the recruiting algorithm systematically excludes people of a certain age group or gender, the bank could be sued for discrimination, even if the bias is "unintentional" (the concept of _disparate impact_ would apply). Therefore, a derived obligation is: to test and ensure that AI criteria are job-related and do not exclude protected categories in a disproportionate percentage, unless there is a justified reason. For example, an algorithm that penalizes long employment gaps could disadvantage women (maternity): it must be calibrated or justified as essential. Furthermore, the bank must comply with transparency obligations towards candidates: the Labor Transparency Decree (Legislative Decree 104/2022), transposing EU Directive 2019/1152, _art. 4, paragraphs 5-6_, requires the employer to inform the worker (or candidate, if interpreted broadly) about the use of automated decision-making or monitoring systems during the pre-employment phase or within the employment relationship. In particular, if algorithmic tools are used that affect recruitment, the candidate/employee has the right to be informed about the functioning of such systems, their main parameters, the objectives pursued, and the evaluation logic. This is very important: since August 2022 in Italy, employers must provide written information _also for recruiting algorithms_ and allow the worker to ask for clarifications. The AI Act will add registration in the database (if a public body, that use of AI will have to be made public). Therefore, there is an intertwining of obligations: the bank will have to prepare a sort of "selection algorithm transparency sheet" to be provided if requested (an anticipation of that obligation is already pushing companies to disclose whether they use AI in HR). In addition to this, related privacy regulations: if the system performs video analysis (facial biometrics on video interviews), specific consent or a strong legal basis and authorization from the Garante (Data Protection Authority) are required, because biometric and judicial data (criminal records) are subject to special regimes.

**Oversight and monitoring measures:** Core principle: _final human decision_. AI can rank CVs, but a recruiter must be able to modify the ranking, include candidates that AI has excluded if they are deemed valid (_override_), and document why. Occasionally, the HR team should conduct qualitative checks: take a sample of discarded CVs and see if there were potentially valid candidates unfairly rejected – if so, analyze the reason (model feature that penalized that aspect). For example, discovering that AI penalizes those living >50km from the office (assuming a lower probability of accepting the job) – is this legitimate? It could be discriminatory towards those living in Southern Italy applying for jobs in the North (indirect discrimination based on origin). That criterion should then be removed or attenuated. Therefore, establish a process of _periodic bias audit_, perhaps also involving the company's Diversity Manager. During implementation, ensure that the model is trained on "clean" data: if historical data reflects a bias (e.g., in the past the company hired very few women in IT), the model will replicate it. It might be useful to apply re-balancing techniques (giving greater weight to minority cases). Oversight can also be external: inform trade unions or the internal Single Guarantee Committee about the use of AI in selection, and agree on principles (for example, a company principle could be: "no candidate is ever excluded solely by the algorithm, unless after human verification"). Continuous monitoring: measure whether the composition of hired staff has changed post-implementation (e.g., has AI unintentionally increased or decreased diversity?). If negative trends emerge, intervene. From the security/data protection side: log access to the AI recruiting system (who consulted profiles, etc.), and ensure that after the search is concluded, data of unhired candidates is stored only for the lawful period (usually max 6-12 months unless consent for future opportunities). The AI Act will also require the user (bank) to register the use of HR AI in the database (if a public entity or one providing public services - banks are not, but if they participated in public job placement programs perhaps; in any case, the FRIA will be documented).

**Practical Examples:** Famous case: **Amazon Recruiting Tool** - an algorithm that scored CVs for IT roles, was discontinued because it was found to be sexist (it had learned from historical data that most hires were men and penalized female keywords in CVs)[\[61\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=I%20rischi%20non%20sono%20teorici,economiche%20e%20sociali%20che%20comportano). This is often cited as a warning: biased training feed → biased output. After that episode, many companies slowed down on AI HR or focused on transparent models. In the EU, the start-up _HireVue_, which offered AI video interviews, was criticized by authorities and advocacy groups, leading the company to remove facial/emotional analysis and retain only verbal content analysis (less intrusive). **Germany** has a cautious approach: the Betriebsrat (works council) often requests to approve the use of automated selection software. With the AI Act approaching, large groups like Deutsche Telekom have publicly stated that they are testing their HR algorithms to eliminate possible biases and are preparing for _"Algorithmen TÜV"_ (certification). In Italy, some banks have started using online aptitude tests with automated scoring, and HR then uses these as one of the factors: this mitigation (AI as an "additional score" and not the sole factor) is considered safer. Since 2023, with the Transparency Decree, we have seen some companies include phrases in their candidate information notices such as: "Your CV may be subjected to pre-analysis by automated systems, with the final evaluation remaining with the recruiter" - a sign that disclosure is already necessary. We will probably see controls in future audits on how AI makes decisions in HR: banks would do well to be proactive, documenting everything (for example, keeping _HR model training data_ archived and impact analysis, so they can present them if the Garante or labor inspectorate request clarification on selection criteria).

## **Use Case 5: KYC and Document Verification (customer onboarding)**

**Description:** AI systems employed in the customer due diligence process (_Know Your Customer_) and in document authentication/validation. Typically, during the digital onboarding of a new customer, identity documents, photos/selfies, and potentially videos are collected; AI can be used to: recognize the type of document and automatically read its data (intelligent OCR), verify the authenticity of the document (document analysis AI that checks security patterns, fonts, photos vs holograms, etc.), compare the document photo with the customer's selfie (**biometric face matching** to ensure it is the same person), check sanction lists or PEPs in the background (again, with name-matching algorithms), and generally validate whether the provided data meets regulatory requirements. Furthermore, AI can be used to extract information from documents such as pay slips, utility bills, and company registration certificates presented by the customer, simplifying due diligence.

**Regulatory Relevance:** _Not expressly in the high-risk AI Act_, unless it includes biometric elements with purposes attributable to public control. The use of **biometrics for identity verification** in a private context (bank onboarding) is not classified as such in Annex III. The AI Act addresses biometrics for remote identification in public places by law enforcement (a different matter, often prohibited except for exceptions) and _document verification_ is mentioned as excluded for migration contexts[\[62\]](https://artificialintelligenceact.eu/annex/3/#:~:text=,the%20verification%20of%20travel%20documents). Therefore, a 1:1 face matching system (selfie vs. document photo comparison) does not fall under high-risk according to a strict interpretation. However, there is room for interpretation: some might argue that ensuring certain identity is _essential for access to financial services_, and an error here can impact security (e.g., account takeover). But the AI Act has not included it in category 5 (which covers credit, insurance, etc.). Therefore, there is no high-risk certification obligation for the face recognition software used by the institution, nor is a FRIA mandatory by law. _However_, it must be considered that KYC involves very sensitive data (identification documents, facial biometrics) and fundamental rights such as privacy and the right to personal identity - a _voluntary FRIA_ would be appropriate to assess how AI impacts (e.g., risk of exclusion: a poorly calibrated document verification system could reject legitimate customers, perhaps more often belonging to certain nationalities with less known documents, creating disparities). Furthermore, the AI Act prohibits biometric categorization to infer sensitive data (e.g., the photo could not be used to deduce ethnicity or age for unnecessary purposes) - this is not the goal of KYC, but to ensure that the software provider does not do things beyond simple verification. In summary, KYC AI is **limited risk AI Act**, but with general transparency obligations if it interacts (here the customer knows they are being recorded, so it's okay) and with strong privacy requirements.

**Privacy & DPIA:** Almost certainly yes. The Garante (Italian Data Protection Authority) has included _"systematic processing of biometric data"_ among those requiring a DPIA[\[63\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=relativi%20a%20condanne%20penali%20e,della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di%20trattamento). A video identification procedure with facial recognition falls into this category (it processes faces on a potentially large scale). OCR of ID documents also touches critical identification data (document number, etc.) and can fall under "extremely personal data" (including identity documents) on a large scale[\[64\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=tramite%20reti%20o%20di%20sorveglianza,Big). The DPIA will assess: legal basis (in most cases, customer identification is an AML legal obligation, thus legal basis Art. 6(1)(c) GDPR - the Italian Garante on SPID and CIE has stated that facial recognition can be based on a regulatory obligation since the law provides it as a possible method, or on legitimate interest if not mandatory but offered as a convenient option); it needs to verify that the _minimum necessary_ is used: e.g., many systems record a short video of the user showing a document and making movements (liveness detection), which generates a lot of biometric data - the DPIA analyzes whether less invasive alternatives exist (e.g., only photo + SMS OTP? etc.). Particular attention to **consents**: for biometric data, GDPR requires a specific basis (Art. 9); here the main basis is the fulfillment of anti-money laundering obligations, which can also cover the acquisition of photos and documents. The Garante, with provision 2021 on facial recognition for FEA, had required explicit consent; for video onboarding, it has so far allowed it as an AML measure (e.g., the IVASS 2021 guide and Bank of Italy circulars 2019 on video recognition have permitted it with measures). DPIA also on: _algorithm accuracy_ - false rejection rate, especially for certain groups (studies show that face recognition can have higher error rates for people of color or very young/elderly individuals). This must be considered and mitigated: e.g., providing alternative identification opportunities for those who are erroneously not recognized (such as manual intervention by an operator). _Security:_ these systems collect ultra-sensitive data (face videos, document images) - strong encryption protections, secure channels, and limiting access (e.g., only compliance personnel). _Retention:_ the identification video must be kept for the time required by AML regulations (5 years from the end of the relationship) then destroyed. _Data subject rights:_ here it clashes with AML - generally, the customer cannot object to identification (they either accept that onboarding or do not open an account), but they must be informed a

adequately about the process (with clear information explaining that there is automatic document analysis and face recognition). If the bank wanted to make an automatic decision ('your identification has failed, contract not opened'), it falls under Article 22 - mitigated by the fact that in practice if the AI fails, there is usually a fallback such as 'repeat the procedure' or 'go to a branch'. However, including human intervention in case of failure is strongly recommended to avoid problems.

**Additional Obligations/Controls:** The specific regulations of the Bank of Italy and IVASS allow _remote video identification_ provided that technical guidelines are followed (Bank of Italy Circular 285 updated 2020, IVASS Provision 97/2020). These guidelines often prescribe: the use of recognition systems with adequate security (SPID level 2 or 3), the presence of _liveness_ checks (to prevent spoofing with photos or deepfakes), retention of recordings, human intervention in doubtful cases. Therefore, the bank must ensure that the AI used meets these requirements. For example, for liveness, AI is used that asks the user to turn their head or read 3 digits: the regulation requires that if the algorithm is not 100% confident, an operator rechecks. Furthermore, anti-terrorism and anti-fraud regulations: the bank must verify the document against lists of stolen/lost documents (often integrated into the AI document analysis). The system must generate evidence for any inspections: if the bank is accused of having incorrectly identified a fraudster, it must be able to show logs: "the system erroneously validated document X; here's how it happened". To this end, define performance KPIs and manual re-verification thresholds: e.g., if face-document match confidence < 90%, have an operator intervene instead of blindly rejecting or accepting. As it is not a high-risk AI Act, there is no obligation for database registration, but being linked to anti-money laundering, the bank will still have to notify the Bank of Italy of the adoption of innovative procedures (often informally requested during supervision, to understand how it complies with requirements).

**Oversight and Monitoring Measures:** Here, oversight means: ensuring that when the AI says "valid document, person matches," the decision can be reviewed if necessary. Many banks already stipulate that a _back-office operator_ rechecks a sample of cases automatically approved by the system, especially initially (post-hoc human validation, to "calibrate" the AI). If false positives emerge (fake documents accepted), it must be immediately reported, and the model retrained or additional rules implemented. At the same time, monitor _false negatives_ (real customers rejected): e.g., if many users with an ID card from a certain country fail verification, perhaps the AI does not know that document well → add samples and improve. Implement feedback loops: e.g., if a user contacts support because they cannot be recognized, that event must be recorded and the cause analyzed. Oversight requires expertise: the compliance officer or onboarding manager must be able to interpret the model's _confidence scores_ and decide thresholds. For example, deciding that below 80% matching the case is rejected, between 80-90% a human reviews it, above 90% is auto-OK. These policies must be reviewed periodically based on real results. Another control: integration with anti-fraud - if an opened account later turns out to be fraudulent, conduct a _post-mortem_ to understand if the AI should have detected it (perhaps the fraudster used a real document but not their own, and the AI failed the match? Study and improve). Therefore, a team (IT + compliance + risk) should review monthly statistics: number of automatically vs. manually completed onboardings, reported errors, etc. Document these controls for any audits (including internal Anti-Money Laundering function audits).

**Practical Examples:** In Italy, almost all banks offering online account opening use document verification and face recognition technologies (provided by vendors such as Experian, Onfido, InfoCert, etc.). The Italian Data Protection Authority has approved SPID/CIE guidelines that use facial recognition to identify individuals remotely, setting safeguards: specifically requiring the algorithm to be highly accurate and that the user be given an alternative if recognition fails (e.g., in-person verification or a procedure with a video chat operator). This applies similarly in banking: often, if self-onboarding fails 2-3 times, users are contacted by an operator or invited to a branch. A notable case: _2019, online account scams_ - criminals exploited flaws in video recognition systems using faces of lookalikes; this prompted the Bank of Italy to strengthen guidelines on liveness detection. From a customer experience perspective, _Intesa Sanpaolo_ introduced facial recognition in branches to pre-fill data from documents: it was authorized by the Garante with safeguards (data not stored after use). Again, this demonstrates feasibility but with a DPIA submitted. In the market, famous Face Recognition errors across different ethnicities (e.g., higher false negatives on dark-skinned faces in some algorithms) serve as a warning: banks must ask providers for evidence of cross-demographic tests and perhaps conduct internal tests on their own customer base (also considering that many banks have foreign customers). Regarding the AI Act, even if KYC AI is not high-risk per se, the upcoming European _"Digital Identity Wallet"_ and related eIDAS regulations will standardize identity processes: banks will need to ensure that their AI systems integrate with these and comply with certified standards (e.g., qualification of identity services: it is likely that providers of video onboarding solutions will seek voluntary certification schemes). Therefore, it is advisable to already lean towards AI KYC solutions from known providers that follow industry standards (ISO/IEC 30107 for anti-spoofing, etc.). The _MEF 2021 Fintech sandbox_ admitted an O-KYC project[\[39\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi) based on DLT for sharing KYC info: although not centered on the use of AI, it indicates an interest in innovation in this field. A future scenario is to use AI for continuous KYC updates (monitoring document anomalies over time, etc.), which will be linked to the AML use case.

# Remaining Open Questions

- **Practical criteria for "significant risk" (Art.6(3) AI Act):** it is unclear how a provider or user will be able to demonstrate that a system falling under Annex III **does not** present a _significant risk_ and thus exempt it from the high-risk regime[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III). Guidelines on risk metrics and who validates such self-assessment will be needed to avoid arbitrariness and under-classification.
- **Coordination between Authorities (AI Act vs privacy vs sector):** who will practically be the reference authority for supervising banking AI systems? The _Market Surveillance Authority_ for high-risk AI could be Banca d'Italia (banking supervision) or a new entity; the Privacy Guarantor will maintain its role on DPIA and data protection; the EBA/ECB will have a say through the _AI Board_. Protocols are needed to avoid conflicts or gaps in competence in ex-ante assessments (FRIA) and ex-post controls.
- **Reference standards and certifications:** given the strong technical nature of the AI Act requirements, it is wondered what technical standards banks will adopt to demonstrate compliance (e.g., ISO 42001 AI Management? bias/fairness certifications?). EBA notes that the Commission will issue high-risk classification guidelines by Feb 2026[\[65\]](https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/#:~:text=EBA%20Factsheet%20%E2%80%93%20AI%20Act%3A,banking%20and%20payments%20sector%2C%20by), but operationally banks would prefer a unified framework. The open question: _is it better to wait for official standards or start with voluntary certifications (e.g., ethical audits, third-party attestations) to stay ahead?_
- **DPIA-FRIA integration:** how to practically implement a single process that covers both? Should two separate reports be produced (one for the Guarantor, one for the AI Act authority) or will a single _"AI Risk Assessment Report"_ suffice for both purposes? And in case of an impact assessment with a doubtful outcome (high residual risk): for GDPR there is an obligation to consult the Guarantor, for the AI Act it is not foreseen but perhaps the Market Authority can intervene; it would be necessary to understand how to align these escalations.
- **Third-party vendor management and liability:** if a vendor provides a non-compliant AI system and the bank suffers a violation (e.g., a fine for discrimination), who bears the responsibility? The AI Act regime foresees primary responsibility of the _provider_ for technical requirements and of the _user_ for improper use. But in real contracts between banks and vendors, robust clauses on guarantees, indemnities, and access to information (audits) will be needed. The open question: will banks be able to contractually ask vendors to conduct and share a FRIA as a _provider_? Or will each bank have to do it alone even for standard packages?
- **Fairness metrics and acceptable thresholds:** will regulators require banks to quantify and maintain certain levels of fairness in models (e.g., _"disparate impact ratio"_ no worse than 80%)? Or will everything remain qualitative? The absence of unambiguous quantitative criteria is a problem: one bank might consider a slight deviation acceptable, another might not. It is wondered whether the EBA or the AI Office will develop guidance in this regard.
- **Use of sensitive data for ethical purposes (bias correction):** a known paradox - to test if a model is discriminatory, it would sometimes be necessary to consider the protected variable (e.g., gender) in the data; but this is prohibited by decision. Will the Guarantor allow the use of simulated or post-hiring collected sensitive data to validate fairness? There is uncertainty on this, and banks struggle to define bias audit methodologies that comply with GDPR.
- **Interoperability with future regulations (ESG, AI liability):** how will AI Act requirements combine with other emerging ones, such as initiatives on _AI liability_ (civil liability for damages caused by AI) or ESG regulations (which might include ethical use of AI)? Banks will also need to map these aspects - and the open question remains whether the prepared documentation (e.g., AI event log, decision logs) could be used against the bank in civil lawsuits (an unresolved liability issue: too much transparency could expose to litigation).
- **Role of the** EU AI Office **vs National Authorities:** the AI Office will have supervisory powers mainly over foundation models and high-risk cross-border. But will it be able to issue binding guidelines also for regulated sectors? Banks will need to keep an eye on possible additional supranational indications. The question: if the AI Office (Commission) identifies a banking model as _"high impact"_ GPAI, could it intervene directly? This needs clarification.

- **Continuous tool update vs. evolving regulations:** recognizing that interpretations and practices surrounding the AI Act and DPIA will evolve (case law, EDPB guidelines, new regulatory amendments), how can the Navigator be kept always up-to-date? The desk research process has limitations - some issues will only be resolved through a _regulatory feedback loop_ (e.g., first FRIAs actually carried out, sanctions imposed, etc.). The topic of tool governance is open: who in the company (or ABI Lab consortium) will update it with _lessons learned_ and new regulatory sources as they emerge? This determines the long-term sustainability of the proposed solution.

# Annotated Bibliography

- **Regulation (EU) 2024/1689 "AI Act"** - Fundamental regulatory text adopted in June 2024[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti). Defines the EU regulatory framework for AI with a risk-based approach. _Relevance:_ Cited annexes and articles identify high-risk banking cases (credit, HR) and impose obligations (e.g., Art. 14 human oversight, Art. 27 FRIA). Basis for many Navigator compliance actions.
- **AI Act Recitals (58) and (57)** - Introductory parts of the Regulation explaining the reasons for including credit and employment among high-risk uses[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[58\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei). _Relevance:_ highlight the risks of discrimination and social impact of such systems, providing justification (to be cited in FRIA) and recalling exceptions (fraud detection excluded[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi)).
- **Italian Data Protection Authority, Order no. 467/2018 (DPIA Annex)** - Italian resolution listing types of processing mandatorily subject to DPIA[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento). Includes scoring, automated decisions, biometrics, employee monitoring, etc. _Relevance:_ it is the national regulatory reference to quickly understand if a banking AI project requires a DPIA. The Navigator incorporates its criteria.
- **WP29/EDPB WP248 Guidelines on DPIA** - European guidelines (2017, confirmed by EDPB 2018) detailing high-risk criteria and DPIA methodology[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza). _Relevance:_ provide the checklist of 9 criteria used in the wizard to determine DPIA obligation and suggest best practices on stakeholder involvement, DPIA updates, etc.
- **EU Council Press Release 9 Dec 2023 (AI Act Agreement)** - Council press release[\[66\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=Transparency%20and%20protection%20of%20fundamental,rights) announcing the political agreement. _Relevance:_ contains in clear language the novelties such as the FRIA obligation for deployers and the extension of transparencies (e.g., emotion recognition disclosure). Useful for extracting key concepts and non-technical explanations for users.
- **EBA Risk Assessment Report, "Special topic AI" (Nov 2024)** - Report by the European Banking Authority[\[67\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=Regarding%20use%20cases%2C%20AI%20is,banks%20are%20leveraging%20AI%20in)[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent). Describes AI adoption in EU banks, common use cases (customer profiling, support, fraud detection, credit scoring) and related challenges (skill gap, governance). _Relevance:_ offers data and confirmations on the use of AI in the 5 target use cases and recommendations (e.g., human-in-loop in GPAI[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)). The Navigator uses it to calibrate the "Sector Use Scope" section and to support recommendations for prudence.

- **EBA Chair José M. Campa's Letter (Nov 2025) - Mapping AI Act vs. Banking Regulation**[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of)[\[68\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving) - Document addressed to the Commission examining overlaps between AI Act obligations and financial regulation (CRR, CRD, MiFID, etc.). _Relevance:_ confirms that many requirements (e.g., human oversight, data governance) already have equivalents in existing rules, but no ad hoc exemption is foreseen. Helps the Navigator highlight where an AI Act compliance can be met through existing compliance and where it is additional.
- **Paradigma.it - "AI and Credit Granting: Between Innovation and Responsibility" (Oct 2025 article)**[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario)[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano) - Italian legal in-depth analysis. It synthesizes banking obligations (TUB 124-bis: creditworthiness with adequate information; human verification obligation) and principles of the EBA Guidelines 2020 on loan origination (transparency, traceability, non-discrimination, constant supervision) with a Bank of Italy 2022 comment on the auxiliary role of AI[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). _Relevance:_ provides an authoritative local interpretation of how to balance AI innovation and responsibility in credit decisions. Cited in the Navigator to support the recommendation "AI in support, not in replacement of human judgment" and to frame Italian legal obligations for human control.
- **Bank of Italy - Economics and Finance Working Paper No. 721 "Artificial intelligence in credit scoring" (2022)**[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove)[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in) - Empirical and regulatory study. It analyzes AI benefits/risks in credit, regulatory coverage, and results of a survey on Italian banks. It concludes that existing prudential regulation covers most AI-ML risks (governance, controls), but highlights gaps regarding the principle of non-discrimination and the complexity of translating ethical principles into practice[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove)[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti). _Relevance:_ this internal source from the Italian authority reinforces the importance of governance measures (which the Navigator emphasizes) and anticipates possible requests from the regulator regarding fairness. It is used as a basis for suggesting additional measures in the wizard, even if not strictly required by the legal text, as "good prudential practice."

- **Italian Data Protection Authority - Provision no. 755/2024 (generative AI sanction case)**[\[42\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata)[\[43\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello) - Sanctioning order against a well-known company developing a generative chatbot (presumably OpenAI). It contests the failure to notify a data breach, lack of a legal basis for training personal data, and violation of transparency. _Relevance:_ although not in the banking sector, it highlights the DPA's stringent positions on crucial points: the obligation to identify a legal basis before using data to train models, and the need to inform data subjects even in innovative contexts. The Navigator uses this precedent to warn banks to avoid similar shortcomings (e.g., if they develop models with internal customer data, pay attention to declared purposes and information notices).
- **Directive (EU) 2019/1152 ("Work Transparency Directive") - Legislative Decree 104/2022 (Art. 4)** - Rule requiring employers to inform workers about the use of automated decision-making systems in the workplace. _Relevance:_ specifically cited for the HR case: it obliges banks to disclose to candidates/employees if they use AI in selection or evaluation, with details on logic, factors, and objectives. The Navigator integrates this constraint into the recommendations for the Recruiting use case (e.g., generating an AI information notice for candidates).
- **Legislative Decree 231/2007 (Anti-Money Laundering Legislation) and UIF/Bank of Italy Technical Rules** - Body of legislation governing KYC, customer due diligence, and anti-money laundering controls. It includes the obligation for remote identification according to precise rules and the principle of a risk-based approach. _Relevance:_ for AML/KYC use cases, it defines the legal perimeter within which AI must operate (it cannot lower due diligence standards). The Navigator recalls its obligations (e.g., data retention for 5 years, manual analysis of suspicions) integrating them with technological ones.
- **ISO/IEC TR 24027:2021 (Bias in AI Systems)** and **NIST AI Risk Management Framework 1.0 (2023)** - International standards and frameworks (non-normative) that address methodologies for evaluating and mitigating bias in AI systems and managing AI risks. _Relevance:_ useful as a technical reference for the more advanced user; the Navigator cites them in the annotated bibliography as resources for concretely implementing fairness and risk management guidelines (for example: _"ISO 24027 provides a taxonomy of biases and approaches to measure them"_). This helps IT/Data Science teams translate general recommendations into actions.
- **EBA Guidelines 2020 on Loan Origination & Monitoring (EBA/GL/2020/06)** - Binding guidelines issued by the EBA[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). They require, among other things, that the use of automated models in the credit granting process respects principles of non-discrimination and incorporates human judgment. _Relevance:_ directly applicable to EU banks, they complete the credit regulatory framework. The Navigator incorporates them into the credit sheet (e.g., on the duty of explainability and human control) and as evidence in the evidence table on constant supervision.
- **Bank of Italy Circular no. 285 (updated 2020), Provisions on remote customer due diligence** - Italian secondary legislation that allows audio-video identification and establishes technical-procedural requirements (e.g., video quality, recording retention, human intervention in case of doubts). _Relevance:_ defines the _minimum standard_ for digital KYC use cases. The Navigator takes this into account by recommending measures such as liveness detection, manual verification if confidence is low, etc., in compliance with these provisions.

- **Italian Regulatory Sandbox Documentation (2021-2023)** - Specifically: _Bank of Italy Communication on the outcomes of the first sandbox window_[\[69\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Informativa%20sulla%20conclusione%20della%20sperimentazione,di%20richiesta%20della%20certificazione%2C%20e)[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi). Describes innovative Fintech projects tested, e.g., credit scoring platforms and KYC solutions in DLT, highlighting benefits and success conditions ("solution suitable for operating outside the sandbox provided all rules are complied with")[\[40\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione). _Relevance:_ provides **concrete Italian cases** to be cited in use case sheets, demonstrating feasibility and highlighting points of attention. The Navigator uses them to build confidence (e.g., AI credit scoring successfully tested in the sandbox) and emphasize that full compliance remains necessary even after innovation.
- **EDPB/Europrivacy Sources on AI & Privacy:** e.g., _EDPB Statement 2022 on the AI Act_, _EDPS Opinion on AI Act (June 2021)_. These express the positions of European privacy authorities on the draft AI Act (calling for rigor on FRIA, banning certain uses) and on the relationship with GDPR. _Relevance:_ although predating the final text, they reiterate the importance of not lowering privacy standards when applying AI (the Navigator takes this into account by ensuring that DPIA and GDPR principles remain central). Included in the bibliography as further reading for those interested in the privacy authorities' perspective on the AI regulation (e.g., EDPS called for mandatory FRIA for all high-risk – later adopted).
- **OECD & EU Commission Guidelines on Trustworthy AI (2019-2020)** - These are not regulations, but they contain the 7 ethical principles for trustworthy AI (transparency, human oversight, diversity/non-discrimination, accountability, etc.). _Relevance:_ they form the conceptual foundation for many subsequent regulatory obligations. The Navigator mentions them in the background (e.g., in the evidence table on principle convergence[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in)) and lists them in the bibliography as contextual reading to understand the philosophy of regulation (e.g., why human oversight is crucial).

_(N.B.: All primary sources cited – laws, EBA guidelines, Garante provisions – are to be considered authoritative. The selected secondary sources (articles, studies) are supported by official references and serve to clarify practical interpretation. This final list contains 18 entries, remaining within the limit of 20 sources.)_

Regolamento - UE - 2024/1689 - EN - EUR-Lex

<https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689>

1. High-risk AI systems referred to in Article 6, paragraph 2, which are intended to be used by public authorities or bodies acting on their behalf to assess the creditworthiness or to establish a credit score of natural persons or to assess their credit reliability, with the exception of AI systems used for compliance with legislation on combating money laundering and terrorist financing, are subject to a fundamental rights impact assessment in accordance with paragraph 3 of this Article. [\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)
2. High-risk AI systems referred to in Article 6, paragraph 2, which are intended to be used to make decisions regarding the admission or assignment of persons to educational and vocational training institutions, or to evaluate participants in standardized tests in such institutions, are subject to a fundamental rights impact assessment in accordance with paragraph 3 of this Article. [\[2\]](https://eur-lex.europa.eu/legal-

[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza) [\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento) [\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento) [\[52\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=7,la%20concessione%20di%20un%20finanziamento) [\[53\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=particolari%20misure%20di%20carattere%20organizzativo,compresi%20i%20trattamenti%20che%20prevedono) [\[54\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=profilazione,del%20volume%20dei%20dati) [\[55\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=8,personali%20raccolti%20per%20finalit%C3%A0%20diverse) [\[56\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,ovvero%20della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di) [\[63\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=relativi%20a%20condanne%20penali%20e,della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di%20trattamento) [\[64\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=tramite%20reti%20o%20di%20sorveglianza,Big) Impact Assessment

<https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto>

[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario) [\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario) [\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori) [\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano) [\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi) [\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano) [\[61\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=I%20rischi%20non%20sono%20teorici,economiche%20e%20sociali%20che%20comportano) AI and Credit Granting: Between Innovation and Responsibility

<https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/>

[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in) [\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti) [\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove) [\[37\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono) bancaditalia.it

<https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf>

[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent) [\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences) [\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants) [\[67\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=Regarding%20use%20cases%2C%20AI%20is,banks%20are%20leveraging%20AI%20in) Special topic - Artificial intelligence | European Banking Authority

<https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence>

[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system) [\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons) [\[51\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=For%20some%20uses%20of%20AI%2C,of%20predictive%20policing%20for%20individuals) [\[60\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=will%20be%20banned%20from%20the,of%20predictive%20policing%20for%20individuals) [\[66\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=Transparency%20and%20protection%20of%20fundamental,rights) Artificial intelligence act: Council and Parliament strike a deal on the first rules for AI in the world - Consilium

<https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/>

[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=%E2%80%A2%20CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving) [\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of) [\[48\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving) [\[68\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving) eba.europa.eu

<https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf>

[\[38\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi) [\[39\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi) [\[40\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione) [\[41\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=ICCREA%20Banca%20e%20Banca%20Monte,una%20futura%20commercializzazione%20della%20soluzione) [\[44\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=intermediari%20finanziari%20per%20offrire%20loro,durata%20massima%20di%2018%20mesi) [\[49\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=finalizzato%20a%20fornire%20alle%20banche,gestione%20del%20rischio%20di%20credito) [\[50\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=La%20sperimentazione%20%E2%80%93%20nell%27ambito%20della,operare%20al%20di%20fuori%20dell%27ambiente) [\[57\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20un%20servizio%20evoluto,durata%20massima%20di%2018%20mesi) [\[69\]](https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Informativa%20sulla%20conclusione%20della%20sperimentazione,di%20richiesta%20della%20certificazione%2C%20e) Bank of Italy - Projects admitted to the first temporary window - experimentation concluded

<https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html>

[\[42\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata) [\[43\]](https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello) Privacy and AI system development: unlawful processing of personal data - DB

<https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/>

[\[62\]](https://artificialintelligenceact.eu/annex/3/#:~:text=,the%20verification%20of%20travel%20documents) Annex III: High-Risk AI Systems Referred to in Article 6(2) | EU Artificial Intelligence Act

<https://artificialintelligenceact.eu/annex/3/>

[\[65\]](https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/#:~:text=EBA%20Factsheet%20%E2%80%93%20AI%20Act%3A,banking%20and%20payments%20sector%2C%20by) EBA Factsheet - AI Act: Implications for the EU banking and ...

<https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/>