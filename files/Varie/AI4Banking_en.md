# AI in Banking: Between DPIA and AI Act

## Abstract

The banking sector is adopting AI in critical areas such as credit scoring, fraud, and customer support, but the regulatory landscape is rapidly changing. This article connects GDPR and the AI Act, clarifying when DPIA and FRIA are triggered and what operational obligations arise. The goal is to offer a practical roadmap for assessing risks, distinguishing high-risk cases, and establishing consistent governance.

## Introduction

The adoption of AI in banking is no longer experimental: it is already part of processes that affect rights, access to credit, and data protection. Before delving into the rules, an introductory framework is needed to link the main use cases to the two regulatory pillars (GDPR and AI Act) and the required impact assessments. This article bridges technology and compliance, to guide project choices from the outset.

## What the rules say and how to proceed

### Key findings

The new European AI rules (Regulation (EU) 2024/1689, "AI Act") classify several uses of Artificial Intelligence (AI) in the banking sector as _"high-risk systems"_, imposing stringent requirements[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al).

In particular, the use of AI to assess the creditworthiness (credit scoring) of natural persons falls under Annex III and is therefore considered _high-risk_[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Similarly, AI systems used in the **recruitment and personnel management** process (e.g., candidate selection, hiring or promotion decisions) are explicitly listed among high-risk uses, given their potential impact on workers' rights and the risk of discrimination[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati).

Other typical banking use cases – such as **chatbot or virtual assistant systems for customers**, **anti-money laundering (AML) and fraud detection** tools, **identity verification (KYC)** systems with biometric recognition, etc. – while not all falling into "high-risk" categories for the AI Act, still entail significant compliance obligations. In particular, the AI Act exempts AI systems used to detect financial fraud or for prudential capital calculations from the _high-risk_ category[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20previsti,ad%20alto%20rischio%20ai%20sensi); this means that, paradoxically, **fraud detection** algorithms or **monitoring suspicious AML transactions** are not subject to the high-risk requirements of the AI Act, despite having to comply with sector-specific (e.g., anti-money laundering) and privacy regulations.

> In any case, banks remain obliged to carry out a case-by-case assessment: for example, a banking AI system that, while not expressly falling under Annex III, presents significant risks to rights, security, or access to essential services could be prudently treated as _high-risk_ for internal purposes.

The primary regulatory sources outline a series of **mandatory checklists** and applicable best practices. On the privacy front, the GDPR imposes the **Data Protection Impact Assessment (DPIA)** for high-risk processing, and authorities (EDPB and Garante Privacy) have listed criteria and typical cases: for example, large-scale profiling or scoring, automated decisions with significant effects (such as granting a loan), systematic monitoring of users, use of sensitive data or innovative technologies such as AI[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento).

Many banking AI use cases meet these criteria, such as:
- credit scoring = financial profiling with impact on rights;
- transaction monitoring = systematic control;
- facial recognition = biometric data;

> _These therefore require a prior privacy impact assessment (DPIA)_

In parallel, the AI Act introduces for _deployers_ of high-risk AI systems the obligation to conduct a **Fundamental Rights Impact Assessment (FRIA)** before putting them into use[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista).

In the banking sector, this particularly concerns entities that use AI Act Annex III systems, for example, banks using AI for credit scoring (Annex III §5(b)) or for decisions on life/health insurance policies. Such FRIA must consider the context of use, categories of affected persons involved, risks to rights (e.g., bias, exclusion), foreseen human oversight measures, and remedial actions[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta). It must be notified to the competent market surveillance authority along with the relevant results[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=obbligo%20di%20notifica), thus becoming a formal compliance requirement before the system's use.

A key issue that has emerged is the **overlap and distinction** between DPIA (focused on GDPR privacy risks) and FRIA (focused on broader _ethical and fundamental rights_ impacts). In the case of AI systems processing personal data, the legislator stipulates that the FRIA _integrates_ with the DPIA already carried out, avoiding duplication: in practice, the **DPIA covers data privacy and security**, while the **FRIA extends the analysis to discrimination, equitable access to services, transparency towards data subjects**, etc.

For example, for a loan granting algorithm, the DPIA will assess the lawfulness and proportionality of data processing, data minimization, security measures, and anonymization; the FRIA will add the assessment of risks of _algorithmic bias_, socio-economic impact (e.g., financial exclusion of vulnerable groups), and the adequacy of the **human oversight** measures adopted. This last point, human supervision, recurs as a fundamental principle: both the GDPR (Art. 22) and the AI Act and sector-specific guidelines insist on the necessity for AI _not to make completely autonomous decisions without the possibility of human intervention_.

The **EBA 2020 Guidelines on Loan Origination and Monitoring** have clarified that the use of automated models is permitted only within precise boundaries, including _transparency, traceability, constant supervision, and avoiding all forms of discrimination_[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,supervisione%20costante).

Similarly, the Bank of Italy has reiterated that AI can support credit risk assessment _but not replace human judgment_: the bank remains ultimately responsible for the correctness and legitimacy of decisions[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=La%20Banca%20d'Italia,%20%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario).

**The indispensability of human control** is also enshrined at the level of international principles: there is broad convergence on the importance of maintaining significant human intervention ("human-in-the-loop") in automated decision-making processes, to ensure accountability and the protection of rights[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in). In banking practice, this translates, for example, into requiring that the most impactful decisions (e.g., credit refusal, suspicious transaction reporting) are reviewed or endorsed by competent personnel, and that the operators involved have _adequate training on AI_ and the power to stop or correct the system[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la).

Large institutions are indeed moving towards "hybrid" models where AI generates recommendations but humans have the final say, investing in staff **upskilling** programs to bridge the technical skills gap[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent). This also addresses risk management needs: as observed by the EBA, many banks adopt a gradual and prudent approach to AI, introducing robust **control measures and "guardrails"** before extending the use of advanced models, especially generative ones, and testing the riskiest use cases only after gaining sufficient experience and confidence in the technology[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences).

From a strictly regulatory perspective, _new operational obligations_ emerge: for example, requirements for **transparency towards users**. The AI Act regulation mandates (even for non-high-risk systems) that anyone interacting with an AI be informed that they are interacting with a machine and not a human being[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). This means that in an automated contact center or banking chatbot, it must be clearly indicated to the customer that the service is provided by an AI system, and channels for _escalation to a human operator_ must be provided upon request.

Similarly, if the GenAI (Generative Artificial Intelligence) tool generates content (e.g., a textual response in natural language to the customer), any disclaimers regarding the automatic origin and accuracy of the information must be provided. Regarding **non-discrimination and fairness**, although Italian banking legislation contains only generic fairness clauses in credit granting, the new framework requires significant attention: the AI Act explicitly prohibits algorithms that introduce classifications based on sensitive characteristics (practices comparable to _social scoring_ are forbidden) and, as mentioned, includes credit among high-risk cases precisely because of possible _algorithmic biases_[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). Privacy guarantors and consumer authorities will be able to scrutinize unfair practices if a model systematically denies services to protected categories.

Therefore, banks must implement **AI governance** techniques: proactive model testing to detect disparities in treatment (bias audit), transparent documentation of variables used (feature importance), and effective complaint mechanisms for data subjects. A customer has the right to know if a decision regarding their financing was influenced by an algorithm and on what parameters[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori), as well as to obtain human intervention and challenge the automated decision - as stipulated by [GDPR art.22](https://www.privacy-regulation.eu/it/22.htm).

### Main Regulatory Ambiguities
Despite regulatory progress, grey areas and interpretive challenges remain. A first element of uncertainty concerns the _exact scope of high-risk categories_: some banking use cases do not clearly fall under Annex III.

For example, the use of AI for **Anti-Money Laundering** (detection of suspicious transactions) is not listed among high-risk cases unless it is carried out directly by law enforcement authorities. Indeed, the European regulation clarifies that AI systems used in administrative proceedings by tax and customs authorities, and by Financial Intelligence Units (FIUs) for anti-money laundering analysis, are not to be treated as high-risk AI systems employed by law enforcement. However, it remains clear that the use of AI must not create inequalities or hinder the right to defense, especially when decisions are difficult to challenge due to lack of transparency.[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo).

This raises doubts: an algorithm that _preventively blocks_ transactions or current accounts could impact fundamental rights (e.g., economic freedom) almost as much as a credit scoring system, but formally the AI Act does not cover it as high-risk. The choice seems motivated by a desire to foster anti-fraud innovation, but the boundary remains ambiguous: banks will have to decide whether to treat these "unclassified" systems with a conservative approach anyway (voluntarily applying requirements similar to those for high-risk systems) for prudence and accountability.

Further ambiguity concerns the **definition of "significant risk"** in the AI Act.

Indeed, the regulation stipulates that systems listed in Annex III _not_ be considered high-risk if, "by way of derogation", they _do not pose a significant risk_ to health, safety, or fundamental rights[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,%20risultato%20del%20processo%20decisionale)). This exemption clause is not easy to apply in practice: for example, a small AI solution used experimentally on a few clients, even if technically falling into a category (e.g., credit scoring), could be argued by the provider to pose a negligible risk; however, the criteria for establishing this are not explicit, and there is a risk of divergent interpretations. Companies might be reluctant to "downgrade" a system from high-risk to non-high-risk, fearing ex-post challenges. A cautious approach is therefore emerging, but the lack of implementing guidelines in this regard is a gap that will require clarification (the Commission is delegated to issue acts to amend Annex III and provide criteria, but it remains to be seen how this will be managed).

A third area of uncertainty concerns the **FRIA methodology and governance**. As this is a new compliance requirement, there are no consolidated standards yet on _how to conduct an ethical/fundamental rights impact assessment_. The AI Act stipulates that the _AI Office_ (a new European body) will provide a questionnaire template, also via an automated tool, to facilitate deployers[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=L'ufficio%20per%20l'IA%20elabora%20un%20modello%20di%20questionario,%20a%20norma%20del%20presente%20articolo%20in%20modo%20semplificato.), but until this is developed, companies will have to manage by drawing inspiration from similar guidelines (e.g., those from the Data Protection Authority for ethical assessments, or frameworks like the EU's Assessment List for Trustworthy AI).

>What expertise will need to be involved? Who will be the "notifying authority" for FRIA in Italy for the banking sector - the Ministry of Economic Development, Banca d'Italia, or a new body?

All of this is not yet precisely defined, and this creates operational ambiguities.

Ambiguities also exist in the **distinction of roles and responsibilities** along the AI supply chain. In many cases, banks use AI solutions provided by third-party vendors or based on pre-trained generative models (e.g., a linguistic foundation model integrated into a chatbot). The regulation distinguishes between a **"provider"** (who places the AI system on the market) and an end **"user (deployer)"**; in the banking sector, however, a bank that internally develops an algorithm for its own use could be considered both a provider and a deployer, with cumulative obligations (including technical compliance and CE marking for the high-risk system). If, on the other hand, it purchases an external AI service, it will still have to ensure the deployer's obligations (FRIA, registration in the EU database, usage surveillance) but it depends on the provider for compliant technical documentation. It is uncertain how to contractually manage this sharing of responsibilities: banks will have to demand AI Act compliance guarantees from vendors (e.g., **EU Declaration of Conformity** for high-risk systems) and access to model information to be able to conduct the FRIA.

Another grey area is the **management of explainability and the exercise of GDPR rights** in the presence of opaque AI models (e.g., deep learning). The GDPR grants data subjects the right to meaningful explanations regarding the logic of automated decisions; however, explainable AI techniques are still emerging, and it might not be possible to provide simple explanations for complex models. Banks will have to balance this obligation with the protection of trade secrets concerning their algorithms. There is no consensus yet on what level of transparency is "sufficient", an area where guidance is expected from the [EDPB or the future AI Office](https://www.edpb.europa.eu/our-work-tools/our-documents/letters/edpb-reply-letter-ai-office-edpb-statement-role-data_en).

Finally, uncertainty remains on **how to translate shared ethical principles into concrete practices**. Literature and authorities converge on principles such as _non-discrimination, transparency, human oversight_[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in), but, as noted by the Bank of Italy, it is less straightforward to incorporate them into binding norms and effective operational procedures[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20internazionale,%20prassi%20concrete%22). For example, everyone agrees on avoiding bias, but defining quantitative fairness metrics and acceptable disparity thresholds is complex and currently left to the autonomy of businesses. Similarly, it is undisputed that there must be human intervention, but how much discretion and at what stage of the process is appropriate? These are aspects still being navigated by sight, with conservative approaches (e.g., always requiring independent dual human control for certain critical decisions) pending consolidated practices.

### Operational Implications for an MVP
The evidence outlined above directly informs the requirements for the prototype of a potential tool (i.e., "AI Act Navigator" or "FRIA/DPIA Evidence Builder").

In summary, the MVP will need to:

 1) incorporate a _use case triage_ system based on targeted questions that allow identifying whether a use case falls into _AI Act high-risk_ categories or presents GDPR DPIA triggers, guiding the user (e.g., a Compliance officer) in correct classifications.
 2) It will need to implement a set of transparent **decision rules** (business rules): for example, if the user indicates that the AI system performs creditworthiness assessment of retail customers, the wizard should automatically flag it as _AI Act Annex III - high-risk_ and prepare the subsequent steps (e.g., list of requirements to be met, FRIA obligation, etc.)[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%27Unione%20al). If the use case involves processing special categories of data or extensive profiling, the tool should suggest a DPIA obligation.
 3) The wizard's output should include an _evidence and obligations dashboard_: e.g., a personalized checklist with "Documents to prepare" (e.g., _AI system descriptive sheet_, _DPIA_, _FRIA_, _processing record_, _contract with supplier_…), "Applicable requirements" (e.g., _Art. 10 AI Act - data governance, Art. 14 - human oversight_…), "Recommended actions" (e.g., _evaluate bias on dataset_, _human intervention required before final decision_, _data subject information to be updated_).
 4) The MVP will need to integrate **disclaimers and explanatory notes** in every critical section, to manage regulatory ambiguities: for example, a note clarifying "_If your case does not fall exactly into the AI Act categories but presents potential risks, the most prudent approach is recommended - see 'Ambiguities' section_". Or, in case of doubt about the need for a DPIA: "_Based on the information provided, there is no explicit DPIA obligation under Art. 35 GDPR, but a documented assessment is nevertheless recommended given the extensive use of personal data (conservative option)_".
 5) **Traceability and justification** of recommendations will be fundamental: the next section "Evidence table" will provide the rationale (regulatory source) behind each wizard rule, increasing user confidence in the provided guidance. The MVP will therefore not only guide step-by-step (wizard) but also serve as a searchable _knowledge base_, with sections like "Why are we asking this?" or "Why is this document required?" drawing from authoritative sources (Garante, EBA, legislation) collected in the research[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti).

### Evidence Table

| **Theme** | **Evidence** | **Source** | **Implication for wizard** | **Confidence** |
| --- | --- | --- | --- | --- |
| **High-risk banking use cases (AI Act)** | The AI Act classifies AI systems used to assess the creditworthiness of individuals (credit scoring) and for employment purposes (e.g., personnel selection) as _high-risk_. Such systems can significantly impact individuals' rights and living standards, risking the perpetuation of discrimination[[1]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al). However, AI systems for detecting financial fraud or calculating capital requirements (not considered high-risk) are excluded from Annex III[[3]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi). | _EU Reg. 2024/1689 (AI Act)_, recitals 58 and 59[[1]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al); _Paradigma, 2025_[[24]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano). | Immediately identify if a use case falls into Annex III categories (e.g., credit, HR). If so, mark it as "High-Risk AI Act" and activate compliance assessment modules (FRIA, requirements of Art. 8-15 AI Act). For excluded areas (e.g., anti-fraud), still indicate sectoral obligations but with a different AI Act regime. | **High** (clear regulatory text; confirmed by doctrine) |
| **DPIA - triggers in the banking sector** | The GDPR requires a DPIA for high-risk processing; WP29/EDPB guidelines list criteria: among them, large-scale profiling or scoring based on economic situation, automated decisions with legal effects (e.g., loan granting), systematic monitoring, use of sensitive data or new technologies (like AI)[[25]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). The Garante (Italian Data Protection Authority) has specified 12 mandatory types, including: _"large-scale evaluative or scoring processing"_ and _"automated decisions that significantly affect the data subject (e.g., screening bank customers for loan granting)"_[[4]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20valutazione%20d'impatto:)[[25]](https://www.latuaprivacy.com/site/approfondimenti/16-la%20valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). | _WP29 Guidelines No. 248/2017_ (EDPB); _Garante Privacy, Order 467/2018_[[5]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). | In the wizard questionnaire, include questions to detect if the use case involves financial profiling, automated decisions about customers, transaction monitoring, use of biometrics, etc. - if affirmative, trigger a "DPIA mandatory" alert and add the "Perform DPIA" task to the output. | **High** (official EDPB guidelines adopted by the Garante) |

| **FRIA - Requirements and Contents** | _EU Reg. 2024/1689_, Art. 27[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=che%20comprende%20gli%20elementi%20seguenti)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista); EU Council, Press Release 9/12/23[\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons). | The wizard must clearly explain when the FRIA is required (e.g., "High-Risk classified use case → FRIA mandatory before putting into operation"). It must guide the user in assembling the elements for the FRIA: e.g., ask to describe purpose and context, list of impacted stakeholders, etc., and produce a report outline. Furthermore, it must remind of the need for _notification to the authority_ and potentially provide a compliant output template (e.g., standard AI Office forms). It must also indicate that if a DPIA has been conducted, it should be updated/integrated rather than duplicated. | **High** (detailed legal provision) |
|---|---|---|---|
| The AI Act (Art. 27) obliges _deployers_ of high-risk AI systems (public authorities and private entities providing public services, as well as those using systems under Annex III points 5(b) and (c)) to carry out a **Fundamental Rights Impact Assessment** before use. The FRIA must include: description of the context of use and purpose of the system, duration and frequency of use, categories of persons impacted, specific risks to rights (taking into account information provided by the provider pursuant to Art. 13 of the AI Act), planned human oversight measures, and mitigation/management measures in case of problems (incl. complaint mechanisms)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta). The outcome must be notified to the market surveillance authority, using the template that will be prepared (also via an automated tool from the AI Office). If the deployer has already conducted a GDPR DPIA that covers some aspects, the FRIA can integrate that analysis without duplicating it[\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d%27impatto%20sulla%20protezione%20dei%20dati). | | | |

| **Human oversight - obligation and models** | | | |
|---|---|---|---|
| Sectoral regulations and the AI Act converge on the obligation to maintain **effective human oversight** over high-risk AI systems. Art. 14 of the AI Act requires that such systems be designed to be "overseeable" by humans, and that the persons responsible for control have the skills, training, and authority to intervene, including by interrupting the system if necessary[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=necessarie%20nonch%C3%A9%20del%20sostegno%20necessario). The **EBA Guidelines** on credit prescribe that AI should not operate with full autonomy: the intermediary must be able to review and _potentially derogate_ from the model's decisions (the _"human-in-the-loop"_ principle)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). For example, Art. 172(3) CRR already requires that IRB banks have the possibility of _human override_ of the results of internal rating models[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving). EBA analyses show that EU banks are adopting graduated approaches where **human intervention** is guaranteed, especially in the initial phases of advanced AI adoption, to control risks[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants). | _AI Act_, Art. 14[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in)[\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana); _EBA Risk Report 2024_[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences); _Paradigma 2025_ (cit. EBA GL)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario). | The wizard must include fields/questions to verify the presence of oversight mechanisms: e.g., "Is human intervention foreseen before the final decision is applied to the client?"; "Does the assigned personnel have the authority to block or correct the AI's output?". Based on the answers, provide alerts if oversight is inadequate (non-compliance trigger Art. 14). Furthermore, in the _output specifications_ for each use case, add recommendations on sustainable control models (e.g., double human verification for critical decisions, specific training for AI operators, periodic audits of model results). | **High** (legal requirement + EBA best practice) |

| **Transparency towards individuals** | The AI Act also imposes transparency obligations for non-high-risk systems: for example, anyone using a **chatbot** or system that interacts with people must clearly inform the user that it is an automated system[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system). Furthermore, if content (text, image) is generated by AI, it must be declared to the end-user (to prevent deception). In the banking sector, GDPR Art. 13-14 and 22 already require communicating to data subjects the existence of automated decisions and providing meaningful information about the logic used and the envisaged consequences[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). National legislation (e.g., TUB Art. 124-bis) reiterates that if credit is granted using automated tools, the customer must be clearly informed and has the right to adequate explanations. | _AI Act_, Art. 52 (now 50)[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system); _GDPR_, Art. 13-14, 22; _Paradigma 2025_[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori). | The wizard must ask if the AI interacts with customers or determines outputs directed at people. If yes, indicate among the mandatory outputs: "Prepare a specific information notice for users", "Insert visible disclaimers in the interface (e.g., 'Automated virtual assistant')". Furthermore, provide guidelines on how to draft explanations of decisions in understandable language. The tool could include an **information notice template** module to be filled with the use case details (e.g., type of algorithmic logic, data used) in compliance with GDPR. | **High** (clear GDPR + AI Act rules) |
| **Non-discrimination and fairness** | The principle of non-discrimination is not explicitly regulated in financial norms, but it is a central focus of the AI Act and authorities. The Bank of Italy notes that banking transparency provisions contain few explicit references to _equal treatment_, and the use of AI-ML techniques calls for new attention on this front[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove). The AI Act prohibits social scoring systems and categorization based on sensitive data, and in its recitals, highlights the risk that credit or recruiting models may _perpetuate historical biases_ (e.g., disadvantaging women, minorities)[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al). Real-world cases confirm the danger: overly opaque HR algorithms or credit algorithms based on data correlated with ethnicity/area can produce disparities. The Italian Data Protection Authority has pointed out that credit profiling must avoid processing sensitive data or proxies thereof (e.g., residential postcode) without adequate safeguards[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi). | _Bank of Italy QEF 721/2022_[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove); _AI Act_ Recital 58[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al); _Paradigma 2025_[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano). | The wizard must include checkpoints dedicated to fairness: e.g., ask if the dataset has been checked for biases (representative imbalances), if the model uses potentially discriminatory attributes (direct or indirect). As output, for sensitive use cases (credit, HR), recommend conducting a _"bias audit"_ and documenting the results in the FRIA. Furthermore, indicate the conservative option of excluding non-pertinent variables or those potentially proxy for protected categories from the model. Provide ref | |

erences to guidelines (e.g., Technical Appendix on fairness metrics) in the wizard's bibliography for further reading. | **Medium** (general principle clear, but lacking unambiguous metrics) |
| **Regulatory Overlap and Conservative Approaches** | The EBA found that many requirements of the AI Act (e.g., data governance, robustness, oversight) are partly covered by existing financial regulations, although there are no _explicit derogations_ in the AI Act for the banking sector[[36]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=includes%20a%20wide%20range%20of). This means that banks and intermediaries will have to comply with both frameworks: for example, an IRB credit model must follow CRR/EBA rules _and_ meet AI Act requirements (technical documentation, testing, etc.). This dual track can create burdens, but also opportunities for integration. For instance, periodic model controls required by Banca d'Italia/EBA (validations, backtesting) can also serve as continuous monitoring measures under the AI Act. In case of interpretative doubt regarding borderline categories, banks are adopting a prudential approach, often voluntarily applying the most rigorous measures. A recommended practice is to consider **FRIA and DPIA combined** as part of a single _AI risk assessment_ process, involving different functions (Compliance, DPO, Risk Management) to cover all aspects. | _EBA Chair Letter 2025_[[36]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=includes%20a%20wide%20range%20of); _Banca d'Italia QEF_[[37]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono)[[13]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in). | The wizard should provide the user with a framework of _"regulatory intersection"_ - for example, a summary section stating: "Your case requires: AI Act compliance (requirements X, Y, Z) **and also** adherence to banking rules ABC (e.g., EBA guidelines x)". Suggest an integrated approach: the output could recommend unifying DPIA+FRIA into a single corporate document/procedure. Furthermore, in the explanations, the tool will highlight where obligations coincide (e.g., data quality is both an AI Act Art. 10 requirement and a good practice for internal models) to avoid duplication. In cases of uncertainty (use case not explicitly regulated), the wizard will adopt the "conservative approach suggested" flag and include extra precautionary measures. | **High** (authoritative EBA reconnaissance; convergence with Banca d'Italia practices) |

## Remaining Open Issues
Some points remain open, such as:

- **Practical criteria for "significant risk" (Art. 6(3) AI Act):** It is unclear how a provider or user will be able to demonstrate that a system falling under Annex III **does not** present a _significant risk_ and thus exempt it from the high-risk regime[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,%20risultato%20del%20processo%20decisionale). Guidelines on risk metrics and who validates such self-assessment will be needed to avoid arbitrary decisions and under-classification.
- **Coordination between Authorities (AI Act vs. privacy vs. sector):** Who will practically be the reference authority for supervising banking AI systems? The _Market Surveillance Authority_ for high-risk AI could be Banca d'Italia (banking supervision) or a new entity; the Garante Privacy will maintain its role on DPIA and data protection; the EBA/ECB will have a say through the _AI Board_. Clear protocols are needed to prevent conflicts or gaps in competence in ex-ante assessments (FRIA) and ex-post controls.
- **Reference standards and certifications:** Given the strong technical nature of the AI Act requirements, the question arises as to which technical standards banks will adopt to demonstrate compliance (e.g., ISO 42001 AI Management? bias/fairness certifications?). The EBA notes that the Commission will issue guidelines for high-risk classification by 2 Feb 2026[\[65\]](https://www.eba.europa.eu/sites/default/files/2025-11/d8b999ce-a1d9-4964-9606-971bbc2aaf89/AI%20Act%20implications%20for%20the%20EU%20banking%20sector.pdf), but operationally, banks would prefer a unified framework. The open question: _is it advisable to wait for official standards or start with voluntary certifications (e.g., ethical audits, third-party attestations) to stay ahead?_
- **DPIA-FRIA Integration:** How to practically implement a single process that covers both? Should two separate reports be produced (one for the Garante, one for the AI Act authority) or will a single _"AI Risk Assessment Report"_ suffice for both purposes? And in the case of an impact assessment with a doubtful outcome (with a high residual risk): for GDPR there is an obligation to consult the Garante, for the AI Act it is not foreseen but perhaps the Market Authority can intervene; it would be necessary to understand how to align these escalations.
- **Third-party vendor management and liability:** If a vendor provides a non-compliant AI system and the bank suffers a breach (e.g., a fine for discrimination), who bears the responsibility? The AI Act regime provides for primary responsibility of the _provider_ for technical requirements and of the _user_ for improper use. But in real contracts between banks and vendors, robust clauses on guarantees, indemnities, and access to information (audits) will be needed. The open question: will banks be able to contractually ask vendors to conduct and share a FRIA as a _provider_? Or will each bank have to do it alone, even for standard packages?
- **Fairness metrics and acceptable thresholds:** Will regulators require banks to quantify and maintain certain levels of fairness in models (e.g., _"disparate impact ratio"_ no worse than 80%)? Or will it all remain qualitative? The absence of unambiguous quantitative criteria is a problem: one bank might consider a slight deviation acceptable, another might not. It is wondered whether the EBA or the AI Office will develop guidance in this regard.
- **Use of sensitive data for ethical purposes (bias correction):** A known paradox – to test if a model is discriminatory, it would sometimes be necessary to consider the protected variable (e.g., gender) in the data; but this is prohibited by decision. Will the Garante allow the use of simulated or post-recruitment sensitive data to validate fairness? There is uncertainty on this, and banks struggle to define GDPR-compliant bias audit methodologies.
- **Interoperability with future regulations (ESG, AI liability):** How will the AI Act requirements combine with other emerging ones, such as initiatives on _AI liability_ (civil liability for AI damages) or ESG regulations (which could include ethical use of AI)? Banks will also need to map these aspects. Furthermore, it remains an open question whether the prepared documentation (e.g., AI event log, decision logs) can be used against the bank in civil lawsuits (an unresolved liability issue: too much transparency could expose to litigation).
- **Role of the EU AI Office vs. National Authorities:** The AI Office will have supervisory powers primarily over foundation models and high-risk AI systems with impact. But will it also be able to issue binding guidelines for regulated sectors? Banks will need to keep an eye on possible additional supranational indications. The question: if the AI Office (Commission) identifies a banking model as _"high-impact"_ GPAI, could it intervene directly? This needs clarification.

- **Continuous tool update vs. evolving norms:** recognizing that interpretations and practices surrounding the AI Act and DPIA will evolve (case law, EDPB guidelines, new regulatory changes), how to keep the Navigator always updated? The desk research process has limitations; some issues will only be resolved through a _regulatory feedback loop_ (e.g., first FRIAs actually carried out, sanctions imposed, etc.). The topic of tool governance is open: who in the company (or ABI Lab consortium) will update it with _lessons learned_ and new regulatory sources as they emerge? This determines the long-term sustainability of the proposed solution.

# Bibliography

- **Regulation (EU) 2024/1689 "AI Act"** - Fundamental legislative text adopted in June 2024[\[1\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al)[\[2\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati)[\[3\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20previsti,ad%20alto%20rischio%20ai%20sensi)[\[6\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti)[\[7\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista)[\[8\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta)[\[9\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=obbligo%20di%20notifica)[\[10\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d%27impatto%20sulla%20protezione%20dei%20dati)[\[14\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la)[\[20\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo)[\[21\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,%20risultato%20del%20processo%20decisionale)[\[22\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=L'ufficio%20per%20l'IA%20elabora%20un%20modello%20di%20questionario,%20a%20norma%20del%20presente%20articolo%20in%20modo%20semplificato.)[\[27\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=necessarie%20nonch%C3%A9%20del%20sostegno%20necessario)[\[30\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in)[\[31\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana)[\[33\]](https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al). It defines the EU regulatory framework for AI with a risk-based approach. _Relevance:_ Cited annexes and articles identify high-risk banking cases (credit, HR) and impose obligations (e.g., Art. 14 human oversight, Art. 27 FRIA). Basis for many Navigator compliance actions.
- **DPIA: Italian Data Protection Authority (Order No. 467/2018) and WP29/EDPB Guidelines WP248** - Summary of processing cases mandatorily subject to DPIA and high-risk criteria[\[4\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20valutazione%20d'impatto:)[\[5\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento)[\[25\]](https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento). _Relevance:_ operational reference to understand when a DPIA is triggered and what elements to include.

- **Paradigma.it - "AI and credit granting: between innovation and responsibility" (Oct 2025)**[\[11\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,supervisione%20costante)[\[12\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=La%20Banca%20d'Italia,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario)[\[19\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori)[\[24\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano)[\[34\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi)[\[35\]](https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano) - Italian legal analysis on obligations and responsibilities in credit decisions. _Relevance:_ useful for framing human oversight and accountability practices.
- **Bank of Italy - Economics and Finance Working Paper No. 721 "Artificial intelligence in credit scoring" (2022)**[\[13\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in)[\[23\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20internazionale,%20prassi%20concrete%22)[\[32\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove)[\[37\]](https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono) - Empirical and regulatory study on AI benefits/risks and governance in credit. _Relevance:_ highlights the need to ensure fairness and transparency with operational measures.
- **EBA Risk Assessment Report, "Special topic AI" (Nov 2024)**[\[16\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent)[\[17\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences)[\[29\]](https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants) - EBA report on AI adoption and operational challenges in banks. _Relevance:_ supports sections on use cases and skill gaps.
- **EU Council Press Release 9 Dec 2023 (AI Act Agreement)** - Council press note summarizing key obligations and transparency requirements[\[18\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system)[\[26\]](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons).
- **Letter from EBA Chair José M. Campa (Nov 2025) - Mapping AI Act vs. banking regulation**[\[28\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving)[\[36\]](https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=includes%20a%20wide%20range%20of) - Document addressed to the Commission examining overlaps between AI Act obligations and banking legislation. _Relevance:_ useful for understanding where existing compliance covers AI Act requirements.
- **EBA Factsheet - "AI Act: Implications for the EU banking sector"**[\[65\]](https://www.eba.europa.eu/sites/default/files/2025-11/d8b999ce-a1d9-4964-9606-971bbc2aaf89/AI%20Act%20implications%20for%20the%20EU%20banking%20sector.pdf) - EBA factsheet with timelines and initial operational guidance on high-risk classification.