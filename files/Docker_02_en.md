# Deep dive in Docker

<details class="post-warning">
<summary><strong>Article under review</strong> (click to expand)</summary>

This article is still being drafted and reviewed. Portions of the text may be incomplete or change significantly as revisions progress.

</details>

## Abstract
Welcome to the second episode of the "Docker for Developers" series. Since the first article was well-received, I'd say we can go deep this time, analyzing Docker's key concepts in a way that's accessible to everyone. Let's try to understand what Docker does conceptually, without yet writing code or configuring anything.

## 1\. Fundamental concepts

![Image](../Assets/Docker.png)

_Figure 1: Docker Architecture - The client sends commands to the Docker daemon, which manages images and containers. Images (e.g., Python, Redis) reside in registries like Docker Hub, from which they can be downloaded ("pull"). Containers are runtime instances created from images, executed in isolation on the Docker Host._

Let's start with one of the main definitions in this field: what is a **Docker image**.

**Docker Images:** A Docker _image_ is essentially a **read-only template** that describes everything needed to create a container. [On the official website](https://docs.docker.com/get-started/docker-overview/), we can see it as a _snapshot_ of the filesystem and configuration of a minimal system. Be careful, though, an image can be based on another. For example, you can start with Ubuntu and then add an Apache server, the application, and the necessary configurations.

One thing to keep in mind is that images are built incrementally and compositely: each **is composed of multiple stacked filesystem layers** (a concept we will explore further later). Another **important** thing: an image itself is **static and immutable**. This means it **never changes during execution**, serving as a template from which to launch containers.

Wait, but what is a Docker container?

**Docker Containers:** A _container_, on the other hand, is a **runtime instance of an image**, or rather [as it is officially defined](https://docs.docker.com/get-started/docker-overview/#containers). If the image is the model, the container is the concrete object in execution.

In practice, a container is nothing more than an isolated process running on the host machine, with its own filesystem, network, and separate process space. [Here I am quoting verbatim](https://docs.docker.com/engine/containers/run/#:~:text=Docker%20runs%20processes%20in%20isolated,tree%20separate%20from%20the%20host).

Docker uses the image as a base and creates an isolated environment in which the application can run. We can think of the container as a *"container"* (hence the name, obviously) that encloses the application and its dependencies, ensuring that it always runs in the same environment, regardless of where the container is executed.

A useful analogy is with object-oriented programming: **the image is like a class, while the container is an instance (an object) of that class**
- the image defines
- the container executes.

Furthermore, multiple containers can be created from the same image (like multiple objects from a single class) without affecting each other, each with its own temporary state.

Since good things come in threes, I want to give a third very important definition.

**Registry (Image Registry):** A [_registry_](https://docs.docker.com/get-started/docker-overview/#docker-registries) is a centralized service for storing and sharing container images. The default public registry (and I add, widely used) is **Docker Hub**, which Docker defaults to for downloading images not found locally. There are also alternative public or private registries: for example, GitHub Container Registry, Amazon's AWS ECR, Google GCR, Azure ACR, or self-hosted solutions like Harbor. In a registry, images are organized into repositories (e.g., username/imagename on Docker Hub identifies a repository). The difference between a public and private registry lies mainly in access controls: a **public registry** (like Docker Hub) **makes images available to anyone**, often with a library of official images; a **private registry**, on the other hand, is accessible only to **authorized users**, useful for maintaining corporate or internal images not publicly visible. In both cases, Docker can authenticate to the registry if necessary and then _pull_ (meaning to download) and _push_ (here meaning to upload) images.

Now that we have understood some definitions, we can start getting hands-on, perhaps by beginning with how to create an image.

**How to create an image (build):** Docker images are typically created with a _build_ process based on a [**Dockerfile**](https://docs.docker.com/get-started/docker-overview/#:~:text=You%20might%20create%20your%20own,compared%20to%20other%20virtualization%20technologies), which is a text file containing instructions on how to build the image from a base. Each instruction in the Dockerfile (e.g., FROM, RUN, COPY, etc.) is executed sequentially by the Docker daemon during the build, in turn producing an additional filesystem layer on the image.

For example, a Dockerfile might start by defining the base Ubuntu image with:
```docker
FROM ubuntu:22.04
```
then to install Python, creating a new layer with these added files
```docker
RUN apt-get install -y python3
```
then to copy the application code into the image:
```docker
COPY . /app
```
and so on. Docker performs the build by going through each of these instructions and "stacking" the results into the final image.

This process leverages the cache: if we rebuild an image without modifying certain instructions, Docker will reuse existing layers without recalculating them, making the build very fast. To make the best use of the cache, I'll directly provide you with [the official code](https://docs.docker.com/build/cache/) for leveraging the cache.

```docker
# syntax=docker/dockerfile:1
FROM ubuntu:latest

RUN apt-get update && apt-get install -y build-essentials
COPY main.c Makefile /src/
WORKDIR /src/
RUN make build
```

At the end of the build, we obtain an image identified by a **unique ID (a hash)**, which we can run or distribute. We can create our own images from scratch (e.g., starting from scratch, an empty image) or base them on others' images, which encourages the reuse of commonly used components (e.g., a standard Linux base) instead of reinventing everything.

We conclude this section by discussing two commands we've already partially covered.

**Pull and Run:** Two fundamental commands for working with images are `docker pull` and `docker run`.

- **Docker Pull.** The `docker pull <name:tag>` command downloads an image from the registry. Behind the scenes, the Docker client contacts the registry via HTTP(S), requests the image manifest (the file that lists the layers), and downloads each layer — the so-called _blob_ — that it doesn't already have in cache, often in parallel, saving it locally. [If you want to learn more](https://www.redhat.com/en/blog/pull-container-image#:~:text=When%20you%20initiate%20a%20pull%2C,a%20manifest%20from%20the%20registry). For example, `docker pull nginx:latest` retrieves the list of Nginx layers from Docker Hub, downloads them one by one, and, upon completion, makes the image ready on the host.

- **Docker Run.** The `docker run` command starts a container from an image. In a single step, it performs an eventual `docker pull` (if the image is not present locally) and creates/starts the new container. If you run `docker run ubuntu:22.04 echo "ciao"`, Docker checks for the presence of the `ubuntu:22.04` image, downloads it if necessary, **creates a container** (allocating resources, preparing the filesystem, assigning an ID), and **starts it** by executing `echo "ciao"` inside it. Once the command has finished, the container can be stopped and removed or reused. In summary, `docker run` manages the entire lifecycle: download (if needed) and execution of the process within the container.

## 2\. Container lifecycle

### From build to execution (behind `docker run`):

After building an image (the _build_ phase we just saw), we can move on to the _run_ phase, which means creating and running containers based on that image.

When we execute `docker run`, as we've already seen, Docker might first download the image (if not already present).

Subsequently, it internally performs an operation equivalent to `docker container create`: in this phase, the container enters the **"Created"** state. During this phase, the necessary resources are allocated, an ID is assigned, and the container's specific filesystem is prepared.

Immediately after, Docker starts the container (equivalent to `docker container start`), launching the main process defined by the image.

At this point, the container transitions to the **"Running"** state and executes its workload within the isolated environment. The container will remain running as long as its main process is active. If that process terminates (either by voluntary exit or crash) or if we send a stop command, Docker stops the container, which then enters the **"Stopped" or "Exited"** state. A **stopped** container still retains the state of its isolated filesystem, logs, and can be restarted if needed (in which case it will return to "Running").

Finally, if we decide to completely free up resources, we can remove the container with `docker rm`: at that point, it transitions to the **"Removed"** state, being deleted from the system. The typical [lifecycle](https://last9.io/blog/docker-container-lifecycle/#:~:text=Every%20container%20typically%20goes%20through,general%20flow%20stays%20the%20same) is therefore: **Created → Running → Stopped/Exited → Removed**.

![Container Lifecycle](../Assets/docker-lifecycle.svg)

_Figure 4: The lifecycle of a Docker container, with the main commands that accompany it from creation to definitive removal._

> **Please note**: Docker also includes a **"[Paused](https://last9.io/blog/docker-container-lifecycle/#:~:text=The%20Paused%20State)"** state, where container processes are frozen via cgroups freezer, without terminating them. Pausing a container allows for temporary CPU reservation, but it is used less frequently. Obviously, a container must be in the Running state to be paused, and then it must be "unpaused" to resume.

**Container States:** In summary, here are the [main states](https://last9.io/blog/docker-container-lifecycle/#:~:text=Every%20container%20typically%20goes%20through,general%20flow%20stays%20the%20same) a Docker container can be in during its lifecycle:

- **Created:** The container has been defined and resources allocated, but the process inside it is not yet running. This state is achieved, for example, with a `docker create` without `start`, or immediately after a `docker run` before the process starts. In this state, the container has an ID, a ready filesystem, but consumes minimal resources (no active processes).
- **Running:** The container is started, and its main process (PID 1 in the container) is running. The container is active: it might expose network ports, use CPU/RAM, perform I/O operations, etc. This is the state when we are using the service or application contained within the container.
- **Stopped/Exited:** The container has executed its process and it has terminated, or it has been stopped manually. The container is not running anything, but it still exists in the system with its intact filesystem and the state it had at the time of stopping. We can inspect it, read logs, or potentially restart it.
- **Removed:** The container has been deleted from the Docker host (typically with `docker rm` or via `--rm` options). In this state, the container no longer occupies space (except for any separate persistent volumes) and no longer appears in `docker ps -a`. Any modifications not preserved outside the container are permanently lost at this point.

### Copy-on-write and container filesystems

One of the key aspects of Docker is how it manages container filesystems with a **copy-on-write** mechanism. Upon container creation, Docker sets up a **[_union file system_](https://www.digitalocean.com/community/tutorials/working-with-docker-containers#overview)** for it, composed of **all** the **read-only** layers of the base image _plus_ a **writable layer** specific to the container.

In practice, the container "sees" a single complete filesystem (called the container's _root filesystem_) that combines the image layers with its writable layer.

When a process in the container modifies or creates a file, copy-on-write comes into play: if the file already existed in one of the image's read-only layers, Docker makes **a copy of it in the container's writable layer** and applies the changes there, leaving the original image file intact. The result is that inside the container, the file appears to have changed, but externally the image remains immutable.

Each container has its own writable layer, so changes made in one container **are not visible in other containers** using the same image. Furthermore, if we delete a container, we lose its writable layer and with it all modifications made in that environment (unless they were saved in persistent volumes or _committed_ to a new image).

This copy-on-write design makes containers **lightweight** but also **ephemeral**. In fact, they share immutable layers among themselves (saving space and memory), and each adds only the necessary differences. For example, if we start 5 containers from the Ubuntu image, on disk we will have only one copy of the Ubuntu layers, used in common, plus 5 small separate layers for the differences of each container.

## 3\. Internal architecture

### Isolation with namespaces and cgroups (Linux kernel)

Docker achieves container isolation by leveraging native Linux kernel features, primarily _namespaces_ and _cgroups_.

Well, we mentioned them earlier, but I haven't forgotten to explain them.

**Namespaces** are mechanisms that the kernel provides to isolate views and system resources among various process groups. Essentially, when Docker creates a container, behind the scenes the kernel assigns a set of dedicated namespaces to the container[\[13\]](https://docs.docker.com/get-started/docker-overview/#:~:text=The%20underlying%20technology). Each namespace isolates one aspect: for example, the _PID namespace_ ensures that the container has its own process numbering (the container's process "1" is the main process of the app, and it does not see processes outside the container)[\[14\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=Namespaces%20and%20Containers)[\[15\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=The%20crucial%20thing%20to%20notice,isolated%20within%20my%20own%20namespace). The _network namespace_ provides the container with its own virtual network interface, with a separate IP address, isolating network traffic. The _mount namespace_ gives the container its own view of the filesystem (we will soon see the specific root filesystem), separate from the host's. There are namespaces for IPC (inter-process communications), for UTS (separate hostname and domain name), and optionally for the user (user namespace, which allows UID/GID mapping to run isolated "root" containers as an unprivileged user on the host). Thanks to namespaces, each container lives in a kind of logical "bubble": **from within, it only sees its own resources**, and not those of other containers or the host[\[14\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=Namespaces%20and%20Containers).

In addition to namespaces, Docker uses **cgroups** (_control groups_) to limit and monitor resource usage (CPU, RAM, I/O) by containers[\[16\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=What%20Are%20cgroups%3F). Cgroups allow the system to set how much maximum CPU or memory a container can consume, ensuring that one does not saturate all resources at the expense of others[\[17\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=A%20control%20group%20,of%20a%20collection%20of%20processes). They also provide accounting (tracking) of consumption and can even freeze or terminate all processes of a container en masse (for example, the freezer cgroup is used for the implementation of the docker pause command)[\[11\]](https://last9.io/blog/docker-container-lifecycle/#:~:text=The%20Paused%20State)[\[18\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=,cgroup%20with%20a%20single%20command). In summary, **namespaces** = isolation (containment for processes, network, filesystem, etc.), **cgroups** = resource control and quotas. Docker combines these mechanisms to create containers that behave like separate environments, while all running on the same underlying Linux kernel.

**Logical vs. Physical Isolation (Container vs. VM):** It is important to understand that the isolation provided by containers is _logical/software_-based, different from the _physical/hardware_ isolation of traditional virtual machines. A container **shares** the host operating system's kernel: there isn't a separate OS instance for each container, as there is for a VM. VMs virtualize an entire hardware and kernel, offering deeper isolation but with greater overhead; containers **virtualize at the operating system level**, isolating processes and resources within the same kernel[\[19\]](https://aws.amazon.com/compare/the-difference-between-containers-and-virtual-machines/#:~:text=Containers%20virtualize%20the%20operating%20system,give%20some%20more%20differences%20below). In other words, containers provide process isolation (thanks to namespaces/cgroups) on the same OS, while VMs provide isolation of entire machines, each with its own guest OS above a hypervisor. This means that a kernel bug is shared between containers and the host (thus a security flaw in the kernel is theoretically exploitable to escape from a container), whereas in VMs, the guest kernel is separate from the host kernel. On the other hand, containers are much lighter: **start in a few seconds, occupy only a few MB** because they don't duplicate an entire OS, and dozens of them can exist on the same host without significant waste, whereas a few VMs would saturate resources[\[20\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=Since%20the%20container%20runs%20natively,you%20configure%20it%20that%20way)[\[21\]](https://aws.amazon.com/compare/the-difference-between-containers-and-virtual-machines/#:~:text=Containers%20virtualize%20the%20operating%20system,give%20some%20more%20differences%20below). A common analogy: _containers_ are like apartments in a single building (same land and structure, separated by internal walls: less total but more efficient isolation), while _VMs_ are like independent houses (each with its own foundations and utilities: greater isolation, but higher costs and times).

**Root Filesystem and OverlayFS:** Inside each container, the process sees an **isolated root filesystem**, meaning the container's / directory contains only the image files plus any local modifications, and not the host's files. As mentioned, this is achieved through a union filesystem with a copy-on-write mechanism. On Linux, Docker by default uses the **OverlayFS** driver (in the _overlay2_ variant) to implement this functionality[\[22\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Docker%20supports%20multiple%20storage%20drivers,storage%20driver)[\[23\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Storage%20Driver%3A%20overlay2). OverlayFS allows combining multiple directories ("lower dirs" read-only + "upper dir" writable) presenting them as a single mounted directory. In the context of Docker, the **image layers** are the lower dirs (read-only) and the **container's "diff" folder** is the upper dir (read-write); when the container is started, Docker mounts an overlay-type filesystem that combines all these layers and sets it as the container's root filesystem (using calls like chroot or namespace mount)[\[24\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=1,chroot). For example, an Ubuntu image can be composed of several layers (one for the minimal OS base, one for some libraries, etc.): Docker stores each layer as a directory under /var/lib/docker/overlay2/ identified by a hash[\[25\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Docker%20uses%20the%20overlay%20filesystem,top%20of%20the%20image%20layers). When we run a container from such an image, Docker creates a new dir for the container's upper layer and then mounts an overlay FS: the result is a merged folder that represents the unified view of the base Ubuntu + modifications. The process in the container is executed with this merged set as /, and thus sees a complete Ubuntu filesystem, isolated from the rest[\[24\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=1,chroot). Every write in the container ends up in the diff dir (upper), while the original files remain intact in the lower dirs[\[26\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=echo%20%27Add%20a%20new%20line%27,1)[\[27\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=The%20original%20file%20present%20inside,layer%60%20is%20created). **OverlayFS** is very efficient: it avoids data duplication between multiple containers and makes starting new containers immediate (just add a new writable layer).

layer on top of existing layers). The use of a layered filesystem is a crucial concept: it's what makes Docker images composed of **reusable layers** and containers lightweight instances that leverage those shared layers.

## 4\. Docker images as layers

**Layer: what it is and how it's stored:** A Docker image is not a single monolithic blob, but is made up of an **ordered series of stacked layers**. Each _layer_ represents a set of **filesystem changes** relative to the underlying layer[[28]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=Each%20layer%20in%20an%20image,look%20at%20a%20theoretical%20image). For example, let's imagine creating an image for a Python application. The layers could be structured as follows[[28]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=Each%20layer%20in%20an%20image,look%20at%20a%20theoretical%20image):

- **Layer 1:** minimal base system (e.g., basic commands and apt package manager on Ubuntu).
- **Layer 2:** installation of the Python interpreter and pip.
- **Layer 3:** addition of the application's requirements.txt file.
- **Layer 4:** installation of app-specific Python packages (dependencies listed in requirements.txt).
- **Layer 5:** copying of the application's source code to the intended destination.

Each layer therefore adds files or makes changes (installations, copies, configurations). In Dockerfile terms, these would correspond respectively to instructions like FROM ubuntu:22.04, RUN apt-get install python3 pip, COPY requirements.txt ., RUN pip install -r requirements.txt, COPY src/ . and so on. During the build, Docker creates a layer for each instruction[[2]](https://docs.docker.com/get-started/docker-overview/#:~:text=You%20might%20create%20your%20own,compared%20to%20other%20virtualization%20technologies). **How are they stored?** Internally, Docker saves each layer as a compressed archive (typically a tarball) and identifies it with a unique hash (a SHA256 digest). On the Docker host's disk (e.g., in /var/lib/docker/overlay2/ for overlay2), each layer is extracted into a distinct directory[[29]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Docker%20uses%20the%20overlay%20filesystem,top%20of%20the%20image%20layers). Layers are **immutable**: once a layer is created, it is never modified again (any changes occur by creating a new layer on top). This immutable layer approach also means that if we update an image by rebuilding it, Docker can reuse previous layers that haven't changed and only regenerate the new ones[[30]](https://docs.docker.com/get-started/docker-overview/#:~:text=with%20a%20simple%20syntax%20for,compared%20to%20other%20virtualization%20technologies)[[31]](https://docs.docker.com/get-started/docker-overview/#:~:text=it,compared%20to%20other%20virtualization%20technologies).

**Caching and Layer Reuse:** One of the key advantages of having layered images is the ability to **reuse** common layers across different images and between successive builds. For example, if two different Docker images both start from ubuntu:22.04, the base Ubuntu layer can be shared: Docker downloads or builds it once and then reuses it for all derived images[\[32\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=This%20is%20beneficial%20because%20it,look%20similar%20to%20the%20following). This enormously reduces the space used and download bandwidth required, in addition to accelerating build times. When creating an image via Dockerfile, Docker builds the instructions in order and stores the hash of the content generated at each step. If we rebuild the same image without modifications at a certain step, Docker recognizes that an identical layer already exists in the cache and reuses it without re-executing the commands (we will see messages like "Using cache" in the docker build output). For example, if the Dockerfile does not change between one build and another, Docker will reuse all layers from the cache, producing the image almost instantly. If we modify only the last instructions (e.g., the application code), the initial layers remain cached, and Docker only recalculates the new or modified ones[\[30\]](https://docs.docker.com/get-started/docker-overview/#:~:text=with%20a%20simple%20syntax%20for,compared%20to%20other%20virtualization%20technologies)[\[31\]](https://docs.docker.com/get-started/docker-overview/#:~:text=it,compared%20to%20other%20virtualization%20technologies). In addition to local caching, the layered design facilitates **incremental pulling**: when we `docker pull` an updated image, Docker only downloads the new layers that are not present and skips those already downloaded in the past (for example, when moving from v1 to v2 of an image that differs only in the last layer, Docker will only download that additional layer). In deployment scenarios, this leads to significant time and bandwidth savings.

**Practical Example of Layers (Dockerfile):** Let's take a simple Dockerfile to illustrate layers:

```
FROM ubuntu:22.04  
RUN apt-get update && apt-get install -y nginx  
COPY index.html /usr/share/nginx/html/index.html
```

When we build this image, Docker executes:

- FROM ubuntu:22.04: This instructs Docker to use the Ubuntu 22.04 base image. This base image already consists of several layers (minimal Ubuntu layers). All these layers together become the **initial layers** of our new image.
- RUN apt-get update && apt-get install -y nginx: Docker starts a temporary container from the base Ubuntu image and executes the commands. The resulting changes (Nginx packages installed, configuration files added) constitute a **new layer** that sits on top of the Ubuntu layers. This layer contains only the differences (files added/modified compared to the base).
- COPY index.html ...: Docker copies the local index.html file into the temporary container's filesystem (at the specified location). This adds/updates that file in the container, and these changes become an additional **final layer**.

The resulting image therefore has: the Ubuntu layers, a layer for the Nginx installation, and a layer for copying the HTML file. If we wanted to create another similar image (for example, another static site also based on Ubuntu and Nginx), Docker would reuse the existing Ubuntu and Nginx layers, only needing to add the layer with the new specific files. This shows how layers allow **extending existing images** by adding only what is additionally needed[\[32\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=This%20is%20beneficial%20because%20it,look%20similar%20to%20the%20following). During container execution, the read-only layers of the newly created Nginx image can be shared among all containers that use it, while any files that differ between containers (e.g., customizations made at runtime) remain confined to their respective writable layers. Ultimately, the layered architecture of Docker images is a fundamental element for **portability** (I can download only what is needed), **consistency** (each layer is immutable and reproducible), and **efficiency** (maximum reuse of common components).

## 5\. Registries and distribution

**What happens with docker pull:** We have seen that docker pull downloads an image from a registry, but let's briefly analyze the process. When we execute docker pull name:tag, the Docker client connects to the registry (by default Docker Hub, or another if specified in the name or config) and performs a series of HTTP API calls. Firstly, it requests the **manifest** of the image - the manifest is a document (in JSON format) that lists the digests of all image layers and metadata (such as the hash of the image's config JSON, architecture, etc.). The registry responds with the manifest, which can be of two types: a _manifest list_ (multi-architecture index containing references to manifests for different platforms) or a single manifest for a specific platform[\[33\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=There%20are%20currently%20two%20types,and%20a%20manifest)[\[34\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=Pull%20a%20manifest). If the Docker client receives a manifest list, it will choose the manifest suitable for the host system (e.g., choosing linux/amd64 layers if we are on an x86_64 PC)[\[35\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=Instead%20of%20blobs%2C%20the%20client,its%20operating%20system%20and%20architecture)[\[36\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=Suppose%20a%20client%20chooses%20the,architecture%20and%20the%20manifest%20digest). Once the specific manifest is obtained, Docker proceeds to **download the listed layers**: for each layer digest, it makes a GET request to the registry to retrieve the corresponding blob[\[7\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=When%20you%20initiate%20a%20pull%2C,a%20manifest%20from%20the%20registry). The layers are downloaded in parallel (to speed up the process) and saved in the local cache (typically under /var/lib/docker/ in the storage driver in use). If some layers were already present (perhaps because they are common with other images), Docker skips them. Once the download of all blobs is complete, Docker composes or updates the image locally (by writing the config JSON that describes the image and referencing the local layers). At this point, the image is available to be run. In summary, docker pull is a process of **downloading image components** orchestrated via REST calls: first the manifest (or multi-arch index), then the layer blobs[\[37\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=A%20Podman%20or%20Docker%20,image%20manifest%20is%20being%20pulled)[\[7\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=When%20you%20initiate%20a%20pull%2C,a%20manifest%20from%20the%20registry). From the user's perspective, this translates into "Pull complete" messages for each layer and finally "Downloaded newer image for name:tag".

**Public vs. Private Registries:** _Container registries_ can be public (open to everyone) or private. A **public registry** like Docker Hub allows anyone to download images published there (and often to publish their own, with limits for free accounts). Docker Hub contains thousands of images, including _Docker Official Images_ (official images for common software, maintained by the community or the manufacturers themselves, e.g., nginx, mysql, etc.). Other public registries include the GitHub Container Registry (GHCR), Google Artifact Registry, Red Hat Quay, etc., where you often find images related to open-source projects or specific products. A **private registry**, on the other hand, is accessible only to authenticated and authorized users or systems. It can be a cloud service (for example, Amazon ECR for AWS, GitLab Container Registry integrated into GitLab, DigitalOcean Container Registry, etc.) or a self-hosted registry instance that the company manages in-house (like Harbor or a private instance of Nexus/Artifactory with Docker support). From Docker's perspective, interacting with a private or public registry is similar, except that for private ones we often need to log in (`docker login`) to obtain an access token. **Key Differences:** Public registries are useful for open distribution and community sharing (with possible bandwidth or storage limitations for free tiers), while private registries ensure confidentiality and control over who can download images – they are essential when working with images containing proprietary code or that we do not want to make public. Docker Hub by default only searches for public images (if it doesn't find a specific tag, it assumes an official library if one exists), but we can specify an alternative registry in image names (e.g., `myregistry.example.com:5000/my-team/my-image:1.0`). In enterprise contexts, the use of private registries also allows connecting CI/CD pipelines to automatically publish new image builds in a secure location and to control versions in production. In short, **public vs. private** boils down to open vs. access-restricted, but technologically both provide the same layer distribution APIs[\[38\]](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-registry/#:~:text=An%20image%20registry%20is%20a,and%20is%20the%20default%20registry)[\[39\]](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-registry/#:~:text=While%20Docker%20Hub%20is%20a,Artifactory%2C%20GitLab%20Container%20registry%2C%20etc).

**Image Versioning - Tags and Digests:** Docker images are identified in two ways: by a human-readable _tag_ (e.g., `nginx:1.25`) or by a cryptographic _digest_ (e.g., `nginx@sha256:<hash>`). A **tag** is essentially an alias that points to a specific version of an image within a registry repository. For example, `nginx:latest` is a conventional tag that indicates "the latest available version of Nginx" (at the time it is updated by the maintainer). Tags are convenient but **mutable**: image publishers can move a tag to a new build (for example, updating `latest` to a new version). A **digest**, on the other hand, is a unique identifier calculated from the image's content: Docker uses SHA-256 digests, represented as a long hexadecimal hash[\[40\]](https://docs.docker.com/engine/containers/run/#:~:text=Images%20using%20the%20v2%20or,the%20digest%20value%20is%20predictable). Every image pushed to a registry will have its own digest (viewable, for example, after a pull as "Digest: sha256:<...>"). That digest changes if and only if any byte of the image (layer or metadata) changes. **Using the digest** guarantees referring to an exact, immutable build – e.g., you can do `docker pull nginx@sha256:abcdef...` to download precisely that image, regardless of tags. Therefore, while tags serve for convenience (e.g., `1.0`, `2.0`, `latest`), digests serve for **immutability and verifiability**[\[41\]](https://docs.docker.com/engine/containers/run/#:~:text=). Many CI/CD workflows promote the use of digests to pin exact versions, avoiding surprises if a tag is reused for a different version. In Kubernetes environments, for example, you can specify an image with its digest to ensure that the deployment does not pull an unexpected newer version with the same tag.

**Image Signing (security):** In addition to tags and digests, Docker also supports **cryptographic image signing** through a feature called Docker Content Trust (based on an open-source project called Notary). By enabling Content Trust, every image push is digitally signed by the publisher, and every pull verifies the signature, ensuring that the image has not been altered and genuinely originates from the declared publisher[\[42\]](https://help.sonatype.com/en/docker-content-trust.html#:~:text=Docker%20Content%20Trust). In practice, a private/public key system is used: whoever builds the image signs it with their private key; whoever downloads it, if verification is enabled, will use the corresponding public key to check that the image's digest matches the one signed by the producer. This prevents _man-in-the-middle_ attacks or the unintentional use of tampered images. Docker Hub supports Content Trust, as do other registries (or it can be implemented in internal pipelines). Furthermore, alternative technologies like **Sigstore Cosign** (an open-source CNCF project) have emerged to sign and verify containers in a way that is even more integrated into cloud-native pipelines. The important conceptual aspect to understand is: the digest ensures **integrity** (detects unauthorized modifications to the content, as the hash no longer matches), while the digital signature also adds **authenticity** (guarantees who produced that image, i.e., that the hash corresponds to an image signed by the holder of a trusted private key)[\[43\]](https://www.cncf.io/blog/2021/07/28/enforcing-image-trust-on-docker-containers-using-notary/#:~:text=match%20at%20L267%20thus%20improving,container%20image%20trust%20using%20Docker)[\[42\]](https://help.sonatype.com/en/docker-content-trust.html#:~:text=Docker%20Content%20Trust). In contexts where supply chain security is critical, verifying image signatures before executing them is highly recommended.

## 6\. Conceptual demo: what happens with docker run nginx

To consolidate the concepts, let's do a **step-by-step narration** of what happens when we execute a concrete Docker command. Let's imagine we launch:

```
docker run nginx
```

What happens "behind the scenes" when we press Enter?

- **Image Resolution:** The Docker client interprets the name nginx as nginx:latest on the default registry (Docker Hub). It then contacts Docker Hub to check if there is an nginx:latest image. Unless an updated copy already exists locally, the Docker daemon proceeds to download the Nginx image[\[9\]](https://docs.docker.com/get-started/docker-overview/#:~:text=1,manually). On the terminal, we would see the progress of the download of the various Nginx layers (e.g., a Debian base layer, layers with Nginx binaries, etc., each with its ID). After a few seconds, the nginx:latest image is present on our host.
- **Container Creation:** Once the pull is complete (if necessary), Docker prepares a new container based on that image. This is equivalent to the internal execution of docker container create nginx:latest. In this step, Docker allocates space for the container: it creates a directory for the container's writable layer, associates a unique identifier with the container (a short hash, e.g., d64f1abcbc23), opens the necessary network ports (for Nginx, port 80 by default, even if not exposed to the host until we publish a port), and sets the execution parameters (entrypoint, default environment variables, working directory, etc., as defined by the image). The container is now in the _Created_ state: it exists but nothing has been executed yet[\[44\]](https://last9.io/blog/docker-container-lifecycle/#:~:text=The%20Created%20State). At this stage, Docker has already built the container's filesystem by merging the Nginx layers with a new empty writable layer.
- **Isolation and Resources:** Before starting the Nginx process, Docker configures isolation. It applies a PID namespace for the container (the Nginx process will have PID 1 inside the container), a network namespace (it creates a virtual eth0 interface in the container connected via a bridge to the host network, so Nginx can only communicate through this isolated channel), a mount namespace (the container will see the prepared layers as its root filesystem, not the host's files), and so on[\[13\]](https://docs.docker.com/get-started/docker-overview/#:~:text=The%20underlying%20technology). Concurrently, it enables default cgroup limits (unless otherwise specified, this usually means the container can use all host resources, but still tracked in its own separate cgroup). Essentially, Docker creates a _separate environment_ where Nginx will be executed: it's as if it puts Nginx in a room where it only has its own system view, isolated from others.

- **Starting the process in the container:** Docker now _starts_ the container, equivalent to running docker start. This involves executing the main command defined in the Nginx image. The official Nginx image has the Nginx daemon itself as its **entrypoint** (entry point) (in foreground mode). So Docker executes, inside the isolated container, the nginx binary with the expected parameters (often nginx -g "daemon off;" to keep it in the foreground). From the host's perspective, a new process appears (visible, for example, with ps -ef as something like nginx: master process nginx -g 'daemon off;' with an arbitrary PID, e.g., 4721, belonging to the root user if the image runs as root). But that process is special: it's anchored to dedicated namespaces, so on the host it has PID 4721, but inside the container it's PID 1. Nginx starts and reads its configuration (in /etc/nginx/nginx.conf inside the container), opens port 80 _inside_ the container. Docker, having no port options in this command, does not publish port 80 on the host - so Nginx only responds to requests made from the container itself or on that isolated network. (If we had run docker run -p 8080:80 nginx, Docker would have created a forward from host port 8080 to container port 80).
- **Container running:** At this point, the Nginx container is in full **Running state**. We can verify this with docker ps: we will see the active nginx container with an ID (the same approximately 12 characters created earlier) and status "Up X seconds". Nginx is running as if it were on its own mini-machine: if we open a shell inside (docker exec -it &lt;container&gt; bash) and issue commands, we will see that the filesystem only has Nginx directories, the active processes are only Nginx's (e.g., the master and workers), the hostname is something like the abbreviated container ID, the network is different (address probably 172.17.x.y). In short, the Nginx web service is operational in its container. If another developer were to run docker run nginx on their computer now, they would get the exact same environment in a few seconds, demonstrating portability.
- **Stopping and removal:** When we decide to stop the container, we have two options: send a stop signal (e.g., docker stop, which typically sends SIGTERM to the PID1 process in the container - Nginx will intercept the signal and terminate gracefully) or simply docker rm -f which forces termination. Let's assume we do docker stop: Docker signals Nginx to stop; Nginx closes connections and terminates. The container transitions to Exited state. At this point, we can restart it (docker start &lt;ID&gt;) if we want to restart Nginx, perhaps because we need to debug or verify something. If, on the other hand, we do docker rm on the stopped container, Docker will delete that container: the process had already terminated, so now the writable layer and meta-information are freed. However, the nginx:latest image remains in the local cache, ready to run other containers in the future without needing to be downloaded again.

**Final Analogy:** As mentioned, a Docker container is conceptually **similar to a runtime instance of a pre-packaged application**, just as an object is an instance of a class in a program. The image acts as the "class" or template (e.g., it defines that there will be an Nginx server with certain files), the container is the concrete, running instance of that template[\[45\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=When%20a%20Docker%20user%20runs,though%2C%20most%20images%20include%20some). Just as multiple objects of the same class can exist simultaneously with different states, we can launch multiple containers from the same Nginx image (each might have a different IP address or different temporary files created during execution, but all start from the same immutable base content). Another effective analogy is that of a _container_ in physical terms: the image is like the **blueprint of a standard container** (e.g., a shipping container with specifications of what it contains), while the Docker runtime container is like a **shipped container** in reality, with the "cargo" (processes and resources) inside, ready for use. Wherever you take it (whether on my PC, on the production server, or in a cloud), that container will always have the same things inside and will function in the same way, as long as the container remains intact. Docker thus provides developers with a way to achieve consistent and replicable environments: if it works inside a container on my laptop, it will work in a container on any other machine with Docker - because **"what's inside the container"** (minimal operating system, dependencies, configurations, application) **remains identical**[\[46\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=Think%20of%20a%20container%20as,containers%20to%20run%20the%20applications). This second episode has shown how Docker, through layered images, containers isolated via kernel features, and efficient filesystem management, manages to put "an application in a box" ensuring portability and consistency. In the next episode, we could delve into how to orchestrate multiple containers together or how to debug and configure containers in real-world scenarios, continuing our journey into the world of Docker.

**References:** Docker Docs, CNCF, Red Hat, DigitalOcean, _et al._ - for definitions and insights on the concepts of containers, images, copy-on-write, namespaces/cgroups, and best practices in using Docker[\[6\]](https://docs.docker.com/get-started/docker-overview/#:~:text=Docker%20registries)[\[1\]](https://docs.docker.com/get-started/docker-overview/#:~:text=)[\[12\]](https://www.digitalocean.com/community/tutorials/working-with-docker-containers#:~:text=Images%20come%20to%20life%20with,are%20taken%20to%20preserve%20them)[\[16\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=What%20Are%20cgroups%3F)[\[21\]](https://aws.amazon.com/compare/the-difference-between-containers-and-virtual-machines/#:~:text=Containers%20virtualize%20the%20operating%20system,give%20some%20more%20differences%20below). This guide has cited several authoritative sources throughout the text to provide confirmation and further details on each topic covered. Happy exploring with Docker!

[\[1\]](https://docs.docker.com/get-started/docker-overview/#:~:text=) [\[2\]](https://docs.docker.com/get-started/docker-overview/#:~:text=You%20might%20create%20your%20own,compared%20to%20other%20virtualization%20technologies) [\[3\]](https://docs.docker.com/get-started/docker-overview/#:~:text=) [\[6\]](https://docs.docker.com/get-started/docker-overview/#:~:text=Docker%20registries) [\[8\]](https://docs.docker.com/get-started/docker-overview/#:~:text=When%20you%20run%20this%20command%2C,using%20the%20default%20registry%20configuration) [\[9\]](https://docs.docker.com/get-started/docker-overview/#:~:text=1,manually) [\[13\]](https://docs.docker.com/get-started/docker-overview/#:~:text=The%20underlying%20technology) [\[30\]](https://docs.docker.com/get-started/docker-overview/#:~:text=with%20a%20simple%20syntax%20for,compared%20to%20other%20virtualization%20technologies) [\[31\]](https://docs.docker.com/get-started/docker-overview/#:~:text=it,compared%20to%20other%20virtualization%20technologies) What is Docker? | Docker Docs

<https://docs.docker.com/get-started/docker-overview/>

[\[4\]](https://docs.docker.com/engine/containers/run/#:~:text=Docker%20runs%20processes%20in%20isolated,tree%20separate%20from%20the%20host) [\[40\]](https://docs.docker.com/engine/containers/run/#:~:text=Images%20using%20the%20v2%20or,the%20digest%20value%20is%20predictable) [\[41\]](https://docs.docker.com/engine/containers/run/#:~:text=) Running containers | Docker Docs

<https://docs.docker.com/engine/containers/run/>

[\[5\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=An%20image%20is%20a%20snapshot,a%20container%20runs%20the%20software) [\[20\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=Since%20the%20container%20runs%20natively,you%20configure%20it%20that%20way) [\[45\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=When%20a%20Docker%20user%20runs,though%2C%20most%20images%20include%20some) [\[46\]](https://circleci.com/blog/docker-image-vs-container/#:~:text=Think%20of%20a%20container%20as,containers%20to%20run%20the%20applications) Docker image vs container: What are the differences? | CircleCI

<https://circleci.com/blog/docker-image-vs-container/>

[\[7\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=When%20you%20initiate%20a%20pull%2C,a%20manifest%20from%20the%20registry) [\[33\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=There%20are%20currently%20two%20types,and%20a%20manifest) [\[34\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=Pull%20a%20manifest) [\[35\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=Instead%20of%20blobs%2C%20the%20client,its%20operating%20system%20and%20architecture) [\[36\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=Suppose%20a%20client%20chooses%20the,architecture%20and%20the%20manifest%20digest) [\[37\]](https://www.redhat.com/en/blog/pull-container-image#:~:text=A%20Podman%20or%20Docker%20,image%20manifest%20is%20being%20pulled) What happens when you pull a container image?

<https://www.redhat.com/en/blog/pull-container-image>

[\[10\]](https://last9.io/blog/docker-container-lifecycle/#:~:text=Every%20container%20typically%20goes%20through,general%20flow%20stays%20the%20same) [\[11\]](https://last9.io/blog/docker-container-lifecycle/#:~:text=The%20Paused%20State) [\[44\]](https://last9.io/blog/docker-container-lifecycle/#:~:text=The%20Created%20State) Docker Container Lifecycle: Key States and Best Practices | Last9

<https://last9.io/blog/docker-container-lifecycle/>

[\[12\]](https://www.digitalocean.com/community/tutorials/working-with-docker-containers#:~:text=Images%20come%20to%20life%20with,are%20taken%20to%20preserve%20them) Working with Docker Containers | DigitalOcean

<https://www.digitalocean.com/community/tutorials/working-with-docker-containers>

[\[14\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=Namespaces%20and%20Containers) [\[15\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=The%20crucial%20thing%20to%20notice,isolated%20within%20my%20own%20namespace) [\[16\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=What%20Are%20cgroups%3F) [\[17\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=A%20control%20group%20,of%20a%20collection%20of%20processes) [\[18\]](https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work#:~:text=,cgroup%20with%20a%20single%20command) What Are Namespaces and cgroups, and How Do They Work? - NGINX Community Blog

<https://blog.nginx.org/blog/what-are-namespaces-cgroups-how-do-they-work>

[\[19\]](https://aws.amazon.com/compare/the-difference-between-containers-and-virtual-machines/#:~:text=Containers%20virtualize%20the%20operating%20system,give%20some%20more%20differences%20below) [\[21\]](https://aws.amazon.com/compare/the-difference-between-containers-and-virtual-machines/#:~:text=Containers%20virtualize%20the%20operating%20system,give%20some%20more%20differences%20below) Containers vs VM - Difference Between Deployment Technologies - AWS

<https://aws.amazon.com/compare/the-difference-between-containers-and-virtual-machines/>

[\[22\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Docker%20supports%20multiple%20storage%20drivers,storage%20driver) [\[23\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Storage%20Driver%3A%20overlay2) [\[25\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Docker%20uses%20the%20overlay%20filesystem,top%20of%20the%20image%20layers) [\[26\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=echo%20%27Add%20a%20new%20line%27,1) [\[27\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=The%20original%20file%20present%20inside,layer%60%20is%20created) [\[29\]](https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/#:~:text=Docker%20uses%20the%20overlay%20filesystem,top%20of%20the%20image%20layers) Desacralizing the Linux overlay filesystem in Docker | Adaltas

<https://www.adaltas.com/en/2021/06/03/linux-overlay-filesystem-docker/>

[\[24\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=1,chroot) [\[28\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=Each%20layer%20in%20an%20image,look%20at%20a%20theoretical%20image) [\[32\]](https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#:~:text=This%20is%20beneficial%20because%20it,look%20similar%20to%20the%20following) Understanding the image layers | Docker Docs

<https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/>

[\[38\]](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-registry/#:~:text=An%20image%20registry%20is%20a,and%20is%20the%20default%20registry) [\[39\]](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-registry/#:~:text=While%20Docker%20Hub%20is%20a,Artifactory%2C%20GitLab%20Container%20registry%20etc) What is a registry? | Docker Docs

<https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-registry/>

[\[42\]](https://help.sonatype.com/en/docker-content-trust.html#:~:text=Docker%20Content%20Trust) Docker Content Trust

<https://help.sonatype.com/en/docker-content-trust.html>

[\[43\]](https://www.cncf.io/blog/2021/07/28/enforcing-image-trust-on-docker-containers-using-notary/#:~:text=match%20at%20L267%20thus%20improving,container%20image%20trust%20using%20Docker) Enforcing image trust on Docker containers using Notary | CNCF

<https://www.cncf.io/blog/2021/07/28/enforcing-image-trust-on-docker-containers-using-notary/>
