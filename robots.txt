# robots.txt
# Rispetta il traffico umano e vieta un crawling aggressivo/automatizzato.

User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: Omgilibot
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: *
Disallow:
Crawl-delay: 10

# Sitemap: aggiungi l'URL della sitemap quando disponibile (es. https://<tuo-dominio>/sitemap.xml)
