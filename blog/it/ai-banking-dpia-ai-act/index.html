<!DOCTYPE html>
<html lang="it" translate="no">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AI nel banking: tra DPIA e AI Act | Mirko Calcaterra</title>
  <meta name="description" content="AI nel banking: tra DPIA e AI Act Abstract Il settore bancario sta adottando l&#39;IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarend‚Ä¶">
  <meta name="author" content="Mirko Calcaterra">
  <link rel="canonical" href="https://rkomi98.github.io/MyBlog/blog/it/ai-banking-dpia-ai-act/">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9EVQ8G9W48"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9EVQ8G9W48');
  </script>

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://rkomi98.github.io/MyBlog/blog/it/ai-banking-dpia-ai-act/">
  <meta property="og:title" content="AI nel banking: tra DPIA e AI Act">
  <meta property="og:description" content="AI nel banking: tra DPIA e AI Act Abstract Il settore bancario sta adottando l&#39;IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarend‚Ä¶">
  <meta property="og:image" content="https://rkomi98.github.io/MyBlog/Assets/Logo.png">
  <meta property="article:published_time" content="2025-12-30T00:00:00.000Z">
  <meta property="article:author" content="Mirko Calcaterra">
  <meta property="article:section" content="Privacy">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:title" content="AI nel banking: tra DPIA e AI Act">
  <meta property="twitter:description" content="AI nel banking: tra DPIA e AI Act Abstract Il settore bancario sta adottando l&#39;IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarend‚Ä¶">
  <meta property="twitter:image" content="https://rkomi98.github.io/MyBlog/Assets/Logo.png">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "AI nel banking: tra DPIA e AI Act",
    "image": "https://rkomi98.github.io/MyBlog/Assets/Logo.png",
    "datePublished": "2025-12-30T00:00:00.000Z",
    "dateModified": "2025-12-30T09:47:10.975Z",
    "author": {
      "@type": "Person",
      "name": "Mirko Calcaterra",
      "url": "https://rkomi98.github.io/MyBlog/"
    },
    "publisher": {
      "@type": "Person",
      "name": "Mirko Calcaterra"
    },
    "description": "AI nel banking: tra DPIA e AI Act Abstract Il settore bancario sta adottando l&#39;IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarend‚Ä¶"
  }
  </script>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    * {
      box-sizing: border-box;
    }
    html {
      scroll-behavior: smooth;
    }
    body {
      margin: 0;
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      background: radial-gradient(120% 120% at 50% 0%, rgba(59, 130, 246, 0.18) 0%, transparent 65%), var(--bg-primary);
      color: var(--text-primary);
      transition: background 0.3s ease, color 0.3s ease;
      --bg-primary: #0f172a;
      --bg-secondary: #111c33;
      --bg-card: rgba(15, 23, 42, 0.78);
      --bg-card-strong: rgba(15, 23, 42, 0.9);
      --border: rgba(148, 163, 184, 0.24);
      --text-primary: #e2e8f0;
      --text-secondary: #cbd5f5;
      --text-muted: #94a3b8;
      --accent: #60a5fa;
      --accent-strong: #38bdf8;
      --shadow-lg: 0 28px 60px -36px rgba(15, 23, 42, 0.9);
      --code-inline-bg: rgba(6, 11, 19, 0.92);
      --code-block-bg: #050912;
      --code-border: rgba(148, 163, 184, 0.35);
      --code-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.12);
      --code-text: #f8fafc;
    }
    body[data-theme="light"] {
      --bg-primary: #f8fafc;
      --bg-secondary: #ffffff;
      --bg-card: rgba(255, 255, 255, 0.96);
      --bg-card-strong: rgba(248, 250, 252, 0.98);
      --border: rgba(148, 163, 184, 0.18);
      --text-primary: #0f172a;
      --text-secondary: #334155;
      --text-muted: #64748b;
      --accent: #2563eb;
      --accent-strong: #1d4ed8;
      --shadow-lg: 0 28px 50px -38px rgba(15, 23, 42, 0.18);
      background: radial-gradient(120% 120% at 50% 0%, rgba(59, 130, 246, 0.12) 0%, transparent 60%), var(--bg-primary);
    }
    body[data-theme="light"] .post-toc {
      background: rgba(255, 255, 255, 0.96);
    }
    body[data-theme="light"] .post-body {
      background: rgba(255, 255, 255, 0.96);
      color: var(--text-secondary);
    }
    body[data-theme="light"] .post-hero__category {
      background: rgba(37, 99, 235, 0.12);
      color: var(--accent-strong);
    }
    body[data-theme="light"] .post-body blockquote {
      background: rgba(37, 99, 235, 0.1);
      color: var(--text-primary);
    }
    a {
      color: inherit;
      text-decoration: none;
    }
    header.site-header {
      position: sticky;
      top: 0;
      z-index: 12;
      backdrop-filter: blur(14px);
      background: rgba(15, 23, 42, 0.85);
      border-bottom: 1px solid var(--border);
      transition: background 0.3s ease;
    }
    body[data-theme="light"] header.site-header {
      background: rgba(248, 250, 252, 0.9);
    }
    .site-header__inner {
      max-width: 1200px;
      margin: 0 auto;
      padding: 1.15rem clamp(1.5rem, 3vw, 3rem);
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 1.5rem;
    }
    .site-header__left {
      display: flex;
      align-items: center;
      gap: 1.75rem;
    }
    .logo {
      display: inline-flex;
      align-items: center;
      gap: 0.7rem;
      font-weight: 600;
      color: var(--text-primary);
      font-size: 1.05rem;
      letter-spacing: 0.01em;
    }
    .logo-img {
      width: 38px;
      height: 38px;
      border-radius: 12px;
      object-fit: cover;
      box-shadow: 0 8px 18px -12px rgba(15, 23, 42, 0.6);
    }
    .site-nav {
      display: flex;
      gap: 1.1rem;
      font-size: 0.95rem;
      font-weight: 500;
      color: var(--text-muted);
    }
    .site-nav a:hover {
      color: var(--accent);
    }
    .header-controls {
      display: flex;
      align-items: center;
      gap: 0.75rem;
    }
    .lang-btn {
      border: 1px solid var(--border);
      background: var(--bg-card);
      color: var(--text-primary);
      padding: 0.45rem 0.9rem;
      border-radius: 12px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, border 0.2s ease, transform 0.2s ease;
    }
    .lang-btn:hover:not(.lang-btn--disabled) {
      background: var(--accent);
      color: #ffffff;
      transform: translateY(-1px);
      border-color: transparent;
    }
    .lang-btn--disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }
    .theme-toggle {
      position: relative;
      width: 52px;
      height: 28px;
      border-radius: 999px;
      border: 1px solid var(--border);
      background: var(--bg-card);
      cursor: pointer;
      padding: 0;
      transition: background 0.3s ease, border 0.3s ease;
      display: flex;
      align-items: center;
    }
    .theme-toggle .theme-thumb {
      position: absolute;
      top: 50%;
      left: 4px;
      transform: translateY(-50%);
      width: 22px;
      height: 22px;
      border-radius: 50%;
      background: #ffffff;
      color: #1f2937;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.9rem;
      transition: transform 0.3s ease, background 0.3s ease, color 0.3s ease;
      box-shadow: 0 6px 18px -8px rgba(15, 23, 42, 0.6);
    }
    body[data-theme="dark"] .theme-toggle .theme-thumb {
      transform: translate(20px, -50%);
      background: #1f2937;
      color: #f8fafc;
    }
    body[data-theme="dark"] .theme-toggle {
      background: rgba(37, 99, 235, 0.2);
      border-color: rgba(37, 99, 235, 0.3);
    }
    main.page {
      max-width: 1200px;
      margin: 0 auto;
      padding: 3.5rem clamp(1.5rem, 3vw, 3rem) 4.5rem;
    }
    .post-hero {
      position: relative;
      overflow: hidden;
      background: linear-gradient(135deg, rgba(99, 102, 241, 0.22) 0%, rgba(14, 165, 233, 0.08) 60%), var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 28px;
      padding: 2.75rem;
      box-shadow: var(--shadow-lg);
      margin-bottom: 3rem;
    }
    .post-hero::after {
      content: '';
      position: absolute;
      inset: 0;
      background: radial-gradient(circle at 20% 20%, rgba(59, 130, 246, 0.22) 0%, transparent 55%);
      pointer-events: none;
    }
    .post-hero__icon {
      position: relative;
      font-size: 3.1rem;
      margin-bottom: 1.5rem;
      display: inline-flex;
      align-items: center;
      justify-content: center;
    }
    .post-hero__category {
      position: relative;
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 0.4rem 1rem;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.35);
      color: #ffffff;
      font-weight: 600;
      letter-spacing: 0.02em;
      margin-bottom: 1.25rem;
      text-transform: uppercase;
      font-size: 0.8rem;
    }
    .post-hero__title {
      position: relative;
      margin: 0 0 1.25rem;
      font-size: clamp(2.4rem, 4vw, 3.2rem);
      letter-spacing: -0.025em;
      line-height: 1.2;
      color: var(--text-primary);
    }
    .post-hero__meta {
      position: relative;
      display: flex;
      flex-wrap: wrap;
      gap: 1.25rem;
      color: var(--text-muted);
      font-size: 0.95rem;
      font-weight: 500;
    }
    .post-hero__meta span {
      display: inline-flex;
      align-items: center;
      gap: 0.45rem;
    }
    .post-layout {
      display: grid;
      grid-template-columns: minmax(220px, 300px) minmax(0, 1fr);
      gap: 2.75rem;
      align-items: flex-start;
    }
    .post-layout--single {
      grid-template-columns: minmax(0, 1fr);
    }
    .post-toc {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 22px;
      padding: 1.5rem 1.6rem 1.8rem;
      box-shadow: var(--shadow-lg);
      position: sticky;
      top: 120px;
      max-height: calc(100vh - 160px);
      overflow: hidden;
      display: flex;
      flex-direction: column;
    }
    .post-toc__header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 0.75rem;
      margin-bottom: 0.5rem;
    }
    .post-toc__title {
      text-transform: uppercase;
      font-size: 0.78rem;
      letter-spacing: 0.18em;
      font-weight: 700;
      color: var(--text-muted);
    }
    .post-toc__toggle {
      display: none;
      border: 1px solid var(--border);
      background: transparent;
      color: var(--text-secondary);
      border-radius: 999px;
      padding: 0.25rem 0.8rem;
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      align-items: center;
      gap: 0.4rem;
      transition: background 0.2s ease, border 0.2s ease, color 0.2s ease;
    }
    .post-toc__toggle:hover {
      background: rgba(96, 165, 250, 0.15);
      border-color: transparent;
      color: var(--accent);
    }
    .post-toc__content {
      margin-top: 0.6rem;
      overflow-y: auto;
      padding-right: 0.4rem;
      transition: max-height 0.25s ease, opacity 0.25s ease;
      max-height: calc(100vh - 220px);
    }
    .post-toc--collapsed .post-toc__content {
      max-height: 0;
      opacity: 0;
      margin-top: 0;
      pointer-events: none;
    }
    .post-toc__list {
      list-style: none;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      gap: 0.45rem;
    }
    .post-toc__sublist {
      margin-left: 0.85rem;
      padding-left: 0.85rem;
      border-left: 1px solid rgba(148, 163, 184, 0.35);
      margin-top: 0.4rem;
      gap: 0.35rem;
    }
    .post-toc__item {
      margin: 0;
    }
    .post-toc__link {
      color: var(--text-secondary);
      font-size: 0.95rem;
      line-height: 1.45;
      display: flex;
      align-items: flex-start;
      gap: 0.5rem;
      border-bottom: 1px dashed transparent;
      transition: color 0.2s ease, border-bottom 0.2s ease, transform 0.2s ease;
    }
    .post-toc__link:hover {
      color: var(--accent);
      border-bottom-color: rgba(96, 165, 250, 0.4);
      transform: translateX(2px);
    }
    .post-toc__link--active {
      color: var(--accent-strong);
      border-bottom-color: var(--accent-strong);
    }
    .post-toc__number {
      font-variant-numeric: tabular-nums;
      font-size: 0.85rem;
      color: var(--text-muted);
      min-width: 2.5ch;
      display: inline-flex;
      justify-content: flex-end;
      padding-top: 0.15rem;
    }
    .post-toc__text {
      flex: 1;
    }
    .post-body {
      background: var(--bg-card-strong);
      border: 1px solid var(--border);
      border-radius: 26px;
      padding: 2.5rem;
      box-shadow: var(--shadow-lg);
      font-size: 1.04rem;
      line-height: 1.75;
      color: var(--text-secondary);
    }
    .post-body h2 {
      margin-top: 2.75rem;
      margin-bottom: 1.25rem;
      font-size: clamp(1.9rem, 3vw, 2.35rem);
      color: var(--text-primary);
      letter-spacing: -0.01em;
    }
    .post-body h3 {
      margin-top: 2.2rem;
      margin-bottom: 1rem;
      font-size: 1.5rem;
      color: var(--text-primary);
    }
    .post-body h4 {
      margin-top: 1.8rem;
      margin-bottom: 0.75rem;
      font-size: 1.2rem;
      color: var(--text-primary);
    }
    .post-body p {
      margin-bottom: 1.4rem;
    }
    .post-body .post-warning {
      margin: 2rem 0;
      border-radius: 18px;
      border: 1px solid rgba(250, 204, 21, 0.35);
      background: rgba(254, 243, 199, 0.9);
      color: #4a3b0a;
      padding: 0 1.25rem 1rem;
      box-shadow: inset 0 0 0 1px rgba(255, 255, 255, 0.35);
    }
    body[data-theme="dark"] .post-body .post-warning {
      background: rgba(253, 230, 138, 0.12);
      border-color: rgba(251, 191, 36, 0.5);
      color: #f6e6b2;
      box-shadow: inset 0 0 0 1px rgba(250, 200, 88, 0.3);
    }
    .post-body .post-warning summary {
      list-style: none;
      cursor: pointer;
      font-weight: 600;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      padding: 1rem 0;
      color: inherit;
    }
    .post-body .post-warning summary::-webkit-details-marker {
      display: none;
    }
    .post-body .post-warning summary::before {
      content: '‚ö†Ô∏è';
      font-size: 1rem;
    }
    .post-body .post-warning[open] {
      padding-bottom: 1.25rem;
    }
    .post-body .post-warning p:last-child {
      margin-bottom: 0;
    }
    .post-body ul,
    .post-body ol {
      margin: 1.4rem 0 1.4rem 1.4rem;
      padding: 0;
    }
    .post-body li {
      margin-bottom: 0.8rem;
    }
    .post-body a {
      color: var(--accent);
      text-decoration: none;
      border-bottom: 1px solid rgba(96, 165, 250, 0.35);
      transition: color 0.2s ease, border-bottom 0.2s ease;
    }
    .post-body a:hover {
      color: var(--accent-strong);
      border-bottom-color: var(--accent-strong);
    }
    .post-body blockquote {
      margin: 2rem 0;
      padding: 1.5rem 1.75rem;
      border-left: 4px solid var(--accent);
      border-radius: 0 18px 18px 0;
      background: rgba(37, 99, 235, 0.12);
      color: var(--text-primary);
    }
    .post-body code {
      background: var(--code-inline-bg);
      color: var(--code-text);
      padding: 0.2rem 0.45rem;
      border-radius: 6px;
      font-family: 'JetBrains Mono', 'Fira Code', monospace;
      font-size: 0.9rem;
    }
    .post-body pre code {
      background: transparent;
      padding: 0;
      display: block;
      font-size: inherit;
      line-height: inherit;
    }
    .hljs {
      color: #e2e8f0;
      background: transparent;
    }
    .hljs-comment,
    .hljs-quote {
      color: #7dd79d;
      font-style: italic;
    }
    .hljs-keyword,
    .hljs-selector-tag,
    .hljs-literal,
    .hljs-name,
    .hljs-strong,
    .hljs-built_in {
      color: #7dd3fc;
      font-weight: 600;
    }
    .hljs-title,
    .hljs-section,
    .hljs-function,
    .hljs-meta .hljs-keyword {
      color: #38bdf8;
      font-weight: 600;
    }
    .hljs-string,
    .hljs-doctag,
    .hljs-addition,
    .hljs-attribute,
    .hljs-template-tag,
    .hljs-template-variable {
      color: #facc15;
    }
    .hljs-number,
    .hljs-symbol,
    .hljs-bullet,
    .hljs-link,
    .hljs-meta,
    .hljs-type {
      color: #f472b6;
    }
    .hljs-variable,
    .hljs-params {
      color: #cbd5f5;
    }
    .post-body pre {
      background: var(--code-block-bg);
      color: var(--code-text);
      padding: 1.2rem 1.4rem;
      padding-right: 3.6rem;
      border-radius: 18px;
      overflow-x: auto;
      font-family: 'JetBrains Mono', 'Fira Code', monospace;
      font-size: 0.95rem;
      box-shadow: var(--code-shadow);
      border: 1px solid var(--code-border);
      margin: 2rem 0;
      position: relative;
    }
    .code-copy-btn {
      position: absolute;
      top: 0.75rem;
      right: 0.75rem;
      background: rgba(15, 23, 42, 0.8);
      color: #e2e8f0;
      border: 1px solid rgba(148, 163, 184, 0.35);
      border-radius: 999px;
      padding: 0.25rem 0.85rem;
      font-size: 0.85rem;
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, border-color 0.2s ease, transform 0.2s ease;
    }
    .code-copy-btn:hover {
      background: rgba(96, 165, 250, 0.85);
      color: #ffffff;
      border-color: transparent;
      transform: translateY(-1px);
    }
    .code-copy-btn--copied {
      background: rgba(34, 197, 94, 0.85);
      color: #ffffff;
      border-color: transparent;
    }
    .code-copy-btn__icon {
      font-size: 0.95rem;
    }
    .code-copy-btn__text {
      display: inline-block;
    }
    body[data-theme="light"] .code-copy-btn {
      background: rgba(248, 250, 252, 0.85);
      color: #0f172a;
      border-color: rgba(148, 163, 184, 0.4);
    }
    body[data-theme="light"] .code-copy-btn--copied {
      background: rgba(34, 197, 94, 0.92);
      color: #ffffff;
    }
    .post-body img {
      max-width: 100%;
      border-radius: 18px;
      margin: 2.2rem 0;
      box-shadow: 0 24px 45px -28px rgba(15, 23, 42, 0.55);
    }
    .post-body .table-wrapper {
      margin: 2rem 0;
      border-radius: 18px;
      border: 1px solid var(--border);
      background: rgba(15, 23, 42, 0.55);
      box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.12);
      position: relative;
      overflow: hidden;
    }
    .post-body .table-wrapper__scroll {
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }
    .post-body .table-wrapper__scroll::-webkit-scrollbar {
      height: 10px;
    }
    .post-body .table-wrapper__scroll::-webkit-scrollbar-thumb {
      background: rgba(96, 165, 250, 0.4);
      border-radius: 999px;
    }
    .post-body .table-wrapper table {
      width: 100%;
      border-collapse: collapse;
      background: transparent;
    }
    .post-body .table-wrapper[data-table-size="medium"] table {
      min-width: 720px;
    }
    .post-body .table-wrapper[data-table-size="wide"] table {
      min-width: 960px;
    }
    .post-body .table-wrapper thead th {
      background: rgba(96, 165, 250, 0.12);
      color: var(--text-primary);
      font-weight: 600;
    }
    .post-body .table-wrapper th,
    .post-body .table-wrapper td {
      padding: 0.9rem 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(148, 163, 184, 0.18);
      white-space: nowrap;
    }
    .post-body .table-wrapper td {
      white-space: normal;
    }
    .post-body .table-wrapper tr:last-child td {
      border-bottom: none;
    }
    .post-body .table-wrapper__expand {
      position: absolute;
      top: 0.75rem;
      right: 0.75rem;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.3);
      color: var(--accent);
      border-radius: 999px;
      padding: 0.35rem 0.9rem;
      font-size: 0.85rem;
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, transform 0.2s ease;
      z-index: 2;
    }
    .post-body .table-wrapper__expand:hover {
      background: rgba(37, 99, 235, 0.35);
      color: #ffffff;
      transform: translateY(-1px);
      border-color: transparent;
    }
    .table-overlay {
      position: fixed;
      inset: 0;
      background: rgba(15, 23, 42, 0.85);
      backdrop-filter: blur(6px);
      display: none;
      align-items: center;
      justify-content: center;
      padding: 2rem;
      z-index: 999;
    }
    .table-overlay--visible {
      display: flex;
    }
    .table-overlay__content {
      background: var(--bg-card-strong);
      border: 1px solid var(--border);
      border-radius: 24px;
      max-width: min(1080px, 92vw);
      max-height: 85vh;
      width: 100%;
      box-shadow: 0 32px 80px -40px rgba(15, 23, 42, 0.9);
      position: relative;
      overflow: hidden;
    }
    .table-overlay__close {
      position: absolute;
      top: 0.85rem;
      right: 0.85rem;
      background: rgba(15, 23, 42, 0.9);
      border: 1px solid rgba(148, 163, 184, 0.35);
      color: var(--text-primary);
      border-radius: 999px;
      padding: 0.4rem 1rem;
      font-size: 0.9rem;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease;
    }
    .table-overlay__close:hover {
      background: rgba(37, 99, 235, 0.4);
      color: #ffffff;
      border-color: transparent;
    }
    .table-overlay__scroll {
      overflow: auto;
      max-height: 85vh;
      padding: 2.5rem 2rem 2rem;
    }
    .table-overlay__scroll table {
      width: 100%;
      border-collapse: collapse;
      background: transparent;
    }
    .table-overlay__scroll table[data-table-size="medium"] {
      min-width: 720px;
    }
    .table-overlay__scroll table[data-table-size="wide"] {
      min-width: 960px;
    }
    .table-overlay__scroll thead th {
      background: rgba(96, 165, 250, 0.12);
      color: var(--text-primary);
      font-weight: 600;
    }
    .table-overlay__scroll th,
    .table-overlay__scroll td {
      padding: 0.9rem 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(148, 163, 184, 0.18);
      white-space: nowrap;
    }
    .table-overlay__scroll td {
      white-space: normal;
    }
    .table-overlay__scroll tr:last-child td {
      border-bottom: none;
    }
    body[data-theme="light"] .post-body .table-wrapper {
      background: rgba(255, 255, 255, 0.96);
      box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.16);
    }
    body[data-theme="light"] .post-body .table-wrapper__expand {
      background: rgba(248, 250, 252, 0.9);
    }
    body[data-theme="light"] .table-overlay {
      background: rgba(15, 23, 42, 0.25);
    }
    body[data-theme="light"] .table-overlay__content {
      background: rgba(255, 255, 255, 0.98);
    }
    body.no-scroll {
      overflow: hidden;
    }
    footer {
      margin-top: 4rem;
      padding: 2rem 0;
      text-align: center;
      color: var(--text-muted);
      font-size: 0.92rem;
      border-top: 1px solid var(--border);
      background: rgba(15, 23, 42, 0.35);
    }
    body[data-theme="light"] footer {
      background: rgba(255, 255, 255, 0.72);
    }
    @media (max-width: 1024px) {
      .site-header__inner {
        padding: 1rem clamp(1.25rem, 4vw, 2rem);
      }
      main.page {
        padding: 2.75rem clamp(1.25rem, 4vw, 2rem) 4rem;
      }
      .post-layout {
        grid-template-columns: minmax(0, 1fr);
      }
      .post-toc {
        position: sticky;
        top: 88px;
        z-index: 6;
        max-height: calc(100vh - 140px);
        margin-bottom: 2rem;
        padding: 1.1rem 1.25rem 1.35rem;
      }
      .post-toc__toggle {
        display: inline-flex;
      }
      .post-toc__content {
        max-height: none;
        margin-top: 0.4rem;
        overflow: visible;
      }
    }
    @media (max-width: 720px) {
      .post-hero {
        padding: 2.1rem 1.65rem;
      }
      .post-body {
        padding: 1.9rem 1.5rem;
      }
      .site-header__inner {
        flex-direction: column;
        align-items: stretch;
        gap: 1rem;
      }
      .site-header__left {
        justify-content: space-between;
      }
      .header-controls {
        align-self: flex-end;
      }
      .post-hero__title {
        font-size: clamp(2rem, 6vw, 2.6rem);
      }
      .post-body .table-wrapper {
        margin: 1.6rem 0;
      }
      .post-body .table-wrapper__expand {
        top: 0.6rem;
        right: 0.6rem;
        font-size: 0.78rem;
        padding: 0.25rem 0.75rem;
      }
      .table-overlay__scroll {
        padding: 1.8rem 1.25rem 1.5rem;
      }
    }
  </style>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      },
    };
  </script>
  <script id="mathjax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body data-theme="dark">
  <header class="site-header">
    <div class="site-header__inner">
      <div class="site-header__left">
        <a class="logo" href="../../../index.html">
          <img src="../../../Assets/Logo.png" alt="Mirko Calcaterra logo" class="logo-img">
          <span class="logo-text">Mirko Calcaterra</span>
        </a>
        <nav class="site-nav">
          <a href="../../../index.html" data-it="Home" data-en="Home">Home</a>
          <a href="../../../blog/index.html" data-it="Blog" data-en="Blog">Blog</a>
        </nav>
      </div>
      <div class="header-controls">
        <button class="lang-btn" type="button">EN</button>
        <button class="theme-toggle" type="button" aria-label="Toggle theme">
          <span class="theme-thumb">‚òÄÔ∏è</span>
        </button>
      </div>
    </div>
  </header>
  <main class="page">
    <article class="post">
      <section class="post-hero">
        <div class="post-hero__icon">üõ°Ô∏è</div>
        <span class="post-hero__category">Privacy</span>
        <h1 class="post-hero__title">AI nel banking: tra DPIA e AI Act</h1>
        <div class="post-hero__meta">
          <span>üìÖ 30 dicembre 2025</span>
          <span>‚è±Ô∏è 78 min</span>
        </div>
      </section>
      <section class="post-layout">
        <aside class="post-toc" data-collapsed="false">
        <div class="post-toc__header">
          <div class="post-toc__title" data-it="Indice" data-en="Table of contents">Indice</div>
          <button class="post-toc__toggle" type="button" aria-expanded="true" aria-label="Nascondi indice">
            <span class="post-toc__toggle-text">Indice</span>
            <span class="post-toc__toggle-icon" aria-hidden="true">‚ñæ</span>
          </button>
        </div>
        <div class="post-toc__content">
          <ul class="post-toc__list">
    <li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#abstract">
            <span class="post-toc__number">1</span>
            <span class="post-toc__text">Abstract</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#introduzione">
            <span class="post-toc__number">2</span>
            <span class="post-toc__text">Introduzione</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#cosa-dicono-le-regole-e-come-muoversi">
            <span class="post-toc__number">3</span>
            <span class="post-toc__text">Cosa dicono le regole e come muoversi</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#principali-evidenze">
            <span class="post-toc__number">3.1</span>
            <span class="post-toc__text">Principali evidenze</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#use-case-1-valutazione-del-credito-credit-scoring">
            <span class="post-toc__number">4</span>
            <span class="post-toc__text">**Use Case 1: Valutazione del Credito (Credit scoring)**</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#use-case-2-contact-center-e-assistenti-virtuali-customer-service-ai">
            <span class="post-toc__number">5</span>
            <span class="post-toc__text">**Use Case 2: Contact Center e Assistenti Virtuali (Customer service AI)**</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#use-case-3-anti-money-laundering-e-monitoraggio-transazioni">
            <span class="post-toc__number">6</span>
            <span class="post-toc__text">**Use Case 3: Anti-Money Laundering e Monitoraggio Transazioni**</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#use-case-4-recruitment-e-gestione-hr-con-ai">
            <span class="post-toc__number">7</span>
            <span class="post-toc__text">**Use Case 4: Recruitment e Gestione HR con AI**</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#use-case-5-kyc-e-verifica-documentale-onboarding-clienti">
            <span class="post-toc__number">8</span>
            <span class="post-toc__text">**Use Case 5: KYC e Verifica Documentale (onboarding clienti)**</span>
          </a>
          
        </li>
  </ul>
        </div>
      </aside>
        <div class="post-body">
          <h2 id="abstract">Abstract</h2>
<p>Il settore bancario sta adottando l&#39;IA in aree critiche come credit scoring, frodi e assistenza clienti, ma il perimetro normativo sta cambiando rapidamente. Questo articolo mette in relazione GDPR e AI Act, chiarendo quando scattano DPIA e FRIA e quali obblighi operativi ne derivano. L&#39;obiettivo √® offrire una mappa pratica per valutare i rischi, distinguere i casi ad alto rischio e impostare una governance coerente.</p>
<h2 id="introduzione">Introduzione</h2>
<p>L&#39;adozione dell&#39;IA in banca non √® piu&#39; sperimentale: √® gi√† parte di processi che toccano diritti, accesso al credito e tutela dei dati. Prima di addentrarci nelle regole, serve un quadro introduttivo che colleghi i principali casi d&#39;uso ai due pilastri regolatori (GDPR e AI Act) e alle valutazioni di impatto richieste. Questo articolo fa da ponte tra tecnologia e compliance, per orientare fin da subito le scelte di progetto.</p>
<h2 id="cosa-dicono-le-regole-e-come-muoversi">Cosa dicono le regole e come muoversi</h2>
<h3 id="principali-evidenze">Principali evidenze</h3>
<p>Le nuove regole europee sull&#39;AI (Reg. UE 2024/1689, &quot;AI Act&quot;) classificano come <em>&quot;sistemi ad alto rischio&quot;</em> diversi impieghi dell&#39;Intelligenza Artificiale (IA) nel settore bancario, imponendo requisiti stringenti<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a>. </p>
<p>In particolare, l&#39;uso dell&#39;AI per valutare l&#39;affidabilit√† creditizia (credit scoring) di persone fisiche rientra nell&#39;Allegato III ed √® quindi considerato <em>high-risk</em><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a>. Analogamente, i sistemi IA impiegati nel processo di <strong>reclutamento e gestione del personale</strong> (es. selezione candidati, decisioni di assunzione o promozione) sono esplicitamente elencati tra gli usi ad alto rischio, dato il potenziale impatto sui diritti dei lavoratori e il rischio di discriminazioni<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati">[2]</a>. </p>
<p>Altri casi d&#39;uso tipici in banca - come sistemi di <strong>chatbot o assistenti virtuali per clienti</strong>, strumenti di <strong>anti-riciclaggio (AML) e rilevazione frodi</strong>, sistemi di <strong>verifica identit√† (KYC)</strong> con riconoscimento biometrico, ecc. - pur non tutti ricadendo in categorie &quot;alto rischio&quot; per l&#39;AI Act, comportano comunque significativi obblighi di conformit√†. In particolare, l&#39;AI Act esenta dal novero <em>high-risk</em> i sistemi IA usati per individuare frodi finanziarie o per calcoli patrimoniali prudenziali<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20previsti,ad%20alto%20rischio%20ai%20sensi">[3]</a>; ci√≤ significa che, paradossalmente, algoritmi di <strong>fraud detection</strong> o di <strong>monitoraggio transazioni sospette AML</strong> non sono soggetti ai requisiti di alto rischio dell&#39;AI Act, pur dovendo rispettare la normativa di settore (es. antiriciclaggio) e privacy. </p>
<blockquote>
<p>Resta fermo, in ogni caso, l&#39;obbligo per le banche di effettuare una valutazione caso-per-caso: ad esempio, un sistema di IA bancario che, pur non rientrando espressamente nell&#39;Allegato III, presenti rischi significativi per diritti, sicurezza o accesso a servizi essenziali potrebbe essere prudentemente trattato come <em>alto rischio</em> ai fini interni.</p>
</blockquote>
<p>Le fonti regolamentari primarie delineano una serie di <strong>checklist obbligatorie</strong> e buone pratiche applicabili. Sul versante privacy, il GDPR impone la <strong>Data Protection Impact Assessment (DPIA)</strong> per trattamenti con rischio elevato, e le autorit√† (EDPB e Garante Privacy) hanno elencato criteri e casi tipici: ad esempio, profilazione o scoring su larga scala, decisioni automatizzate con effetti significativi (come concessione di un prestito), monitoraggio sistematico di utenti, uso di dati sensibili o di tecnologie innovative come l&#39;IA<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=la%20concessione%20di%20un%20finanziamento">[5]</a>. </p>
<p>Molti use case di IA bancari soddisfano tali criteri come per esempio:</p>
<ul>
<li>credit scoring = profilazione finanziaria con impatto su diritti;</li>
<li>monitoraggio transazioni = controllo sistematico;</li>
<li>riconoscimento facciale = dato biometrico;</li>
</ul>
<blockquote>
<p><em>Questi richiedono quindi una valutazione preventiva d&#39;impatto sulla privacy (DPIA)</em> </p>
</blockquote>
<p>In parallelo, l&#39;AI Act introduce per gli utilizzatori (<em>deployers</em>) di sistemi AI ad alto rischio l&#39;obbligo di condurre una <strong>Valutazione d&#39;Impatto sui Diritti Fondamentali (FRIA)</strong> prima della messa in uso<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista">[7]</a>. </p>
<p>In ambito bancario ci√≤ riguarda in particolare gli enti che impiegano sistemi di AI Act Annex III, ad esempio le banche che usano IA per credit scoring (Allegato III ¬ß5(b)) o per decisioni su polizze vita/sanitarie. Tale FRIA deve considerare contesto d&#39;uso, categorie di interessati coinvolti, rischi per diritti (es. bias, esclusione), misure di controllo umano previste e azioni di rimedio<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Obblighi%20dei%20fornitori%20di%20modelli%20di%20IA%20per%20finalit%C3%A0%20generali%20con%20rischio%20sistemico,%20In%20aggiunta">[8]</a>. Va notificata all&#39;autorit√† di vigilanza di mercato competente con i relativi risultati<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=obbligo%20di%20notifica">[9]</a>, inserendosi quindi come adempimento formale prima dell&#39;uso del sistema.</p>
<p>Un tema chiave emerso √® la <strong>sovrapposizione e distinzione</strong> tra DPIA (focalizzata sui rischi privacy GDPR) e FRIA (focalizzata su impatti <em>etici e diritti fondamentali</em> pi√π ampi). In caso di sistemi IA che trattano dati personali, il legislatore prevede che la FRIA <em>si integri</em> con la DPIA gi√† svolta, evitando duplicazioni: in pratica, la <strong>DPIA copre privacy e sicurezza dati</strong>, mentre la <strong>FRIA estende l&#39;analisi a discriminazione, accesso equo a servizi, trasparenza verso gli interessati</strong>, ecc. </p>
<p>Ad esempio, per un algoritmo di concessione prestiti, la DPIA valuter√† liceit√† e proporzionalit√† del trattamento dati, minimizzazione, misure di sicurezza e anonimizzazione; la FRIA aggiunger√† la valutazione dei rischi di <em>bias algoritmico</em>, impatto socio-economico (es. esclusione finanziaria di gruppi vulnerabili) e l&#39;adeguatezza delle misure di <strong>human oversight</strong> adottate. Quest&#39;ultimo punto, la supervisione umana, ricorre come principio basilare: tanto il GDPR (art.22) quanto l&#39;AI Act e le linee guida di settore insistono sulla necessit√† che l&#39;IA <em>non prenda decisioni completamente autonome senza possibilit√† di intervento umano</em>. </p>
<p>Le <strong>Linee Guida EBA 2020 sul governo del credito</strong> hanno chiarito che l&#39;uso di modelli automatizzati √® consentito solo entro confini precisi, includendo <em>trasparenza, tracciabilit√†, supervisione costante ed evitando ogni forma di discriminazione</em><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,supervisione%20costante">[11]</a>. </p>
<p>Allo stesso modo, Banca d&#39;Italia ha ribadito che l&#39;IA pu√≤ supportare la valutazione del rischio creditizio <em>ma non sostituire il giudizio umano</em>: la banca rimane responsabile ultima di correttezza e legittimit√† delle decisioni<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=La%20Banca%20d'Italia,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[12]</a>. </p>
<p><strong>L&#39;imprescindibilit√† del controllo umano</strong> √® sancita anche a livello di principi internazionali: vi √® ampia convergenza sull&#39;importanza di mantenere un intervento umano significativo (&quot;human-in-the-loop&quot;) nei processi decisionali automatizzati, per garantire accountability e tutela dei diritti<a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=convergenza%20sui%20principi,traduzione%20di%20questi%20principi%20in">[13]</a>. Nella pratica bancaria ci√≤ si traduce, ad esempio, nel prevedere che le decisioni pi√π impattanti (es. rifiuto di credito, segnalazione di operazione sospetta) siano riesaminate o avallate da personale competente, e che gli operatori addetti abbiano <em>formazione adeguata sull&#39;IA</em> e potere di fermare o correggere il sistema<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la">[14]</a>. </p>
<p>I grandi istituti si stanno infatti orientando verso modelli &quot;ibridi&quot; in cui l&#39;IA elabora raccomandazioni ma l&#39;uomo ha l&#39;ultima parola, investendo in programmi di <strong>upskilling</strong> del personale per colmare il gap di competenze tecniche<a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent">[16]</a>. Ci√≤ risponde anche a esigenze di gestione del rischio: come osservato dall&#39;EBA, molte banche adottano un approccio graduale e prudente all&#39;AI, introducendo solide <strong>misure di controllo e &quot;guardrails&quot;</strong> prima di estendere l&#39;uso di modelli avanzati, specie di tipo generativo, e testando i casi d&#39;uso pi√π rischiosi solo dopo aver maturato sufficiente esperienza e confidenza nella tecnologia<a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences">[17]</a>.</p>
<p>Dal punto di vista strettamente normativo, emergono <em>nuovi obblighi operativi</em>: ad esempio requisiti di <strong>trasparenza verso gli utenti</strong>. Il regolamento AI Act impone (anche per sistemi non high-risk) che chi interagisce con un&#39;IA sia informato del fatto che sta interagendo con una macchina e non un essere umano<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a>. Ci√≤ significa che in un contact center automatizzato o chatbot bancario dev&#39;essere chiaramente segnalato al cliente che il servizio √® erogato da un sistema AI, e occorre prevedere canali di <em>escalation a un operatore umano</em> su richiesta.</p>
<p>Analogamente, se lo strumento di GenAI (Generative Artificial Intelligence) genera contenuti (es. una risposta testuale in linguaggio naturale al cliente), vanno forniti eventuali disclaimer sull&#39;origine automatica e accuratezza delle informazioni. Sul fronte <strong>non-discriminazione e fairness</strong>, sebbene la normativa bancaria italiana contenga solo generiche clausole di equit√† nell&#39;erogazione del credito, il nuovo quadro richiede una grande attenzione: l&#39;AI Act inserisce esplicitamente il divieto di algoritmi che introducano classificazioni basate su caratteristiche sensibili (pratiche assimilabili al <em>social scoring</em> sono vietate) e, come detto, include il credito tra i casi ad alto rischio proprio per i possibili <em>bias algoritmici</em><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a>. I garanti privacy e le autorit√† di consumo potranno sindacare pratiche scorrette se un modello nega sistematicamente servizi a categorie protette. Pertanto, le banche devono implementare tecniche di <strong>AI governance</strong>: test proattivi dei modelli per rilevare disparit√† di trattamento (bias audit), documentazione trasparente delle variabili usate (feature importance), e meccanismi di reclamo efficaci per gli interessati. Un cliente ha diritto di sapere se una decisione sul suo finanziamento √® stata influenzata da un algoritmo e su quali parametri<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori">[19]</a>, nonch√© di ottenere intervento umano e contestare la decisione automatizzata - come previsto dal GDPR art.22.</p>
<p><strong>Principali ambiguit√† normative:</strong> Nonostante i progressi regolamentari, permangono aree grigie e nodi interpretativi. Un primo elemento di incertezza riguarda la <em>portata esatta delle categorie di alto rischio</em>: alcuni use case bancari non rientrano in modo netto nell&#39;Allegato III. Ad esempio, l&#39;utilizzo di IA per <strong>Anti-Money Laundering</strong> (rilevazione di operazioni sospette) non √® elencato tra gli high-risk a meno che avvenga direttamente da autorit√† di contrasto - e infatti il legislatore UE ha escluso i sistemi usati dalle FIU dagli ambiti di polizia<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo">[20]</a>. Ci√≤ solleva dubbi: un algoritmo che <em>blocca preventivamente</em> transazioni o conti correnti potrebbe incidere su diritti fondamentali (es. libert√† economica) quasi quanto un sistema di credit scoring, ma formalmente l&#39;AI Act non lo copre come high-risk. La scelta sembra motivata dal voler favorire l&#39;innovazione anti-frode, ma resta ambigua la linea di confine: le banche dovranno decidere se trattare questi sistemi &quot;non classificati&quot; comunque con un approccio conservativo (applicando volontariamente requisiti affini a quelli high-risk) per prudenza e accountability.</p>
<p>Ulteriore ambiguit√† concerne la <strong>definizione di &quot;rischio significativo&quot;</strong> nell&#39;AI Act. La norma prevede infatti che i sistemi elencati in Allegato III <em>non</em> siano considerati ad alto rischio se, &quot;in deroga&quot;, <em>non presentano un rischio significativo</em> per salute, sicurezza o diritti<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III">[21]</a>. Questa clausola di esenzione √® di non facile applicazione pratica: ad esempio, una piccola soluzione AI usata in via sperimentale su pochi clienti, pur tecnicamente rientrando in una categoria (mettiamo credit scoring), potrebbe essere sostenuta come a rischio trascurabile dal fornitore; tuttavia i criteri per stabilirlo non sono esplicitati e c&#39;√® il rischio di interpretazioni difformi. Le aziende potrebbero essere restie a &quot;declassare&quot; un sistema da high-risk a non, temendo contestazioni a posteriori - si profila quindi un atteggiamento prudenziale, ma la mancanza di linee guida attuative al riguardo √® un gap che richieder√† chiarimenti (la Commissione √® delegata a emettere atti per modificare l&#39;Allegato III e fornire criteri, ma occorrer√† vedere come verr√† gestito).</p>
<p>Un terzo profilo di incertezza riguarda la <strong>metodologia e governance della FRIA</strong>. Trattandosi di un adempimento nuovo, non esistono ancora standard consolidati su <em>come condurre una valutazione di impatto etico/fondamentale</em>. L&#39;AI Act prevede che l&#39;<em>AI Office</em> (nuovo organismo europeo) fornisca un modello di questionario anche via tool automatizzato per facilitare i deployer<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato">[22]</a>, ma fino a che ci√≤ non sar√† sviluppato, le imprese dovranno arrangiarsi ispirandosi a linee guida analoghe (es. quelle del Garante per valutazioni etiche, o framework come l&#39;Assessment List for Trustworthy AI dell&#39;UE). Quali competenze dovranno coinvolgere? Chi sar√† l&#39;&quot;autorit√† di notifica&quot; della FRIA in Italia per il settore bancario - il Ministero dello Sviluppo Economico, Banca d&#39;Italia o un nuovo organo? - Non √® ancora definito con precisione, e ci√≤ crea ambiguit√† operative. Inoltre, mentre per la DPIA privacy esiste l&#39;obbligo di consultare il Garante solo se residuano rischi elevati non mitigati, per la FRIA sembra esserci un obbligo generalizzato di notifica pre-uso<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica">[9]</a>, senza per√≤ un chiaro processo su cosa accade se l&#39;autorit√† ritiene la FRIA inadeguata o l&#39;uso troppo rischioso: potr√† bloccarlo? servir√† una sorta di &quot;nulla osta&quot;? Saranno temi da chiarire a livello attuativo.</p>
<p>Ambiguit√† anche nella <strong>distinzione di ruoli e responsabilit√†</strong> lungo la filiera AI. In molti casi le banche utilizzano soluzioni di IA fornite da vendor terzi o basate su modelli generativi pre-addestrati (es. un foundation model linguistico integrato nel chatbot). Il regolamento distingue <strong>&quot;fornitore&quot;</strong> (chi immette sul mercato il sistema AI) e <strong>&quot;utilizzatore (deployer)&quot;</strong> finale; nel caso bancario, per√≤, una banca che sviluppi internamente un algoritmo per uso proprio potrebbe essere considerata sia fornitore che deployer, con obblighi cumulativi (inclusa la conformit√† tecnica e marcatura CE del sistema high-risk). Se invece acquista un servizio AI esterno, dovr√† comunque garantire gli obblighi dei deployer (FRIA, registrazione nel database UE, sorveglianza d&#39;uso) ma dipende dal fornitore per la documentazione tecnica conforme. √à incerto come gestire contrattualmente questa condivisione di responsabilit√†: le banche dovranno pretendere dai vendor garanzie di conformit√† AI Act (es. <strong>EU Declaration of Conformity</strong> per sistemi high-risk) e accesso alle info sul modello per poter fare la FRIA, ma non √® ancora pratica comune.</p>
<p>Un&#39;altra area grigia √® la <strong>gestione della spiegabilit√† ed esercizio dei diritti GDPR</strong> in presenza di modelli AI opachi (es. deep learning). Il GDPR d√† all&#39;interessato diritto ad avere spiegazioni significative sulla logica di decisioni automatizzate; tuttavia, le tecniche di explainable AI sono ancora emergenti e potrebbe non essere possibile fornire spiegazioni semplici di modelli complessi. Le banche dovranno bilanciare questo obbligo con la tutela del segreto industriale sui propri algoritmi. Non esiste ancora un consenso su quale livello di trasparenza sia &quot;sufficiente&quot; - ambito in cui sono attesi orientamenti dal EDPB o dal futuro AI Office.</p>
<p>Infine, permane incertezza su <strong>come tradurre in prassi concrete i principi etici condivisi</strong>. La letteratura e le autorit√† convergono su principi come <em>non-discriminazione, trasparenza, oversight umano</em><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in">[13]</a>, ma - come notato da Banca d&#39;Italia - risulta meno agevole incorporarli in norme vincolanti e procedure operative efficaci<a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti">[23]</a>. Ad esempio, tutti concordano sull&#39;evitare bias, ma definire metriche quantitative di fairness e soglie accettabili di disparit√† √® complesso e lasciato all&#39;autonomia delle imprese per ora. Similmente, √® pacifico che debba esserci un intervento umano, ma quanta discrezionalit√† e in quale fase del processo √® adeguato? Sono aspetti su cui si naviga ancora a vista, con approcci conservativi (ad es. richiedere sempre un doppio controllo umano indipendente per certe decisioni critiche) in attesa di prassi consolidate.</p>
<p><strong>Implicazioni operative per l&#39;MVP:</strong> Le evidenze sopra delineate informano direttamente i requisiti del prototipo di tool (&quot;AI Act Navigator&quot; e &quot;FRIA/DPIA Evidence Builder&quot;). In sintesi, l&#39;MVP dovr√†: <strong>(1)</strong> incorporare un sistema di <em>triage dei casi d&#39;uso</em> basato su domande mirate che consentano di identificare se un use case ricade in categorie di <em>alto rischio AI Act</em> o presenta trigger DPIA GDPR, guidando l&#39;utente (es. un Compliance officer) nelle classificazioni corrette. <strong>(2)</strong> Dovr√† implementare un insieme di <strong>regole decisionali</strong> (business rules) trasparenti: ad esempio, se l&#39;utente indica che il sistema AI effettua valutazione del merito creditizio di clienti retail, il wizard dovr√† automaticamente segnalarlo come <em>AI Act Allegato III - high-risk</em> e predisporre gli step successivi (es. elenco requisiti da soddisfare, obbligo FRIA, ecc.)<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a>. Se il caso d&#39;uso comporta trattamento di categorie particolari di dati o profilazione estesa, il tool dovr√† suggerire obbligo di DPIA<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a>. <strong>(3)</strong> L&#39;output del wizard dovr√† includere un <em>cruscotto di evidenze e obblighi</em>: ad es. una checklist personalizzata con &quot;Documenti da predisporre&quot; (es. <em>Scheda descrittiva sistema AI</em>, <em>DPIA</em>, <em>FRIA</em>, <em>registro trattamento</em>, <em>contratto con fornitore</em>‚Ä¶), &quot;Requisiti applicabili&quot; (es. <em>art. 10 AI Act - data governance, art. 14 - oversight umano</em>‚Ä¶), &quot;Azioni consigliate&quot; (es. <em>valutare bias su dataset</em>, <em>previsto intervento umano prima decisione definitiva</em>, <em>informativa agli interessati da aggiornare</em>). <strong>(4)</strong> L&#39;MVP dovr√† integrare <strong>disclaimer e note esplicative</strong> in ogni sezione critica, per gestire le ambiguit√† normative: ad esempio una nota che chiarisca &quot;<em>Se il vostro caso non rientra esattamente nelle categorie AI Act ma presenta rischi potenziali, si consiglia l&#39;approccio pi√π prudente - vedere sezione &#39;Ambiguit√†&#39;</em>&quot;. Oppure, in caso di dubbio sulla necessit√† di DPIA: &quot;<em>In base alle informazioni fornite, non sussiste obbligo esplicito di DPIA ai sensi art.35 GDPR, ma si raccomanda comunque una valutazione documentata dato l&#39;uso esteso di dati personali (opzione conservativa)</em>&quot;. <strong>(5)</strong> Fondamentale sar√† la <strong>tracciabilit√† e giustificazione</strong> delle raccomandazioni: il deliverable &quot;Evidence table&quot; fornir√† il razionale (fonte normativa) dietro ogni regola del wizard, aumentando la confidenza dell&#39;utente nelle indicazioni fornite. L&#39;MVP quindi non solo guider√† step-by-step (wizard) ma funger√† anche da <em>knowledge base</em> consultabile, con sezioni &quot;Perch√© ti chiediamo questo?&quot; o &quot;Perch√© √® richiesto questo documento?&quot; che attingono alle fonti autorevoli (Garante, EBA, normativa) raccolte nella ricerca<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[11]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a>.</p>
<h1 id="evidence-table">Evidence Table</h1>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th><strong>Tema</strong></th>
<th><strong>Evidenza</strong></th>
<th><strong>Fonte</strong></th>
<th><strong>Implicazione per wizard</strong></th>
<th><strong>Confidenza</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Use case bancari ad alto rischio (AI Act)</strong></td>
<td>L&#39;AI Act classifica come <em>alto rischio</em> i sistemi IA usati per valutare affidabilit√† creditizia di persone (credit scoring) e per impieghi in ambito occupazionale (es. selezione del personale). I sistemi di questo tipo possono infatti incidere significativamente su diritti e tenore di vita degli individui, rischiando di perpetuare discriminazioni<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati">[2]</a>. Sono esclusi invece dall&#39;Allegato III i sistemi IA per rilevare frodi finanziarie o calcolo requisiti patrimoniali (non considerati ad alto rischio)<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi">[3]</a>.</td>
<td><em>Reg. UE 2024/1689 (AI Act)</em>, consid. 58 e 59<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi">[3]</a>; <em>Paradigma, 2025</em><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano">[24]</a>.</td>
<td>Identificare subito se un use case rientra in categorie di Allegato III (es. credito, HR) - in tal caso marcare come &quot;High-Risk AI Act&quot; e attivare i moduli di valutazione conformit√† (FRIA, requisiti art. 8-15 AI Act). Per ambiti esclusi (es. anti-frode) segnalare comunque obblighi settoriali ma con regime AI Act diverso.</td>
<td><strong>Alta</strong> (testo normativo chiaro; confermato da dottrina)</td>
</tr>
<tr>
<td><strong>DPIA - trigger nel settore bancario</strong></td>
<td>Il GDPR richiede la DPIA per trattamenti ad alto rischio; linee guida WP29/EDPB elencano criteri: tra essi profilazione o scoring su larga scala su situazione economica, decisioni automatizzate con effetti giuridici (es. concessione prestiti), monitoraggio sistematico, uso di dati sensibili o tecnologie nuove (come IA)<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[25]</a>. Il Garante ha specificato 12 tipologie obbligatorie, includendo: <em>&quot;trattamenti valutativi o di scoring su larga scala&quot;</em> e <em>&quot;decisioni automatizzate che incidono significativamente sull&#39;interessato (es. screening clienti di una banca per concessione finanziamento)&quot;</em><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[25]</a>.</td>
<td><em>Linee Guida WP29 n.248/2017</em> (EDPB)<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a>; <em>Garante Privacy, Provv. 467/2018</em><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[5]</a>.</td>
<td>Nel questionario wizard, domande per rilevare se il caso d&#39;uso coinvolge profilazione finanziaria, decisioni automatizzate su clienti, monitoraggio transazioni, uso di biometria, ecc. - in caso affermativo, far scattare alert &quot;DPIA obbligatoria&quot; e aggiungere il task &quot;Esegui DPIA&quot; nell&#39;output.</td>
<td><strong>Alta</strong> (linee guida ufficiali EDPB recepite dal Garante)</td>
</tr>
<tr>
<td><strong>FRIA - obbligo e contenuti</strong></td>
<td>L&#39;AI Act (art. 27) obbliga i <em>deployer</em> di sistemi IA ad alto rischio (p.a. e privati che forniscono servizi pubblici, nonch√© chi usa sistemi di Allegato III punti 5(b) e (c)) a effettuare una <strong>Valutazione d&#39;Impatto sui Diritti Fondamentali</strong> prima dell&#39;uso<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a>. La FRIA deve includere: descrizione del contesto d&#39;uso e scopo del sistema, durata e frequenza d&#39;uso, categorie di persone impattate, rischi specifici per diritti (tenendo conto info fornite dal provider ai sensi art.13 AI Act), misure di sorveglianza umana previste, e misure di mitigazione/gestione in caso di problemi (incl. meccanismi di reclamo)<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista">[7]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d">[8]</a>. L&#39;esito va notificato all&#39;autorit√† di vigilanza di mercato, usando il modello che sar√† predisposto (anche via tool automatizzato dall&#39;AI Office)<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica">[9]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato">[22]</a>. Se il deployer ha gi√† svolto una DPIA GDPR che copre parte degli aspetti, la FRIA pu√≤ integrare quella analisi senza duplicarla<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=4,d%27impatto%20sulla%20protezione%20dei%20dati">[10]</a>.</td>
<td><em>Reg. UE 2024/1689</em>, art. 27<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista">[7]</a>; Consiglio UE, Comunicato 9/12/23<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons">[26]</a>.</td>
<td>Il wizard deve spiegare chiaramente quando √® richiesta la FRIA (es. &quot;Use case classificato High-Risk ‚Üí obbligatoria FRIA prima della messa in esercizio&quot;). Dovr√† guidare l&#39;utente nell&#39;assemblare gli elementi per la FRIA: es. chiedere di descrivere finalit√† e contesto, elenco interessati impattati, ecc., e produrre uno schema di report. Inoltre, deve ricordare la necessit√† di <em>notifica all&#39;autorit√†</em> e fornire eventualmente un template di output conforme (es. modulistica standard AI Office). Deve anche segnalare che se √® stata fatta una DPIA, questa va aggiornata/integrata piuttosto che duplicata.</td>
<td><strong>Alta</strong> (disposizione di legge dettagliata)</td>
</tr>
<tr>
<td><strong>Human oversight - obbligo e modelli</strong></td>
<td>Le normative settoriali e l&#39;AI Act convergono sull&#39;obbligo di mantenere <strong>supervisione umana efficace</strong> sui sistemi IA ad alto rischio. Art. 14 AI Act impone che tali sistemi siano progettati per essere &quot;sorvegliabili&quot; da persone, e che le persone deputate al controllo abbiano competenze, formazione e autorit√† per intervenire, anche interrompendo il sistema se necessario<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la">[14]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario">[27]</a>. Le <strong>Linee Guida EBA</strong> sul credito prescrivono che l&#39;IA non operi in autonomia piena: l&#39;intermediario deve poter rivedere ed <em>eventualmente derogare</em> alle decisioni del modello (principio del <em>&quot;human-in-the-loop&quot;</em>)<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[12]</a>. Ad es., l&#39;art. 172(3) CRR gi√† richiede che nelle banche IRB vi sia possibilit√† di <em>override umano</em> dei risultati dei modelli interni di rating<a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=%E2%80%A2%20CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving">[28]</a>. Dalle analisi EBA emerge che le banche UE stanno adottando approcci graduati dove l&#39;<strong>intervento umano</strong> √® garantito specie nelle fasi iniziali di adozione di IA avanzata, per controllare i rischi<a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent">[16]</a><a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants">[29]</a>.</td>
<td><em>AI Act</em>, art. 14<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in">[30]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana">[31]</a>; <em>EBA Risk Report 2024</em><a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent">[16]</a><a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences">[17]</a>; <em>Paradigma 2025</em> (cit. EBA GL)<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[12]</a>.</td>
<td>Il wizard deve includere campi/domande per verificare la presenza di meccanismi di oversight: es. &quot;√à previsto un intervento umano prima che la decisione finale venga applicata al cliente?&quot;; &quot;Il personale addetto ha facolt√† di bloccare o correggere l&#39;output dell&#39;AI?&quot;. In base alle risposte, fornire alert se l&#39;oversight risulta inadeguata (trigger di non conformit√† art. 14). Inoltre, nelle <em>specifiche di output</em> per ogni use case, aggiungere raccomandazioni su modelli di controllo sostenibili (es. doppia verifica umana per decisioni critiche, training specifico per operatori AI, audit periodici dei risultati del modello).</td>
<td><strong>Alta</strong> (requisito legale + best practice EBA)</td>
</tr>
<tr>
<td><strong>Trasparenza verso individui</strong></td>
<td>L&#39;AI Act impone obblighi di trasparenza anche per sistemi non high-risk: ad esempio, chi utilizza un <strong>chatbot</strong> o sistema che interagisce con persone deve informare chiaramente l&#39;utente che si tratta di un sistema automatizzato<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a>. Inoltre, se un contenuto (testo, immagine) √® generato da IA, occorre dichiararlo all&#39;utente finale (per prevenire inganni). In ambito bancario, il GDPR art.13-14 e 22 gi√† richiedono di comunicare agli interessati l&#39;esistenza di decisioni automatizzate e fornire informazioni significative sulla logica usata e sulle conseguenze previste<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori">[19]</a>. La normativa nazionale (es. TUB art. 124-bis) ribadisce che se la concessione di credito avviene con strumenti automatizzati, il cliente va informato in modo chiaro e ha diritto a spiegazioni adeguate.</td>
<td><em>AI Act</em>, art. 52 (ora 50)<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a>; <em>GDPR</em>, art. 13-14, 22; <em>Paradigma 2025</em><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori">[19]</a>.</td>
<td>Il wizard deve chiedere se l&#39;AI interagisce con clienti o determina output rivolti a persone. In caso affermativo, indicare tra gli output obbligatori: &quot;Preparare un&#39;informativa specifica per gli utenti&quot;, &quot;Inserire disclaimer visibili nell&#39;interfaccia (es. &#39;Assistente virtuale automatizzato&#39;)&quot;. Inoltre, fornire linee guida su come redigere spiegazioni delle decisioni in linguaggio comprensibile. Il tool potrebbe includere un modulo di <strong>template di informativa</strong> da riempire con i dettagli del caso d&#39;uso (es. tipo di logica algoritmica, dati usati) in conformit√† a GDPR.</td>
<td><strong>Alta</strong> (norme chiare GDPR + AI Act)</td>
</tr>
<tr>
<td><strong>Non discriminazione e fairness</strong></td>
<td>Il principio di non discriminazione non √® dettagliatamente regolato nelle norme finanziarie, ma √® un focus centrale dell&#39;AI Act e delle autorit√†. La Banca d&#39;Italia nota che nelle disposizioni di trasparenza bancaria vi sono scarsi riferimenti espliciti alla <em>parit√† di trattamento</em>, e l&#39;uso di tecniche AI-ML sollecita nuove attenzioni su questo fronte<a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove">[32]</a>. L&#39;AI Act vieta sistemi di scoring sociale e categorizzazione in base a dati sensibili, e nei considerando evidenzia il rischio che modelli di credito o di recruiting possano <em>perpetuare bias storici</em> (ad es. sfavorire donne, minoranze)<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al">[33]</a>. Casi reali confermano il pericolo: algoritmi di HR troppo opachi o di credito basati su dati correlati a etnia/zona possono produrre disparit√†. Il Garante privacy italiano ha richiamato come la profilazione creditizia debba evitare di trattare dati sensibili o proxy di quelli (es. cap di residenza) senza adeguate cautele<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi">[34]</a>.</td>
<td><em>Banca d&#39;Italia QEF 721/2022</em><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove">[32]</a>; <em>AI Act</em> consid. 58<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al">[33]</a>; <em>Paradigma 2025</em><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano">[35]</a>.</td>
<td>Il wizard deve includere checkpoint dedicati alla fairness: es. chiedere se il dataset √® stato controllato per bias (squilibri rappresentativi), se il modello utilizza attributi potenzialmente discriminanti (diretti o indiretti). In output, per casi d&#39;uso sensibili (credito, HR), raccomandare di effettuare un <em>&quot;bias audit&quot;</em> e documentare gli esiti nella FRIA. Inoltre, segnalare l&#39;opzione conservativa di escludere dal modello variabili non pertinenti o potenzialmente proxy di categorie protette. Fornire riferimenti a linee guida (es. Appendice tecnica su metriche di fairness) nella bibliografia del wizard per approfondimento.</td>
<td><strong>Media</strong> (principio generale chiaro, ma mancano metriche univoche)</td>
</tr>
<tr>
<td><strong>Overlap normativo e approcci conservativi</strong></td>
<td>L&#39;EBA ha riscontrato che molte esigenze dell&#39;AI Act (es. data governance, robustezza, oversight) sono in parte coperte dalle norme finanziarie esistenti, sebbene non vi siano <em>deroghe esplicite</em> nell&#39;AI Act per il settore bancario<a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of">[36]</a>. Ci√≤ significa che banche e intermediari dovranno rispettare entrambe le cornici: ad es., un modello di credito IRB deve seguire le regole CRR/EBA <em>e</em> soddisfare i requisiti AI Act (documentazione tecnica, testing, ecc.). Questo doppio binario pu√≤ creare oneri, ma anche opportunit√† di integrazione. Ad esempio, i controlli periodici sui modelli richiesti da Banca d&#39;Italia/EBA (validazioni, backtesting) possono valere anche come misure di monitoraggio continuo ai sensi AI Act. Nel dubbio interpretativo su categorie borderline, le banche stanno adottando un approccio prudenziale, spesso applicando volontariamente le misure pi√π rigorose. Una prassi raccomandata √® considerare la <strong>FRIA e DPIA combinate</strong> come parte di un unico processo di <em>AI risk assessment</em>, coinvolgendo funzioni diverse (Compliance, DPO, Risk Management) per coprire tutti gli aspetti.</td>
<td><em>EBA Chair Letter 2025</em><a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of">[36]</a>; <em>Banca d&#39;Italia QEF</em><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono">[37]</a><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in">[13]</a>.</td>
<td>Il wizard dovrebbe fornire all&#39;utente un quadro di <em>&quot;intersezione normativa&quot;</em> - ad esempio una sezione riassuntiva che dica: &quot;Il tuo caso richiede: conformit√† AI Act (requisiti X, Y, Z) <strong>e anche</strong> rispetto delle regole bancarie ABC (es. Linee guida EBA x)&quot;. Suggerire un approccio integrato: output potrebbe consigliare di unificare DPIA+FRIA in un unico documento/procedura aziendale. Inoltre, nelle spiegazioni, il tool evidenzier√† dove gli obblighi coincidono (es. qualit√† dati √® sia requisito AI Act art.10 che buona pratica modelli interni) per evitare duplicazioni. In caso di incertezza (use case non esplicitamente normato), il wizard adotter√† il flag &quot;approccio conservativo suggerito&quot; e includer√† misure extra precauzionali.</td>
<td><strong>Alta</strong> (ricognizione EBA autorevole; convergenza con prassi Banca d&#39;Italia)</td>
</tr>
<tr>
<td><strong>Esempi e precedenti</strong></td>
<td>I sandbox regolatori italiani hanno gi√† sperimentato soluzioni AI bancarie innovative: es. progetto &quot;Kalaway&quot; per piattaforma di credit risk scoring su PMI e &quot;O-KYC&quot; per adeguata verifica clienti con DLT<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi">[38]</a><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi">[39]</a>. Dai report finali emerge che tali soluzioni sono considerate implementabili fuori dal sandbox <em>a condizione del rispetto di tutte le normative applicabili</em><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione">[40]</a><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=ICCREA%20Banca%20e%20Banca%20Monte,una%20futura%20commercializzazione%20della%20soluzione">[41]</a>. Inoltre, interventi del Garante Privacy (come il caso del chatbot generativo sanzionato nel 2023) mostrano che l&#39;utilizzo di IA deve fondarsi su basi giuridiche chiare e trasparenza: nel provvedimento n.755/2024 il Garante ha multato un provider di IA generativa per aver addestrato il modello su dati personali senza base valida e senza informare adeguatamente gli interessati<a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata">[42]</a><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello">[43]</a>. Questi esempi indicano sia la fattibilit√† delle nuove tecnologie in banca, sia le <em>trappole da evitare</em> (es. data breach non notificati, informativa lacunosa).</td>
<td><em>Banca d&#39;Italia Sandbox - report</em><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione">[40]</a>; <em>Garante Privacy, Provv. 755/2024</em><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata">[42]</a><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello">[43]</a>.</td>
<td>Nel wizard si possono integrare <em>case study</em> sintetici: ad esempio, una scheda &quot;Lezioni dal Sandbox&quot; che ricordi all&#39;utente di verificare la base giuridica per training AI (consenso/contratto per usare dati di clienti?) e di predisporre notifiche in caso di data breach AI. L&#39;evidence builder potr√† anche includere come &quot;evidence&quot; positiva il fatto che Banca d&#39;Italia ha validato in sandbox il modello X per credit scoring, indicando come implicazione che use case analoghi sono ammessi se conformi alle linee guida EBA sulla concessione credito<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=intermediari%20finanziari%20per%20offrire%20loro,durata%20massima%20di%2018%20mesi">[44]</a>. Ci√≤ aumenta la confidenza per l&#39;utente nel perseguire certi progetti, ma accompagnandola con avvertenze (compliance by design).</td>
<td><strong>Media</strong> (evidenze empiriche utili, ma riferimenti indiretti)</td>
</tr>
</tbody></table>
</div></figure><h1 id="specifiche-mvp-ai-compliance-navigator">Specifiche MVP &quot;AI Compliance Navigator&quot;</h1>
<p><strong>Campi di input minimi (max 20):</strong> Per alimentare il processo di triage, il tool richieder√† all&#39;utente informazioni chiave sul caso d&#39;uso AI in esame. I campi includeranno:<br>- <strong>Descrizione del caso d&#39;uso</strong> (libero): breve testo sullo scenario (es. &quot;Chatbot per assistenza clienti che suggerisce prodotti finanziari&quot;);<br>- <strong>Dominio applicativo primario</strong>: menu a scelta (Credito, Assistenza Clienti, AML/Frode, Risorse Umane, KYC/Onboarding, Altro) - ai fini di mappatura vs. Allegato III;<br>- <strong>Finalit√† e funzione AI</strong>: es. <em>scoring/valutazione</em>, <em>decisione automatizzata</em>, <em>supporto decisionale</em>, <em>generazione contenuti</em>, <em>monitoraggio/anomalia</em>, ecc.;<br>- <strong>Impatto su individui</strong>: tipo di decisione influenzata (es. <em>concessione o diniego di servizio</em>, <em>classificazione rischio cliente</em>, <em>valutazione candidato</em>, <em>nessun impatto diretto</em>);<br>- <strong>Coinvolgimento di dati personali?</strong> (s√¨/no) - discriminante per DPIA;<br>- <strong>Tipologia di dati trattati</strong>: checklist (dati finanziari, identificativi, demografici, biometrici, sensibili ex art.9 GDPR, anonimi, ecc.);<br>- <strong>Scala del trattamento</strong>: numero indicativo di soggetti interessati (es. &lt;1000, migliaia, milioni) e provenienza geografica (UE/non-UE) - per valutare &quot;larga scala&quot; e trasferimenti;<br>- <strong>Logica del modello</strong>: scelta multipla (Algoritmi deterministici/regole fisse; Machine Learning tradizionale; Deep Learning/NN; Generative AI/foundation model) - per valutare spiegabilit√† e requisiti tecnici;<br>- <strong>Fonte e qualit√† dei dati</strong>: origine dei dati di training/input (dati interni banca, dati da terzi, open data, web scraping) e presenza di possibili bias noti (campo note);<br>- <strong>Ruolo dell&#39;azienda</strong>: selezione (Sviluppatore del modello in-house; Utilizzatore di modello fornito da terzi; Entrambi) - per determinare obblighi provider vs. deployer;<br>- <strong>Coinvolgimento di fornitori terzi</strong>: nome/descrizione eventuale vendor o modello pre-addestrato (es. uso API esterna GPT) - per predisporre clausole contrattuali e richieste conformit√†;<br>- <strong>Presenza di decisione automatizzata</strong>: (s√¨/no) - se l&#39;output dell&#39;IA viene applicato senza intervento umano diretto (es. rifiuto automatico di richiesta) - trigger per art.22 GDPR e oversight;<br>- <strong>Misure di oversight previste</strong>: menu o checkbox (Revisione umana di tutti gli output critici; Intervento umano su richiesta; Nessun intervento umano; Altro) - per valutare compliance art.14 AI Act;<br>- <strong>Trasparenza verso gli interessati</strong>: checkbox (Informativa dedicata predisposta; Non ancora; Non applicabile - es. tool solo interno);<br>- <strong>Rischi noti/autovalutati</strong>: campo libero o selezione multipla (Possibili bias discriminatori; Rischio errori/false segnalazioni; Impatto su privacy; Altro) - per innescare consigli mirati e predisporre FRIA;<br>- <strong>Ambito regolamentare specifico</strong>: selezione se applicabile (es. <em>Credito al consumo</em>, <em>Credito immobiliare</em>, <em>Investimenti/MiFID profiling</em>, <em>Pagamenti PSD2</em>, <em>HR - Equality</em>, <em>AML</em>‚Ä¶) - per collegare a normative settoriali aggiuntive;<br>- <strong>Esistenza di DPIA precedente</strong>: (s√¨/no) e se s√¨, breve riferimento - per integrare con FRIA;<br>- <strong>Stadio del progetto</strong>: (Idea; Sviluppo; Pilota interno; Produzione attiva) - per orientare le raccomandazioni (es. se √® ancora in sviluppo, considerare sandbox/testing controllato).</p>
<p><em>(NB: Il totale campi √® ottimizzato e alcune voci possono essere combinate nell&#39;interfaccia per restare entro ~20 input effettivi, ad es. un&#39;unica sezione &quot;Tipi di dati &amp; categorie interessati&quot; che copra sia dati sensibili che utenti vulnerabili.)</em></p>
<p><strong>Regole/Trigger logici principali:</strong> Sulla base degli input sopra, l&#39;MVP applicher√† regole &quot;if-then&quot; per determinare obblighi e output:<br>- <strong>Classificazione High-Risk AI Act:</strong> se <em>Dominio</em> = Credito <strong>oppure</strong> se <em>Finalit√†</em> = valutazione affidabilit√† creditizia <strong>‚Üí</strong> flag &quot;Possibile AI Act Allegato III (5)(b) - sistema ad alto rischio (credit scoring)&quot;<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a>. Se <em>Dominio</em> = HR/Recruiting <strong>‚Üí</strong> flag &quot;Alto rischio (assunzione/lavoro)&quot;. Questi flag attiveranno: obbligo FRIA, indicazione di requisiti AI Act (conformit√†, registrazione, etc.), se del caso notifica database UE<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=usati%20nel%20settore%20elencati%20nell%27allegato%C2%A0III%2C,una%20valutazione%20dell%27impatto%20sui%20diritti">[45]</a>.<br>- <strong>DPIA obbligatoria:</strong> se <em>Coinvolgimento dati personali</em> = s√¨ <strong>e</strong> (Profilazione automatizzata su larga scala <strong>oppure</strong> Decisione senza intervento umano con effetti giuridici <strong>oppure</strong> Uso dati sensibili/biometrici <strong>oppure</strong> Monitoraggio sistematico comportamento clienti)<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[25]</a> <strong>‚Üí</strong> suggerire &quot;DPIA necessaria ai sensi art.35 GDPR&quot; con elenco motivazioni (evidenziando i criteri specifici soddisfatti).<br>- <strong>Richiesta consenso art.22 GDPR:</strong> se <em>Decisione automatizzata</em> = s√¨ <strong>e</strong> impatta interessato significativamente (esito credito, assunzione) <strong>‚Üí</strong> avviso: &quot;Verificare base giuridica per decisione automatizzata: consenso esplicito dell&#39;interessato <em>oppure</em> deroga di legge&quot; (es. necessario per esecuzione contratto).<br>- <strong>Trasparenza e diritto di spiegazione:</strong> se <em>Decisione automatizzata</em> = s√¨ <strong>‚Üí</strong> output da includere: &quot;Informare l&#39;interessato della logica del sistema (GDPR 13-15) e predisporre canale per richiesta intervento umano&quot;<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori">[19]</a>.<br>- <strong>Oversight insufficiente:</strong> se <em>Misure oversight</em> = &quot;Nessun intervento umano&quot; <strong>oppure</strong> <em>Finalit√†</em> = decisione automatica <em>e</em> oversight = limitato <strong>‚Üí</strong> flag di <em>non conformit√† potenziale</em>: raccomandare inserimento di controllo umano (es. &quot;Si raccomanda di prevedere revisione umana: attualmente non conforme a art.14 AI Act&quot;)<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in">[30]</a>.<br>- <strong>Bias/Fairness:</strong> se <em>Dominio</em> ‚àà {Credito, HR, AML} <strong>oppure</strong> utente ha selezionato rischio &quot;bias discriminatori&quot; <strong>‚Üí</strong> includere nel report sezione &quot;Valutazione Fairness&quot;: suggerire audit algoritmico, rimozione variabili proxy, ecc., richiamando principio non discriminazione<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al">[33]</a>.<br>- <strong>GenAI/Foundation model:</strong> se <em>Logica modello</em> = Generative <strong>‚Üí</strong> raccomandare misure extra: es. validazione output, filtro dei prompt, disclosure contenuti generati (obbligo trasparenza)<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a>, controllo di propriet√† intellettuale sui dati di training, ecc.<br>- <strong>Vendor management:</strong> se <em>Ruolo</em> = utilizzatore terzo <strong>‚Üí</strong> far comparire checklist: &quot;Richiedere dal fornitore: scheda sicurezza AI, documentazione tecnica (Annex IV AI Act), clausole su aggiornamenti, diritto audit, etc.&quot;.<br>- <strong>Sandbox/regulatory testing:</strong> se <em>Stadio progetto</em> = Pilota/Ideazione <strong>‚Üí</strong> suggerire valutazione ingresso in sandbox (citando iniziative come quella di Banca d&#39;Italia) come opzione.<br>- <strong>Non applicabilit√† AI Act:</strong> se <em>Dominio</em> = &quot;Altro&quot; <em>e</em> nessun rischio rilevante <strong>‚Üí</strong> output minimale: es. &quot;Il caso d&#39;uso non sembra ricadere in obblighi specifici AI Act oltre ai requisiti generali di trasparenza - si applicano comunque norme GDPR se dati personali&quot;.</p>
<p><strong>Output generato dall&#39;MVP:</strong> L&#39;output si comporr√† di diversi elementi strutturati, pensati come deliverable A-G richiesti:<br>- <strong>Executive Summary personalizzato:</strong> un riassunto in 8-10 punti delle finding per lo specifico use case inserito. Es: <em>&quot;Il sistema proposto rientra tra quelli AI Act ad alto rischio (Credito): dovrai ottenere marcatura CE e condurre una FRIA prima dell&#39;uso</em><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti"><em>[6]</em></a><em>. Presenta profilazione di dati finanziari su larga scala: √® necessaria una DPIA GDPR</em><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza"><em>[4]</em></a><em>. Sono emerse possibili ambiguit√† normative (es. decisione automatica su base profilazione creditizia): si consiglia approccio conservativo con intervento umano finale e informativa rafforzata al cliente‚Ä¶&quot;</em>. Questo summary includer√† anche 3-5 implicazioni chiave per design MVP (ad es. <em>&quot;integrare modulo bias testing&quot;</em>, <em>&quot;prevedere disclaimer verso l&#39;utente finale&quot;</em>).<br>- <strong>Evidence Table personalizzata:</strong> una tabella simile a quella generale ma focalizzata sul caso specifico, con righe su temi rilevanti. Ad esempio, per un caso di credit scoring compariranno righe su &quot;Credito = High-risk (fonte: AI Act)&quot;, &quot;Profilazione = DPIA (fonte: GDPR)&quot;, &quot;Bias rischio (fonte: considerazioni AI Act/EBA)&quot;, ciascuna con implicazioni per il progetto. Questa tabella attinge al database di evidenze generali ma filtra quelle applicabili.<br>- <strong>Checklist Requisiti &amp; Azioni:</strong> un elenco puntato di attivit√† da svolgere per conformit√†: es. <em>Redigere Documento Tecnico AI Act (Annex IV)</em>, <em>Effettuare test di accuratezza e robustezza</em> (art.15), <em>Iscrivere il sistema nel registro UE High-Risk</em> (art. 60) se applicabile, <em>Condurre training a personale addetto (oversight)</em>, <em>Aggiornare privacy policy per trasparenza</em>, ecc. Ogni voce verr√† marcata come &quot;Obbligatorio&quot; o &quot;Raccomandato&quot; a seconda della fonte (hard law vs. best practice).<br>- <strong>Specifiche MVP funzionali:</strong> se pertinente, l&#39;output includer√† raccomandazioni di design per l&#39;<strong>implementazione tecnica</strong> dell&#39;AI stesso in ottica compliance (ad es.: <em>&quot;Loggare tutte le decisioni del modello per audit (art.12 record-keeping AI Act)&quot;</em>, <em>&quot;Implementare alert se input fuori distribuzione (monitoraggio drift)&quot;</em>, <em>&quot;Interfaccia utente: prevedere campo spiegazione decisione per il cliente&quot;</em>). Queste specifiche aiutano IT e Data Science a costruire il sistema &quot;compliant by design&quot;.<br>- <strong>Template &quot;Use Case Sheet&quot;:</strong> il tool generer√† un documento sintetico (1-2 pagine) per ciascuno dei 5 casi d&#39;uso target (se rilevanti) - ad esempio, se l&#39;utente ha classificato il caso come &quot;Credito&quot;, verr√† allegata la scheda pre-compilata per <em>AI in Credit Scoring</em> con sezioni: Descrizione, Rischi specifici, Requisiti regolamentari, Soluzioni di oversight, Esempi pratici. Questo corrisponde al deliverable use case sheet richiesto, adattato con i dati forniti.<br>- <strong>Domande residuali e to-do:</strong> una lista di max 5-10 domande aperte o verifiche da effettuare prima della go-live, tarate sul caso. Es.: <em>&quot;Verificare con l&#39;ufficio legale se la base contrattuale articolo 6(1)(b) GDPR √® applicabile per questo trattamento automatizzato&quot;</em>; <em>&quot;Consultare DPO su necessit√† consultazione preventiva Garante in caso di rischi residui DPIA&quot;</em>; <em>&quot;Approfondire metriche fairness adatte (es. disparate impact) e ripetere test bias con dataset ampliato&quot;</em>.<br>- <strong>Bibliografia commentata:</strong> elenco delle fonti normative e linee guida pertinenti citate, con link e breve descrizione, cos√¨ l&#39;utente pu√≤ approfondire (ad esempio: <em>&quot;Regolamento UE 2024/1689 (AI Act) - Articoli 6, 14, 27: definisce high-risk e obblighi di valutazione impatti&quot;</em>, <em>&quot;Provvedimento Garante 467/2018 - Elenco trattamenti richiedenti DPIA in Italia (include scoring creditizio)&quot;</em>, <em>&quot;EBA Guidelines on Loan Origination 2020 - par. 74-96: uso di modelli automatizzati nel credito con requisiti di trasparenza e controllo umano&quot;</em>, etc.).</p>
<p><strong>Disclaimers (MVP):</strong> Il tool mostrer√† avvertenze chiare per gestire le aspettative e limitare le responsabilit√†. All&#39;avvio, un <strong>disclaimer generale</strong> chiarir√† che: <em>&quot;Questo strumento fornisce un supporto alla conformit√† basato su fonti normative pubbliche, ma non costituisce consulenza legale n√© assicura automaticamente la conformit√† al 100%. L&#39;utente √® responsabile delle decisioni finali e dell&#39;implementazione delle misure proposte&quot;</em>. Inoltre, ogni sezione delicata avr√† <strong>note esplicative</strong>. Esempio: alla sezione output, un disclaimer indicher√† che <em>&quot;Le raccomandazioni fornite (es. necessit√† di DPIA) derivano dalle informazioni inserite; assicurarsi di averle fornite accuratamente. In caso di dubbio, consultare il DPO o l&#39;Autorit√† competente.&quot;</em><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori">[19]</a>. Per i casi borderline, il wizard potrebbe includere un disclaimer tipo: <em>&quot;Approccio conservativo: dato il quadro normativo non definitivo su questo punto, suggeriamo di adottare tutte le misure come se fossero obbligatorie, in ottica di prudenza.&quot;</em> In calce al report finale, un ultimo disclaimer ricorder√† la data e versione delle fonti (es. <em>&quot;Normativa aggiornata a dicembre 2025&quot;</em>), avvisando che future modifiche regolamentari (es. linee guida in arrivo dalla Commissione o EBA) potrebbero richiedere revisione delle indicazioni.</p>
<p><strong>Out-of-scope (limiti dell&#39;MVP):</strong> Saranno esplicitati anche gli ambiti non coperti dal tool. Ad esempio: <em>&quot;Non tratta scenari di AI esclusivamente militare o di sicurezza nazionale (esclusi dall&#39;AI Act)&quot;, &quot;Non valuta aspetti di propriet√† intellettuale o brevetti sull&#39;algoritmo&quot;, &quot;Non copre la compliance di dettaglio su normative settoriali non direttamente collegate all&#39;uso di AI (es. requisiti di trasparenza precontrattuale MiFID se l&#39;AI √® usato in consulenza investimenti, che andranno valutati a parte)&quot;.</em> Inoltre verr√† dichiarato che il tool <em>non sostituisce test tecnici di sicurezza o validazione matematica</em> del modello: esso suggerisce di farli ma non li esegue. Infine, sar√† fuori scope qualsiasi giudizio di merito sull&#39;opportunit√† commerciale o etica di un progetto AI: l&#39;MVP si limita a mappare obblighi e gap, lasciando alle funzioni aziendali la decisione finale se procedere o meno.</p>
<h1 id="schede-use-case-5-casi-target">Schede Use Case (5 casi target)</h1>
<h2 id="use-case-1-valutazione-del-credito-credit-scoring"><strong>Use Case 1: Valutazione del Credito (Credit scoring)</strong></h2>
<p><strong>Descrizione:</strong> Impiego di algoritmi di AI per valutare la solvibilit√† dei clienti e supportare/determinare decisioni di concessione di credito (es. rating automatico di richieste di prestito al consumo o fidi PMI). Pu√≤ includere modelli di machine learning addestrati su dati storici di credito per prevedere probabilit√† di default. Spesso sostituisce o integra sistemi di credit scoring tradizionali (es. CRIF score) con modelli pi√π complessi (random forest, reti neurali).</p>
<p><strong>Rilevanza regolamentare:</strong> <em>Alto rischio AI Act</em>. Il credito figura esplicitamente nell&#39;Allegato III: <em>&quot;sistemi di IA utilizzati per valutare il merito creditizio delle persone fisiche&quot;</em> sono high-risk<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a>, comportando obbligo di conformit√† ai requisiti degli artt. 8-15 AI Act (gestione rischio, qualit√† dati, documentazione tecnica, trasparenza, oversight umano, accuratezza). Inoltre, la banca come <em>utilizzatore (deployer)</em> dovr√† effettuare la <strong>FRIA</strong> prima di mettere in uso il sistema<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a>, valutando impatti su diritti (in primis rischio di discriminazione finanziaria). Va previsto l&#39;inserimento del sistema nel registro UE dei sistemi ad alto rischio (tramite autorit√† di riferimento, salvo esenzioni). <em>Nota:</em> se la banca sviluppa internamente l&#39;algoritmo e lo impiega soltanto per s√©, essa cumula anche il ruolo di <em>fornitore</em> ai sensi AI Act, dovendo effettuare la valutazione di conformit√† (verosimilmente tramite autocertificazione con controllo interno per Annex III punto 5(b)) e rilasciare la Dichiarazione UE di Conformit√†<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,coinvolgimento%20di%20un%20organismo%20notificato">[46]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=match%20at%20L7793%20alto%20rischio,dati%20dell%27UE%20di%20cui%20all%27articolo%C2%A071">[47]</a>.</p>
<p><strong>Privacy &amp; DPIA:</strong> Quasi certamente richiesta. Il credit scoring tratta dati personali (finanziari, comportamentali, socio-demografici) spesso su larga scala e incide significativamente sugli interessati (accettazione/rifiuto credito = effetto giuridico). Rientra quindi in almeno due criteri DPIA (profilazione + decisione automatizzata)<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[25]</a>. Il Garante italiano include esplicitamente lo <em>&quot;screening clienti con dati di centrale rischi per decidere finanziamento&quot;</em> come esempio che <em>richiede DPIA</em><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[5]</a>. Occorre quindi condurre una DPIA ex art.35 GDPR, valutando necessit√† e proporzionalit√† dei dati usati, rischi per diritti (es. errore, esclusione di soggetti meritevoli) e misure di sicurezza. Particolare attenzione a: eventuale uso di dati <em>sensibili</em> (diretti o indiretti) - es. se il modello inferisce reddito da CAP/residenza rischiando disparit√† geografiche (potenziale discriminazione indiretta su base etnica/socio-economica). GDPR art.22 √® rilevante: una decisione puramente automatizzata di rifiuto credito richiede base contrattuale adeguata o consenso. In UE di solito le banche inseriscono ancora un intervento umano finale per evitare l&#39;applicazione rigida di art.22, oppure giustificano la profilazione come necessaria all&#39;esecuzione di un contratto (erogazione del credito) - impostazione possibile ma non esente da dibattito<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi">[34]</a>. La <strong>trasparenza</strong> verso il cliente √® cruciale: vanno comunicati almeno gli elementi generali della logica di scoring (quali categorie di dati incidono: es. storico pagamenti, rapporto debito/reddito, ecc.) e garantito il diritto di ottenere spiegazioni e contestare la decisione. Spesso ci√≤ si traduce, operativamente, nell&#39;istituzione di un <em>&quot;ufficio reclami AI&quot;</em> o procedure interne per riesaminare manualmente le pratiche respinte su richiesta del cliente.</p>
<p><strong>Obblighi/controlli aggiuntivi:</strong> Settore strettamente regolato da norme di trasparenza bancaria (TUB, provv. Banca d&#39;It. Trasparenza) e dalle <em>EBA Guidelines on Loan Origination and Monitoring</em> (EBA/GL/2020/06). Queste ultime richiedono che nell&#39;utilizzo di modelli statistici/AI per concessione credito, la banca garantisca: qualit√† e pertinenza dei dati, <strong>assenza di bias illegittimi</strong> (divieto di discriminare per genere, etnia, etc.), <em>documentazione</em> delle metodologie, e controllo umano sulle decisioni finali<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[11]</a>. Inoltre impongono valutazioni del merito creditizio <em>affidabili e comprensibili</em>: l&#39;IA deve essere <em>&quot;explainable&quot;</em> agli addetti e in parte al cliente. La normativa consumer credit (direttiva 2008/48/CE e succ.) prevede l&#39;obbligo di spiegare al richiedente i motivi dell&#39;eventuale rifiuto, specialmente se basato su processi automatizzati o data base (es. credit bureau) - il che si allinea al GDPR. Dal punto di vista di <strong>modellistica prudenziale</strong>, se il credit scoring IA √® usato anche per calcolo accantonamenti o rating regolamentari (AIRB), deve rispettare CRR e linee guida EBA sui modelli interni: ci√≤ comporta requisiti di validazione indipendente, <em>stress test</em> periodici, evidenza che il modello non invecchi (monitoraggio del <em>drift</em> delle performance). Fortunatamente, come evidenziato dall&#39;EBA, tali pratiche di model risk management collimano in parte con gli obblighi AI Act su robustezza e accuracy<a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving">[48]</a>. Un&#39;attenzione specifica: se il sistema utilizza dati provenienti da <strong>fonti esterne non tradizionali</strong> (es. social media, dati telco per credit scoring &quot;alternativo&quot;), occorre verificarne la liceit√† (consenso dell&#39;interessato o fonte pubblica) e la qualit√†/pertinenza; inoltre, l&#39;uso di dati non convenzionali potrebbe aumentare i rischi di discriminazione e va ponderato nella FRIA.</p>
<p><strong>Misure di oversight e monitoraggio:</strong> Praticare il <em>&quot;four eyes principle&quot;</em> sulle decisioni: se l&#39;algoritmo d√† esito &quot;rifiuta&quot;, idealmente un funzionario lo rivede (almeno a campione per verificare che il modello non penalizzi ingiustamente). Implementare soglie di allerta: es. se il modello scarta pi√π del X% di una certa categoria (es. residenti in una zona) valutarne le cause. Costituire un comitato interno AI per il credito, coinvolgendo compliance, risk e data science, che rivede periodicamente i risultati (tassi di default reali vs predetti, segnalazioni reclami ricevuti da clienti per ingiusto rifiuto, etc.) e approva eventuali revisioni del modello o delle policy. Prevedere <strong>formazione</strong> ai credit analyst sul funzionamento dell&#39;AI, affinch√© possano spiegare ai clienti e gestire i casi eccezionali. Sul monitoraggio post-deployment: loggare tutte le decisioni con gli score e fattori principali, per consentire audit a posteriori (richiesto da AI Act art.12 record-keeping). L&#39;AI Act richieder√† anche di mettere in atto misure per garantire <em>accuracy, robustness e cybersecurity</em> (art.15): nel credit scoring, ci√≤ significa testare il modello su dati storici e in simulazione per assicurare che le previsioni siano sufficientemente accurate e stabili, e che il modello non possa essere facilmente ingannato (ad esempio da dati falsi). In caso di aggiornamenti del modello (retraining), andr√† rifatta la validazione e aggiornata la documentazione tecnica, notificando se necessario sostanziali modifiche nel registro UE.</p>
<p><strong>Esempi pratici:</strong> Diversi istituti hanno sperimentato IA nel credit scoring. Un caso √® stato portato in Sandbox regolamentare italiano: la piattaforma di Kalaway S.r.l., in collaborazione con Banca Patavina, per <em>early warning</em> e valutazione automatica su imprese<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi">[38]</a><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=finalizzato%20a%20fornire%20alle%20banche,gestione%20del%20rischio%20di%20credito">[49]</a>. La sperimentazione ha mostrato benefici in efficacia e capacit√† di profilare il rischio in linea con linee guida EBA, ma ha evidenziato la necessit√† di rispettare tutte le normative applicabili prima della commercializzazione<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=La%20sperimentazione%20%E2%80%93%20nell%27ambito%20della,operare%20al%20di%20fuori%20dell%27ambiente">[50]</a>. Questo conferma che tali use case sono ammessi se ben governati. Sul fronte internazionale, si ricorda il caso <strong>Apple Card 2019</strong>, dove un algoritmo di credito fu accusato di discriminare sul genere (limiti di fido inferiori alle donne a parit√† di condizioni) - le autorit√† USA hanno indagato e, pur non avendo normative AI equivalenti al AI Act, il caso ha sollevato attenzione globale sui bias nei modelli creditizi. Ci√≤ evidenzia la necessit√† per le banche di testare ex ante l&#39;output del modello su diversi sub-gruppi demografici (gender, etnia se disponibile, et√†) per assicurare che non vi siano disparit√† ingiustificate (<em>fair lending test</em>). Nel contesto europeo, tali verifiche diventano parte integrante della FRIA e delle pratiche di <em>ethical AI</em> promosse anche dall&#39;EBA.</p>
<h2 id="use-case-2-contact-center-e-assistenti-virtuali-customer-service-ai"><strong>Use Case 2: Contact Center e Assistenti Virtuali (Customer service AI)</strong></h2>
<p><strong>Descrizione:</strong> Utilizzo di sistemi di intelligenza artificiale (es. chatbot testuali, voicebot IVR intelligenti) per gestire l&#39;assistenza clienti in banca. Questi sistemi possono rispondere a FAQ, fornire informazioni su saldo e movimenti, aiutare nell&#39;esecuzione di operazioni semplici (es. reset PIN), o anche proporre prodotti (finanziamenti, investimento) in modalit√† conversazionale. Possono basarsi su NLP (Natural Language Processing) e, pi√π recentemente, su modelli generativi tipo GPT addestrati su knowledge base bancarie. In alcuni contact center, l&#39;IA funge da primo livello, con possibilit√† di escalation a un operatore umano in caso di richieste complesse o insoddisfazione.</p>
<p><strong>Rilevanza regolamentare:</strong> <em>Non tipicamente alto rischio secondo AI Act</em>, a meno che il bot svolga funzioni riconducibili a categorie critiche (cosa rara per un assistente generico). Tuttavia, l&#39;AI Act impone <em>obblighi di trasparenza</em> specifici: un sistema destinato a interagire con persone deve dichiararsi come tale<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a>. Quindi, il cliente va informato chiaramente che sta parlando con un agente virtuale e non umano. Inoltre, se il contact center AI incorpora funzionalit√† di <strong>riconoscimento emotivo</strong> (analisi del tono di voce per inferire lo stato d&#39;animo del cliente) - cosa a volte proposta per routing chiamate in base a sentiment - l&#39;AI Act la classifica come pratica a rischio (in contesti come lavoro, l&#39;emotion recognition √® persino proibita)<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=For%20some%20uses%20of%20AI%2C,of%20predictive%20policing%20for%20individuals">[51]</a>. Nel contesto customer service non lavorativo non √® espressamente vietata, ma se utilizzata va gestita con estrema cautela e informando l&#39;utente che caratteristiche emotive sono analizzate<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a>. Dunque, il contesto impone compliance soprattutto su trasparenza e rispetto dei diritti dei consumatori.</p>
<p><strong>Privacy &amp; DPIA:</strong> Dipende dalle funzionalit√†. Se il chatbot tratta dati personali (quasi certo, visto che accede a dati conto cliente su richiesta, etc.) e potenzialmente effettua profilazione (ad es. interpretando richieste per offrire prodotti mirati), potrebbe attivare la necessit√† di DPIA. Di per s√©, un assistente virtuale Q&amp;A su base di dati forniti dal cliente rientra in <em>trattamento su larga scala di dati degli interessati e uso di tecnologia innovativa (IA)</em> - due criteri che combinati suggeriscono DPIA<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=7,la%20concessione%20di%20un%20finanziamento">[52]</a><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=particolari%20misure%20di%20carattere%20organizzativo,compresi%20i%20trattamenti%20che%20prevedono">[53]</a>. Se poi c&#39;√® anche monitoraggio di conversazioni o analisi sentiment (che √® <em>sorveglianza comunicazioni + dati potenzialmente sensibili</em>, come stress o emozioni), la DPIA √® fortemente raccomandata. Aspetti da valutare nella DPIA: sicurezza delle informazioni (i dialoghi contengono dati riservati bancari, vanno protetti con crittografia e controlli accesso); rischio di errori o <em>allucinazioni</em> del modello (un generative potrebbe fornire risposte errate finanziariamente dannose - es. info sbagliata su bonifici - con impatto su cliente); gestione del consenso per registrazione chiamata/chat (solitamente i call center informano della registrazione a fini qualit√† - se l&#39;IA elabora la voce, √® trattamento ulteriore da coprire). Importante: se il bot prende decisioni che influiscono su contratti (non comune: di solito esegue istruzioni del cliente, non decide al posto suo), allora subentra art.22. Nella generalit√†, il contact center AI assiste ma non &quot;decide&quot;, quindi art.22 GDPR non √® attivato; comunque, il cliente deve poter sempre richiedere di parlare con umano (questo potrebbe configurarsi quasi come un &quot;diritto analogo&quot; in termini di user experience, anche se non giuridicamente basato su art.22). La DPIA dovrebbe coprire anche il rispetto del <strong>segreto bancario</strong>: le conversazioni con l&#39;AI contengono dati confidenziali, l&#39;algoritmo e i suoi manutentori non devono violare la riservatezza (es. se si usa un servizio cloud esterno per NLP, √® trasferimento dati all&#39;estero? bisogna valutare).</p>
<p><strong>Obblighi/controlli aggiuntivi:</strong> Al di l√† di AI Act/GDPR, si applicano normative consumer: il <strong>Codice del Consumo</strong> richiede che pratiche commerciali siano corrette e non ingannevoli - il chatbot deve dare info veritiere, aggiornate e comprensibili. Se il bot propone contratti (es. un prestito) potrebbe configurarsi come comunicazione commerciale o offerta contrattuale: va quindi conforme a regole di trasparenza precontrattuale (Fogli informativi, ECC‚Ä¶). √à improbabile lasciare a un&#39;AI generativa la spiegazione di condizioni di un mutuo senza supervisione, ma se lo facesse, la banca ne resta responsabile. In Italia, normative come l&#39;art. 8-ter TUB (per l&#39;assistenza ai clienti bancari) implicano che i sistemi di risposta non umani non aggravino gli obblighi di risposta entro tempi definiti ai reclami: quindi se l&#39;AI gestisce parzialmente i reclami, deve rispettare procedure. Dal punto di vista ICT, le linee guida Banca d&#39;Italia in materia di sicurezza e continuit√† richiedono che i sistemi - compresi quelli di front-end innovativi - abbiano adeguati piani di continuit√†: un contact center AI deve avere fallback (es. se il bot non capisce o √® offline, deve subentrare un operatore umano o un messaggio di scuse con invito a canali alternativi). Se l&#39;AI viene &quot;addestrata&quot; con trascrizioni di conversazioni passate, bisogna valutare base giuridica per riutilizzare quei dati di clienti - tipicamente lo si fa anonimo/aggregato, altrimenti serve includere nell&#39;informativa privacy che i dati di contatto cliente potranno essere usati per migliorare i servizi AI.</p>
<p><strong>Misure di oversight e monitoraggio:</strong> Anche se questo caso d&#39;uso non √® high-risk, <em>human-in-the-loop</em> √® comunque importante per qualit√† di servizio. Best practice: predisporre che <strong>almeno il X% delle conversazioni</strong> (specie quelle su operazioni dispositive) siano revisionate a posteriori da personale quality control, per correggere eventuali risposte scorrette fornite dall&#39;AI e migliorare il sistema. Implementare metriche di monitoraggio continuo: tasso di richieste non comprese (fallback to human), tasso di feedback negativi/rating basso da clienti post-chat, tipologie di errori comuni. Queste metriche vanno riportate in dashboard al responsabile customer care e al <em>team AI governance</em>. Oversight in tempo reale: se l&#39;utente esprime frustrazione o inserisce parole chiave (es. &quot;voglio parlare con operatore&quot;), il sistema deve immediatamente trasferire la chat/call a un umano. Inoltre, addestrare specificamente il personale di secondo livello su come rilevare se il bot ha commesso errori informativi, in modo da rettificare subito le info date al cliente. Dal lato tecnico, per modelli generativi √® utile inserire <em>validazione</em> dell&#39;output: es. se il cliente chiede saldo conto, il bot deve usare l&#39;API interna e restituire il numero esatto, non &quot;inventare&quot;; per questo, architetture come Retrieval-Augmented Generation (RAG) possono essere adottate, garantendo che l&#39;AI fornisca solo risposte basate su fonti certe (base dati bancaria). L&#39;AI Act richiede trasparenza logging: si dovrebbero conservare le chat e registrazioni (previo info al cliente) per eventuali audit - questo gi√† avviene per i call center tradizionali (registrazioni a fini qualitativi).</p>
<p><strong>Esempi pratici:</strong> Molte banche hanno lanciato chatbot: es. <em>Intesa Sanpaolo</em> con il suo virtual assistant in app, <em>UniCredit</em> con il chatbot sul sito, etc. Un caso interessante √® <strong>Widiba</strong> (banca online italiana) che nel 2018 lanci√≤ un assistente virtuale vocale (&quot;Widdy&quot;) integrato con smart speaker, ma lo ritir√≤ dopo poco: si era rilevato che i clienti preferivano interfacciarsi via chat o app e c&#39;erano dubbi su privacy (dialoghi su device come Alexa). Questo insegna di testare l&#39;aderenza del canale AI alle preferenze utente e percezione di fiducia. Sul fronte normativo, la <strong>Garante Privacy spagnolo (AEPD)</strong> ha pubblicato linee guida sui chatbot indicando che devono informare chiaramente l&#39;utente e che l&#39;utente mantenga diritti GDPR pieni (incluso sapere se sta parlando con AI). Inoltre alcuni casi mediatici, es. il chatbot di <em>Bank of America</em> (&quot;Erica&quot;), hanno mostrato che la chiave del successo √® definire bene l&#39;ambito: Erica risponde a query finanziarie ma non d√† consigli d&#39;investimento personalizzati (per evitare rischi compliance MiFID). Quindi restringere il <em>scope</em> dell&#39;AI e mettere confini (il nostro contact center AI potrebbe essere configurato per non rispondere su temi sensibili o legali, indirizzando subito ad umano).</p>
<h2 id="use-case-3-anti-money-laundering-e-monitoraggio-transazioni"><strong>Use Case 3: Anti-Money Laundering e Monitoraggio Transazioni</strong></h2>
<p><strong>Descrizione:</strong> Utilizzo di sistemi IA (in particolare machine learning, anche deep learning) per analisi di transazioni finanziarie e individuazione di attivit√† sospette legate a riciclaggio di denaro o frodi. Tradizionalmente le banche usano regole fisse e scenari predefiniti (es. importi soglia, determinati pattern noti) nei sistemi AML. L&#39;IA avanzata promette di rilevare anche schemi anomali non predefiniti (&quot;<em>unknown unknowns</em>&quot;) tramite algoritmi di anomaly detection o classificazione addestrati su dati storici di segnalazioni. Ad esempio, un modello pu√≤ apprendere il profilo di operativit√† normale di un cliente e segnalare deviazioni atipiche. Oppure pu√≤ correlare dati di diverse fonti (movimenti, profili di rischio, info OSINT) per alzare alert mirati.</p>
<p><strong>Rilevanza regolamentare:</strong> <em>Non classificato come high-risk AI Act,</em> perch√© l&#39;Allegato III copre solo usi di IA in attivit√† di <em>law enforcement</em> pubblico. L&#39;AML sta in un&#39;area ibrida: la <em>Direttiva AML</em> impone agli enti obbligati (banche) controlli su transazioni e segnalazione di sospetti all&#39;UIF (Unit√† di Informazione Finanziaria), ma l&#39;AI Act non considera la banca come autorit√† di contrasto. Anzi, il cons. 59 specifica che le unit√† di informazione finanziaria con compiti amministrativi di analisi non sono incluse negli usi ad alto rischio di polizia<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo">[20]</a>. Quindi, l&#39;impiego di AI per AML rientra nei sistemi &quot;non high-risk&quot; (a meno che si usi in collaborazione diretta con forze dell&#39;ordine per indagini, scenario fuori standard). Ci√≤ significa niente obbligo di certificazione ex ante AI Act n√© FRIA obbligatoria <em>per legge</em>. Tuttavia, da un punto di vista di impatto sui diritti, un algoritmo AML pu√≤ congelare temporaneamente operazioni o segnalare clienti all&#39;autorit√†, con potenziali gravi effetti (blocco conto, indagini) - quindi <em>la banca prudentemente dovrebbe condurre una valutazione di impatto</em> (FRIA volontaria o simil-DPIA estesa) per assicurarsi che il sistema non violi diritti di individui inconsapevoli. Inoltre, l&#39;AI Act vieta espressamente l&#39;uso di IA per <em>profilazione massiva in contesto di polizia</em> e altre pratiche invasive: un sistema AML borderline (che profilasse tutti i clienti in chiave criminale) deve comunque rispettare principi di <em>necessit√† e proporzionalit√†</em>.</p>
<p><strong>Privacy &amp; DPIA:</strong> Molto probabile che sia necessaria una DPIA. Monitorare sistematicamente le transazioni di tutti i clienti alla ricerca di illeciti √® un esempio classico di <em>&quot;sorveglianza sistematica su larga scala&quot;</em><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=profilazione,del%20volume%20dei%20dati">[54]</a>, combinata con profiling comportamentale potenzialmente anche di dati sensibili (es. bonifici a organizzazioni religiose, spese mediche possono rivelare dati sensibili). Il Garante francese (CNIL) ha in passato indicato che i sistemi antiriciclaggio rientrano nelle liste di trattamenti da valutare. Il Garante italiano nelle sue 12 categorie menziona <em>&quot;trattamenti per prevenire frodi&quot;</em> e <em>&quot;interconnessioni di dati di consumo con dati di pagamento&quot;</em><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=8,personali%20raccolti%20per%20finalit%C3%A0%20diverse">[55]</a><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,ovvero%20della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di">[56]</a> - entrambi attinenti all&#39;AML - come casi da esaminare. Quindi DPIA s√¨, focalizzata su: liceit√† (banca ha obbligo legale di AML, quindi base giuridica √® adempimento di obbligo di legge - art.6(1)(c) GDPR, esente da consenso), minimizzazione (usare solo i dati necessari per pattern: per√≤ AML tende a <em>massimizzare</em> dati per scoprire correlazioni - trovare equilibrio), limitazione conservazione (dati di allerta non confermati vanno cancellati dopo un tempo, etc.). La DPIA deve considerare anche <em>falsi positivi</em>: un elevato numero di segnalazioni non giustificate pu√≤ ledere la reputazione e i diritti dei clienti (che possono subire disservizi). Andrebbe quindi predisposta una misura per mitigare ci√≤ (es. threshold ragionevoli per allarmi, verifica umana prima di segnalare all&#39;UIF). Dal punto di vista privacy, l&#39;AML sta in equilibrio con normative antiriciclaggio: GDPR consente trattamento senza consenso e eventuali limitazioni dei diritti (es. diritto informazione pu√≤ essere limitato per non allertare il sospetto, secondo art.23 GDPR attuato da D.Lgs 231/2007). Quindi al cliente non verr√† detto &quot;ti stiamo profilando per AML&quot;, e ci√≤ √® lecito per legge - ma internamente la banca deve monitorare attentamente gli accessi a questi dati e garantire che non si ecceda (logiche di <em>&quot;data protection by design&quot;</em> qui complesse: serve tracciare chi vede gli alert, etc.).</p>
<p><strong>Obblighi/controlli aggiuntivi:</strong> Fortissima regolamentazione settoriale: D.Lgs. 231/07 (recepimento AMLD) richiede che la banca applichi approccio basato su <strong>rischio</strong> (risk-based approach) nella sua funzione antiriciclaggio. L&#39;uso di modelli avanzati √® incoraggiato implicitamente, purch√© la banca ne capisca i risultati e li inserisca nella propria valutazione rischio. Le autorit√† di vigilanza (UIF, Banca d&#39;It) si aspettano che l&#39;utilizzo di strumenti informatici non sostituisca il giudizio del compliance officer AML. Ci sono obblighi di <em>conservazione</em> (tenere traccia di tutte le segnalazioni e analisi 10 anni) e di <em>riservatezza</em> (non rivelare al cliente segnalato). Un algoritmo AI che segnala in automatico all&#39;UIF andrebbe calibrato per minimizzare errori clamorosi - perch√© l&#39;UIF poi pu√≤ chiedere spiegazioni. Bisogna quindi poter spiegare (almeno internamente) perch√© il modello ha segnalato X -&gt; questo √® critico: molti modelli ML non sono facilmente spiegabili; la banca dovr√† magari avere un modulo di <em>explanation</em> per ogni alert (es. evidenziare la sequenza di transazioni anomala che ha portato al punteggio elevato). Dal punto di vista <em>auditing</em>, l&#39;Organo di controllo interno e i revisori ispezioneranno il processo AML: occorre documentare metodologia AI, validazione fatta (per assicurare che non &quot;buchi&quot; qualche tipologia di rischio noto), e risultati. Se il sistema IA sostituisce parametri delle <em>Disposizioni Banca d&#39;Italia</em> su adeguata verifica semplificata vs rafforzata, etc., bisogna verificare di restare entro cornice normativa: l&#39;AI pu√≤ aiutare a attribuire automaticamente classi di rischio cliente (low/medium/high risk) basandosi su dati transazionali oltre che tipologia cliente, ma la metodologia dovrebbe essere validata e approvata dall&#39;AML Officer e soggetta a update continuo.</p>
<p><strong>Misure di oversight e monitoraggio:</strong> Approccio raccomandato: <em>AI as assistant, not decider</em> in AML. Quindi: l&#39;algoritmo genera <em>segnalazioni interne</em> (&quot;alerts&quot;), ma una <strong>analista AML umano le rivede tutte</strong> e decide quali far confluire in Segnalazione di Operazione Sospetta ufficiale all&#39;UIF. Questo gi√† avviene con scenari statici (l&#39;analista filtra molti falsi positivi generati dalle regole); con IA potrebbe cambiare volume e natura degli alert, ma il principio di doppio controllo rimane fondamentale (ed √® richiesto dalla normativa AML di fatto). Inoltre, serve <em>monitoraggio dell&#39;efficacia</em> del modello: metriche come % di alert AI che diventano effettive SOS inviate (precision), % di SOS effettive originate dall&#39;AI vs da altri canali (recall), e confronti con anni precedenti - se l&#39;AI riduce drasticamente o aumenta troppo le SOS, va investigato. Oversight da parte del <em>Compliance Officer AML</em>: costui deve poter capire il sistema, avere possibilit√† di intervenire sui parametri (es. alzare soglia sensibilit√† se sta segnalando troppo, o aggiungere un controllo specifico se normativa cambia - se modello black box, prevedere meccanismi per integrarlo con regole aggiuntive). Implementare un processo periodico (annuale almeno) di revisione del modello con stakeholder: IT, Compliance, Data Scientist e anche rappresentanti legali per valutare se il modello sta rispettando requisiti e se emergono nuovi rischi (ad es. il modello potrebbe &quot;imparare&quot; a ignorare certe transazioni comuni ma potenzialmente lecite - rischio di <em>blind spots</em>).</p>
<p><strong>Esempi pratici:</strong> Molte banche stanno valutando IA in AML: es. <em>ING</em> ha dichiarato l&#39;uso di ML per rilevare transazioni sospette migliorando il segnale, <em>Swedbank</em> dopo uno scandalo di mancato rilevamento ha investito in AI. In Italia, la <strong>Banca d&#39;Italia</strong> ha avviato dal 2021 un progetto (<em>&quot;Gianos&quot;</em>) di machine learning applicato ai dati aggregati di segnalazioni antiriciclaggio per aiutare l&#39;UIF a identificare fenomeni nascosti - segno che anche le autorit√† vedono utile l&#39;AI, ma con grande cautela. Un case concreto: <em>HSBC</em> ha usato AI per incrociare dati di transazioni internazionali e social network nel famoso progetto <em>&quot;FXogle&quot;</em>, scoprendo pattern di frodi valutarie; tuttavia questo ha comportato questioni su privacy (utilizzo di dati personali non finanziari). Dal punto di vista normativo, un fatto interessante: nel Regno Unito il regolatore (FCA) ha organizzato <em>TechSprints</em> sull&#39;AML, evidenziando che i modelli black box sono problematici per auditors e preferendo modelli interpretabili (<em>white box AI</em>). Quindi, uno <em>shift</em> possibile √® privilegiare algoritmi pi√π spiegabili (es. alberi decisionali, reti bayesiane) rispetto a deep learning puro, almeno finch√© normative richiedono spiegazioni. Per la sandbox italiana, nella prima finestra <em>Vidyasoft</em> ha testato &quot;Hands-Free SCA&quot; per frodi pagamenti usando AI<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20un%20servizio%20evoluto,durata%20massima%20di%2018%20mesi">[57]</a> - ambito leggermente diverso (Strong Customer Authentication con esenzioni AI), ma affine: il report finale di sandbox (seconda finestra) non √® pubblico, ma la lezione generale √® che AI pu√≤ operare se integrata con controlli come quelli sopra.</p>
<h2 id="use-case-4-recruitment-e-gestione-hr-con-ai"><strong>Use Case 4: Recruitment e Gestione HR con AI</strong></h2>
<p><strong>Descrizione:</strong> Applicazione di sistemi IA nel processo di selezione, assunzione e gestione del personale in banca. Include strumenti che fanno <em>screening automatico dei CV</em> (ad esempio software che filtrano candidati in base a requisiti e punteggi, o che usano NLP per analizzare lettere motivazionali), sistemi di <em>video-interview analysis</em> (algoritmi che valutano espressioni facciali, tono e linguaggio del candidato in video-colloqui), fino a AI che propongono una shortlist di candidati o addirittura forniscono un punteggio di idoneit√†. Altre applicazioni in ambito HR: tool per valutazione delle performance dei dipendenti, analisi predittiva di churn (chi potrebbe lasciare l&#39;azienda), ottimizzazione turni. Qui ci concentriamo sull&#39;assunzione e mobilit√† interna, essendo menzionata come area target.</p>
<p><strong>Rilevanza regolamentare:</strong> <em>Alto rischio AI Act</em>. L&#39;Allegato III ¬ß4 include <em>&quot;sistemi di IA destinati a essere utilizzati per il reclutamento o la selezione delle persone fisiche, per decidere su assunzione, promozione, cessazione, allocazione di compiti, o valutazione in ambito lavorativo&quot;</em><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei">[58]</a>. Chiaramente quindi un ATS (Applicant Tracking System) con AI rientra. Conseguenze: requisiti AI Act pienamente applicabili (dati di training da controllare per qualit√† e bias, documentazione tecnica sul modello, misure di sicurezza etc.). Il fornitore del software ATS dovr√† curare la certificazione CE del sistema come conforme. La banca come utente dovr√† svolgere la FRIA prima di usare tali sistemi su candidati/lavoratori<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a>, dato che deployer di sistemi di Allegato III punto 4 sono presumibilmente compresi (in quanto datori di lavoro). Inoltre, l&#39;AI Act mette l&#39;accento sui rischi di discriminazione in ambito occupazionale: i considerando notano come tali sistemi possano replicare discriminazioni storiche (contro donne, fasce d&#39;et√†, etnie) e incidere sulle opportunit√† di vita<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=lavoratori,I%C2%A0sistemi%20di%20IA">[59]</a>. Quindi la FRIA dovr√† in particolare valutare fairness e impatto su diritti del lavoro (es. diritto alla pari opportunit√†, non-discriminazione in fase di selezione, dignit√†). Segnaliamo anche che alcune pratiche AI HR possono incrociarsi con divieti: <em>es.</em> l&#39;AI Act <strong>vieta</strong> l&#39;uso di sistemi di riconoscimento emotivo nel contesto lavorativo<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=will%20be%20banned%20from%20the,of%20predictive%20policing%20for%20individuals">[60]</a> - ci√≤ riguarda ad esempio software che analizzano le emozioni durante test o sul posto di lavoro (questo √® <em>banned</em>, quindi la banca non potrebbe lecitamente usare un sistema che monitora emozioni dei dipendenti per valutarli). Altra pratica proibita: <em>social scoring generalizzato</em> - non comune in HR, ma significa evitare sistemi che profilano la &quot;affidabilit√†&quot; del lavoratore su parametri non pertinenti lavorativi (es. vita privata).</p>
<p><strong>Privacy &amp; DPIA:</strong> Obbligatoria al 99%. Trattare dati dei candidati o dipendenti con sistemi automatizzati di valutazione rientra nel profiling e monitoraggio. Il GDPR, art.88, consente Stati membri di prevedere garanzie per trattamenti in ambito lavoro: il D.Lgs 101/2018 in Italia richiama l&#39;art. 113 del Codice Privacy, che affida al Garante definire misure. Il Garante italiano e l&#39;EDPB hanno chiarito che decisioni automatizzate in contesto HR sono particolarmente delicate: di norma serve il <em>consenso</em> o altra base e non devono violare statuto lavoratori (art. 4 legge 300/1970 su controlli). Una DPIA valuter√† quindi: base giuridica (es. consenso del candidato a screening automatico, oppure legittimo interesse? Consenso rischia di non essere &quot;libero&quot; in contesto lavoro; in genere si preferisce base legittimo interesse bilanciato con garanzie); necessit√† - la banca deve dimostrare che l&#39;AI migliora l&#39;obiettivit√† e efficienza senza ledere diritti; rischi di errori (scartare il candidato ideale erroneamente) e come rimediare (ex: far comunque visionare un campione di CV scartati a recruiter umani per controllo). Importante: se c&#39;√® <em>decisione automatizzata pura</em> su un candidato (ad es. &quot;il sistema esclude Tizio senza intervento umano&quot;), questo attiva art.22 GDPR - tipicamente, i datori di lavoro in EU evitano di avere decisione solo automatica, includendo un HR nella decisione finale di chi invitare a colloquio, proprio per conformit√†. Se per assurdo la banca volesse decisione fully automated (pochissimo probabile per rischi reputazionali e normativi), dovrebbe ottenere consenso esplicito dei candidati interessati - scenario complesso (il candidato pu√≤ sentirsi obbligato a darlo). Inoltre, andrebbe garantito il diritto di ottenere un intervento umano (un recruiting manager che riconsideri la candidatura su richiesta). Dal lato dati: CV e cover letter spesso contengono <em>dati personali delicati</em> (foto = dati biometrici se usato face recognition; testo pu√≤ rivelare origine etnica, et√†, stato familiare, che sono aspetti sensibili in HR per normative antidiscriminatorie; anche eventuali dati sanitari o penali se il candidato li menziona). Un&#39;AI nel leggerli deve ignorare info irrilevanti e la banca deve configurare il sistema per non tenere conto di attributi protetti. Questo pu√≤ implicare tecniche di <em>bias mitigation</em>, ad esempio far analizzare CV anonimi (rimuovendo nome, genere, et√†). La DPIA dev&#39;essere coordinata con le normative lavoro: lo Statuto dei Lavoratori art.4 vieta controlli a distanza non accordati su dipendenti - se l&#39;AI fosse usata per monitorare performance (ad es. analisi email, chat, produttivit√†), serve accordo sindacale o autorizzazione. Nel recruiting, ci√≤ non si applica ai candidati (non sono dipendenti ancora), ma nel mobility interna s√¨ (un dipendente che concorre ad altra posizione - i suoi dati usati da AI vanno tutelati come dati dipendente).</p>
<p><strong>Obblighi/controlli aggiuntivi:</strong> Norme antidiscriminatorie in assunzione: in UE e Italia √® vietata discriminazione diretta o indiretta per sesso, et√†, etnia, religione, disabilit√†, orientamento, ecc. Un sistema AI recruiting √® soggetto a tali leggi (D.Lgs 216/2003 in IT attuazione direttiva eguaglianza). Se venisse fuori che l&#39;algoritmo di recruiting esclude sistematicamente persone di una certa fascia d&#39;et√† o genere, la banca potrebbe essere citata per discriminazione, anche se il bias √® &quot;non intenzionale&quot; (il concetto di <em>disparate impact</em> si applicherebbe). Quindi come obbligo derivato c&#39;√®: testare e assicurare che i criteri di AI siano job-related e non escludano categorie protette in percentuale sproporzionata, a meno di giustificato motivo. Ad esempio, un algoritmo che penalizza gap lavorativi lunghi potrebbe svantaggiare le donne (maternit√†): va calibrato o giustificato come essenziale. Inoltre, la banca deve rispettare obblighi di trasparenza verso i candidati: il Decreto Trasparenza Lavoro (D.Lgs 104/2022) recependo la direttiva UE 2019/1152, <em>art. 4, comma 5-6</em>, impone al datore di informare il lavoratore (o candidato, se interpretabile estensivamente) sull&#39;uso di sistemi decisionali o di monitoraggio automatizzati in fase preassuntiva o nel rapporto. In particolare, se si usano strumenti algoritmici che incidono su assunzione, il candidato/dipendente ha diritto a essere informato del funzionamento di tali sistemi, dei parametri principali e degli obiettivi perseguiti, nonch√© delle logiche di valutazione. Questo √® importantissimo: dal agosto 2022 in Italia i datori devono consegnare un&#39;informativa scritta <em>anche per gli algoritmi in recruiting</em> e permettere al lavoratore di chiedere chiarimenti. L&#39;AI Act aggiunger√† registrazione nel database (se ente pubblico, quell&#39;uso di AI andr√† reso noto). Quindi c&#39;√® un intreccio di obblighi: la banca dovr√† predisporre una sorta di &quot;scheda trasparenza algoritmo di selezione&quot; da fornire se richiesto (un&#39;anticipazione di quell&#39;obbligo sta gi√† spingendo aziende a rivelare se usano AI in HR). Oltre a questo, normative privacy collegate: se il sistema fa analisi video (biometria facciale su colloqui video), serve o consenso specifico o base giuridica forte e autorizzazione Garante, perch√© dati biometrici e giudiziari (casellario) sono soggetti a regimi speciali.</p>
<p><strong>Misure di oversight e monitoraggio:</strong> Principio cardine: <em>decisione finale umana</em>. L&#39;AI pu√≤ rankare i CV, ma un recruiter deve poter modificare la graduatoria, includere candidati che l&#39;AI ha escluso se li ritiene validi (<em>override</em>), e documentare perch√©. Ogni tanto, il team HR dovrebbe fare controlli qualitativi: prendere un campione di CV scartati e vedere se c&#39;erano candidati potenzialmente validi scartati ingiustamente - se s√¨, analizzare il motivo (feature del modello che ha penalizzato quell&#39;aspetto). Ad esempio, scoprire che l&#39;AI penalizza chi vive a &gt;50km dalla sede (supponendo minore probabilit√† di accettare il lavoro) - √® lecito? Potrebbe essere discriminatorio verso chi abita in Sud Italia candidandosi a Nord (indiretto su origine). Serve allora rimuovere o attenuare quel criterio. Quindi, predisporre un processo di <em>bias audit periodico</em>, magari coinvolgendo anche il Diversity Manager aziendale. In fase di implementazione, assicurarsi che il modello sia addestrato su dati &quot;puliti&quot;: se i dati storici riflettono un bias (es. in passato l&#39;azienda assumeva pochissime donne in IT), il modello lo replicher√†. Potrebbe essere utile applicare tecniche di re-balancing (dare peso maggiore ai casi minoritari). L&#39;oversight pu√≤ anche essere esterno: informare i sindacati o il Comitato Unico di Garanzia interno sull&#39;utilizzo dell&#39;AI in selezione, e concordare principi (ad esempio, un principio aziendale potrebbe essere: &quot;nessun candidato viene mai escluso solo dall&#39;algoritmo, se non dopo verifica umana&quot;). Monitoraggio continuo: misurare se post-implementazione √® cambiata la composizione del personale assunto (es. l&#39;AI involontariamente ha alzato o abbassato la diversit√†?). Se emergono trend negativi, intervenire. Dal lato sicurezza/protezione dati: loggare gli accessi al sistema di recruiting AI (chi ha consultato i profili, ecc.), e garantire che dopo conclusa la ricerca i dati dei candidati non assunti siano conservati solo per il tempo lecito (di solito max 6-12 mesi salvo consenso per opportunit√† future). L&#39;IA Act inoltre richieder√† all&#39;utente (banca) di registrare l&#39;uso dell&#39;AI HR nel database (se soggetto pubblico o che fornisce servizi pubblici - le banche non lo sono, ma se partecipassero a programmi pubblici di job placement forse; comunque la FRIA risulter√† documentata).</p>
<p><strong>Esempi pratici:</strong> Caso famoso: <strong>Amazon Recruiting Tool</strong> - un algoritmo che dava punteggi ai CV per ruoli IT, fu dismesso perch√© si scopr√¨ essere sessista (aveva appreso da dati storici che la maggior parte degli assunti erano uomini e penalizzava parole chiave femminili nei CV)<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=I%20rischi%20non%20sono%20teorici,economiche%20e%20sociali%20che%20comportano">[61]</a>. Questo √® citato spesso come monito: feed di training bias ‚Üí output bias. Dopo quell&#39;episodio, molte aziende hanno frenato sull&#39;AI HR o puntato a modelli trasparenti. In UE, la start-up <em>HireVue</em> che offriva video-interview AI fu criticata da autorit√† e advocacy groups, portando l&#39;azienda a rimuovere l&#39;analisi facciale/emotiva e mantenere solo analisi del contenuto verbale (meno intrusiva). La <strong>Germania</strong> ha un approccio cauto: il Betriebsrat (consiglio lavoratori) spesso chiede di approvare l&#39;uso di software di selezione automatizzata. Con l&#39;AI Act in arrivo, grandi gruppi come Deutsche Telekom hanno pubblicamente affermato di stare testando i propri algoritmi HR per eliminare possibili pregiudizi e di predisporre <em>&quot;Algorithmen T√úV&quot;</em> (certificazione). In Italia, alcune banche hanno iniziato ad usare test attitudinali online con scoring automatizzato e poi HR li usano come uno dei fattori: questa mitigazione (AI come &quot;punteggio aggiuntivo&quot; e non unico fattore) √® considerata pi√π sicura. Dal 2023, con il Decreto Trasparenza, abbiamo visto alcune aziende includere nell&#39;informativa ai candidati frasi come: &quot;il Suo CV potr√† essere sottoposto a pre-analisi mediante sistemi automatizzati, ferma restando valutazione finale da parte di recruiter&quot; - segno che gi√† ora la disclosure √® necessaria. Probabilmente assisteremo in audit futuri a controlli su come l&#39;AI decide in HR: la banca farebbe bene ad essere in anticipo, documentando tutto (ad esempio tenendo i <em>dati di training del modello HR</em> archiviati e l&#39;analisi di impatto, cos√¨ da poterli esibire se il Garante o l&#39;ispettorato lavoro chiedessero chiarimenti su criteri di selezione).</p>
<h2 id="use-case-5-kyc-e-verifica-documentale-onboarding-clienti"><strong>Use Case 5: KYC e Verifica Documentale (onboarding clienti)</strong></h2>
<p><strong>Descrizione:</strong> Sistemi IA impiegati nel processo di adeguata verifica della clientela (<em>Know Your Customer</em>) e nell&#39;autenticazione/validazione di documenti. Tipicamente, durante l&#39;onboarding digitale di un nuovo cliente, vengono raccolti documenti di identit√†, foto/selfie, eventualmente video; l&#39;IA pu√≤ essere usata per: riconoscere il tipo di documento e leggerne automaticamente i dati (OCR intelligente), verificare l&#39;autenticit√† del documento (document analysis AI che controlla pattern di sicurezza, font, foto vs ologrammi, etc.), confrontare la foto del documento con il selfie del cliente (<strong>face matching biometrico</strong> per assicurare che sia la stessa persona), controllare in background liste sanzioni o PEP (anche qui con algoritmi name-matching), e validare in generale se i dati forniti soddisfano requisiti normativi. Inoltre, l&#39;IA pu√≤ essere usata per estrarre informazioni da documenti come buste paga, bollette, visure camerali presentati dal cliente, semplificando la due diligence.</p>
<p><strong>Rilevanza regolamentare:</strong> <em>Non espressamente in high-risk AI Act</em>, a meno che includa elementi biometrici con finalit√† riconducibili a controllo pubblico. L&#39;uso di <strong>biometria per verifica identit√†</strong> in contesto privato (onboarding bancario) non √® classificato in Allegato III come tale. L&#39;AI Act tratta la biometria per identificazione remota in luoghi pubblici da parte forze ordine (un&#39;altra cosa, spesso vietata salvo eccezioni) e la <em>verifica di documenti</em> √® menzionata come esclusa per contesto migrazione<a href="https://artificialintelligenceact.eu/annex/3/#:~:text=,the%20verification%20of%20travel%20documents">[62]</a>. Quindi, un sistema di face matching 1:1 (confronto selfie vs foto documento) non rientra tra high-risk secondo lettera stretta. Tuttavia, c&#39;√® margine interpretativo: alcuni potrebbero sostenere che garantire l&#39;identit√† certa √® <em>essenziale per accesso a servizi finanziari</em>, e un errore qui pu√≤ avere impatto su sicurezza (es. account takeover). Ma l&#39;AI Act non l&#39;ha incluso in categoria 5 (che copre credito, assicurazioni, etc.). Pertanto, niente obbligo di certificazione high-risk per il software di face recognition usato dall&#39;istituto, n√© FRIA obbligatoria per legge. <em>Tuttavia</em>, va considerato che in KYC si toccano dati molto sensibili (documenti identificativi, biometria facciale) e diritti fondamentali come privacy e diritto all&#39;identit√† personale - una <em>FRIA volontaria</em> sarebbe opportuna per valutare come l&#39;AI incide (es. rischio di esclusione: un sistema di verifica documento mal calibrato potrebbe respingere clienti legittimi, magari pi√π spesso appartenenti a certe nazionalit√† con documenti meno noti, creando disparit√†). Inoltre, l&#39;AI Act vieta la categorizzazione biometrica per inferire dati sensibili (es. non si potrebbe usare la foto per dedurre etnia o et√† per scopi non necessari) - non √® l&#39;obiettivo del KYC, ma assicurarsi che il fornitore del software non faccia cose oltre la semplice verifica. In sintesi, KYC AI √® <strong>limited risk AI Act</strong>, ma con obblighi generali di trasparenza se interagisce (qui il cliente sa di essere ripreso, quindi ok) e con forti requisiti privacy.</p>
<p><strong>Privacy &amp; DPIA:</strong> Quasi certamente s√¨. Il Garante ha incluso <em>&quot;trattamenti sistematici di dati biometrici&quot;</em> tra quelli che richiedono DPIA<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=relativi%20a%20condanne%20penali%20e,della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di%20trattamento">[63]</a>. Una procedura di video-identificazione con riconoscimento facciale rientra in questo (tratta volti su scala potenzialmente ampia). Anche l&#39;OCR di documenti ID tocca dati identificativi critici (numero documento, ecc.) e pu√≤ rientrare in &quot;dati estremamente personali&quot; (tra cui i documenti identit√†) su larga scala<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=tramite%20reti%20o%20di%20sorveglianza,Big">[64]</a>. La DPIA valuter√†: base giuridica (nella maggior parte dei casi, l&#39;identificazione cliente √® obbligo di legge AML, quindi base di legge art.6(1)(c) GDPR - il Garante italiano su SPID e CIE ha detto che il riconoscimento facciale pu√≤ basarsi su obbligo normativo visto che la legge lo prevede come metodo possibile, oppure su interesse legittimo se non obbligatorio ma offerto come opzione di comodo); necessita di verificare che sia usato il <em>minimo necessario</em>: es. molti sistemi registrano un breve video dell&#39;utente che mostra documento e fa movimenti (liveness detection), ci√≤ genera molti dati biometrici - la DPIA analizza se alternative meno invasive esistono (es. solo foto + sms OTP? etc.). Attenzione particolare ai <strong>consensi</strong>: per dati biometrici, il GDPR richiede una base specifica (art.9); qui la base principale √® l&#39;adempimento di obbligo antiriciclaggio, che pu√≤ coprire anche l&#39;acquisizione foto e documento. Il Garante con provv. 2021 su riconoscimento facciale per FEA aveva richiesto consenso esplicito; per onboarding video, finora l&#39;ha permesso come misura AML (es. la guida IVASS 2021 e circolari Banca d&#39;It 2019 su video riconoscimento l&#39;hanno consentito con misure). DPIA inoltre su: <em>accuratezza algoritmi</em> - tasso di falso rifiuto, soprattutto per alcuni gruppi (studi mostrano che face recognition pu√≤ avere errori pi√π alti su persone di colore o molto giovani/anziane). Va considerato e mitigato: e.g. fornire opportunit√† alternative di identificazione a chi viene erroneamente non riconosciuto (come intervento manuale di un operatore). <em>Sicurezza:</em> questi sistemi raccolgono dati ultrasensibili (video volti, immagini documenti) - protezioni cifratura forte, canali sicuri, e limitare l&#39;accesso (solo personale compliance, per es.). <em>Conservazione:</em> il video di identificazione va tenuto il tempo richiesto da normative AML (5 anni dalla fine rapporto) poi distrutto. <em>Diritti interessato:</em> qui si scontra con AML - in genere il cliente non pu√≤ opporsi all&#39;identificazione (o accetta quell&#39;onboarding o non apre conto), ma va informato adeguatamente sul processo (con informativa chiara che spiega c&#39;√® un&#39;analisi automatica del documento e riconoscimento volto). Se la banca volesse fare decisione automatica (&quot;la sua identificazione √® fallita, contratto non aperto&quot;), ricade in art.22 - mitigato dal fatto che in pratica se l&#39;AI fallisce, c&#39;√® di solito fallback come &quot;ripeti la procedura&quot; o &quot;vai in filiale&quot;. Comunque, includere intervento umano in caso di fallimento √® fortemente consigliato per evitare problemi.</p>
<p><strong>Obblighi/controlli aggiuntivi:</strong> La regolamentazione specifica di Banca d&#39;Italia e IVASS consente la <em>video-identificazione a distanza</em> a patto di seguire linee guida tecniche (Circolare 285 agg. 2020 di Banca d&#39;Italia, Provv. IVASS 97/2020). Queste linee guida spesso prescrivono: uso di sistemi di riconoscimento di sicurezza adeguata (livello SPID 2 o 3), presenza di controlli di <em>liveness</em> (per evitare spoofing con foto o deepfake), conservazione delle registrazioni, intervento umano in casi dubbi. Quindi la banca deve assicurare che l&#39;AI usata soddisfi tali requisiti. Per esempio, per liveness si usa AI che chiede all&#39;utente di girare la testa o leggere 3 cifre: la normativa chiede che se l&#39;algoritmo non √® sicuro al 100%, un operatore ricontrolli. Inoltre, normative antiterrorismo e antifrode: la banca deve verificare il documento con liste di documenti rubati/perduti (spesso integrato nell&#39;AI doc analysis). Il sistema deve generare evidenze per eventuali ispezioni: se la banca viene accusata di aver identificato male un truffatore, deve poter mostrare log: &quot;il sistema ha validato erroneamente il doc X; ecco come √® successo&quot;. A tal fine, definire KPI di performance e soglie di ri-verifica manuale: es. se confidenza match volto-documento &lt; 90%, far intervenire operatore anzich√© rifiutare o accettare alla cieca. In quanto non high-risk AI Act, non c&#39;√® obbligo di registrazione database, ma essendo legato ad antimoney laundering, la banca dovr√† comunque notificare alla Banca d&#39;Italia l&#39;adozione di procedure innovative (spesso richiesto informalmente in vigilanza, per capire come rispetta requisiti).</p>
<p><strong>Misure di oversight e monitoraggio:</strong> Qui oversight significa: assicurare che quando l&#39;AI dice &quot;documento valido, persona coincide&quot; la decisione possa essere rivista se necessario. Molte banche gi√† prevedono che un <em>operatore di backoffice</em> ricontrolli un campione di pratiche approvate automaticamente dal sistema, soprattutto all&#39;inizio (validazione umana a posteriori, per &quot;tarare&quot; l&#39;AI). Se emergono falsi positivi (documenti falsi accettati), va immediatamente segnalato e il modello ri-addestrato o regole aggiuntive messe. Al contempo, monitorare i <em>falsi negativi</em> (clienti reali scartati): es. se molti utenti con carta d&#39;identit√† di un certo paese falliscono la verifica, forse l&#39;AI non conosce bene quel documento ‚Üí aggiungere campioni e migliorare. Implementare feedback loop: es. se utente contatta supporto perch√© non riesce a farsi riconoscere, quell&#39;evento va registrato e la causa analizzata. L&#39;oversight richiede competenze: il compliance o il responsabile onboarding deve poter interpretare i <em>confidence score</em> del modello e decidere soglie. Ad esempio, decidere che sotto 80% di matching il caso √® rifiutato, tra 80-90% lo vede un umano, sopra 90% auto-OK. Queste politiche vanno riviste periodicamente in base ai risultati reali. Un altro controllo: integrazione con antifrode - se poi risulta un account aperto era fraudolento, fare <em>post-mortem</em> per capire se l&#39;AI avrebbe dovuto rilevarlo (magari il truffatore ha usato un documento vero ma non suo, e l&#39;AI ha fallito l&#39;abbinamento? Studiare e migliorare). Quindi un team (IT + compliance + risk) dovrebbe rivedere statistiche mensili: numero di onboarding effettuati automaticamente vs manuali, errori segnalati, etc. Documentare questi controlli per eventuali audit (anche interni di funzione Antiriciclaggio).</p>
<p><strong>Esempi pratici:</strong> In Italia quasi tutte le banche che offrono apertura conto online usano tecnologie di verifica documenti e face recognition (fornite da vendor come Experian, Onfido, InfoCert, etc.). Il Garante Privacy ha approvato linee guida SPID/CIE che usano riconoscimento facciale per identificare soggetti a distanza, fissando cautele: in particolare richiede che l&#39;algoritmo sia altamente accurato e che all&#39;utente sia data un&#39;alternativa se il riconoscimento fallisce (es. andare in presenza o procedura con operatore in videochat). Questo parallelamente vale in banca: spesso se il self onboarding fallisce 2-3 volte, si viene contattati da un operatore o invitati in filiale. Un caso di cronaca: <em>2019, truffe conti online</em> - criminali sfruttavano falle in sistemi di video riconoscimento usando volti di altri simili; questo ha spinto Banca d&#39;Italia a rafforzare indicazioni su liveness detection. Dal lato customer experience, <em>Intesa Sanpaolo</em> introdusse il riconoscimento facciale in filiale per pre-compilare dati da documento: fu autorizzato dal Garante con garanzie (dati non conservati dopo uso). Di nuovo, dimostra fattibilit√† ma con DPIA presentata. Sul mercato, errori famosi di Face Recognition su differenti etnie (es. pi√π alti falsi negativi su volti con pelle scura in alcuni algoritmi) mettono in guardia: la banca deve chiedere al fornitore prove di test cross-demografici e magari condurre test interni sul proprio campione clienti (considerando anche molte banche hanno clienti stranieri). A livello AI Act, anche se KYC AI non √® high-risk di per s√©, il <em>&quot;Digital Identity Wallet&quot;</em> europeo in arrivo e regolamenti eIDAS correlati standardizzeranno i processi di identit√†: le banche dovranno assicurare che i loro sistemi AI si integrino con questi e rispettino standard certificati (es. qualificazione dei servizi di identit√†: √® probabile che i fornitori di soluzioni di video onboarding cercheranno schemi di certificazione volontaria). Quindi conviene gi√† orientarsi a soluzioni AI KYC di provider noti che seguono standard di settore (ISO/IEC 30107 per anti-spoofing, ecc.). La <em>sandbox Fintech MEF 2021</em> ha ammesso un progetto O-KYC<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi">[39]</a> basato su DLT per condividere info KYC: pur non centrato sull&#39;uso di AI, indica l&#39;interesse innovazione in questo campo. Un futuro scenario √® usare AI per aggiornamento continuo KYC (monitorare anomalie nei documenti nel tempo, etc.), che si legher√† al case AML.</p>
<h1 id="questioni-residue-aperte">Questioni Residue Aperte</h1>
<ul>
<li><strong>Criteri pratici per &quot;rischio significativo&quot; (Art.6(3) AI Act):</strong> non √® chiaro come un fornitore o utilizzatore potr√† dimostrare che un sistema rientrante in Allegato III <strong>non</strong> presenta un <em>rischio significativo</em> e quindi esentarlo dal regime high-risk<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III">[21]</a>. Serviranno linee guida su metriche di rischio e chi avalla tale auto-valutazione, per evitare arbitri e under-classification.</li>
<li><strong>Coordinamento tra Autorit√† (AI Act vs privacy vs settore):</strong> chi sar√† in pratica l&#39;autorit√† di riferimento per vigilare sui sistemi AI bancari? Il <em>Market Surveillance Authority</em> per high-risk AI potrebbe essere Banca d&#39;Italia (vigilanza bancaria) o un nuovo soggetto; il Garante Privacy manterr√† ruolo su DPIA e data protection; l&#39;EBA/ECB avranno voce tramite l&#39;<em>AI Board</em>. Servono protocolli per evitare conflitti o vuoti di competenza nelle valutazioni ex ante (FRIA) e controlli ex post.</li>
<li><strong>Standard di riferimento e certificazioni:</strong> dato il forte carattere tecnico dei requisiti AI Act, ci si chiede quali standard tecnici adotteranno le banche per dimostrare conformit√† (es. ISO 42001 AI Management? certificazioni di bias/fairness?). EBA nota che Commissione emaner√† linee guida classificazione high-risk entro Feb 2026<a href="https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/#:~:text=EBA%20Factsheet%20%E2%80%93%20AI%20Act%3A,banking%20and%20payments%20sector%2C%20by">[65]</a>, ma sul piano operativo le banche vorrebbero un framework unificato. La domanda aperta: <em>conviene attendere standard ufficiali o partire con certificazioni volontarie (es. audit etici, attestazioni da terze parti) per stare avanti?</em></li>
<li><strong>Integrazione DPIA-FRIA:</strong> come implementare praticamente un processo unico che copra entrambe? Si deve produrre due report separati (uno per Garante, uno per autorit√† AI Act) o un unico <em>&quot;AI Risk Assessment Report&quot;</em> baster√† per entrambi scopi? E in caso di valutazione d&#39;impatto con esito dubbio (rischio residuo alto): per GDPR c&#39;√® obbligo di consultazione Garante, per AI Act non √® prevista ma magari l&#39;Autorit√† mercato pu√≤ intervenire; bisognerebbe capire come allineare queste escalation.</li>
<li><strong>Gestione dei fornitori terzi e liability:</strong> se un vendor fornisce un sistema AI non conforme e la banca subisce una violazione (es. multa per discriminazione), su chi ricade la responsabilit√†? Il regime AI Act prevede responsabilit√† primaria del <em>provider</em> per requisiti tecnici e del <em>user</em> per uso improprio. Ma nei contratti reali tra banca e vendor serviranno clausole robuste su garanzie, indennizzi e accesso alle informazioni (audit). La questione aperta: le banche potranno chiedere contrattualmente ai vendor di condurre e condividere una FRIA da <em>provider</em>? Oppure ogni banca dovr√† farla in solitudine anche per pacchetti standard?</li>
<li><strong>Metriche di fairness e soglie accettabili:</strong> i regolatori richiederanno alle banche di quantificare e mantenere certi livelli di fairness nei modelli (es. <em>&quot;disparate impact ratio&quot;</em> non peggiore di 80%)? O rimarr√† tutto qualitativo? L&#39;assenza di criteri quantitativi univoci √® un problema: una banca potrebbe considerare accettabile un leggero scostamento, un&#39;altra no. Ci si chiede se l&#39;EBA o l&#39;AI Office elaboreranno guidance in tal senso.</li>
<li><strong>Uso di dati sensibili per finalit√† etiche (bias correction):</strong> paradosso noto - per testare se un modello √® discriminante servirebbe a volte considerare la variabile protetta (es. genere) nei dati; ma ci√≤ √® vietato per decisione. Il Garante permetter√† di usare dati sensibili simulati o raccolti post-assunzione per validare fairness? Su questo c&#39;√® incertezza e le banche faticano a definire metodologie di bias audit rispettose del GDPR.</li>
<li><strong>Interoperabilit√† con regolamenti futuri (ESG, AI liability):</strong> come si combineranno i requisiti AI Act con altri emergenti, ad esempio le iniziative sull&#39;<em>AI liability</em> (responsabilit√† civile per danni da AI) o la normativa ESG (che potrebbe includere uso etico di AI)? Le banche dovranno mappare anche questi aspetti - e rimane domanda aperta se la documentazione predisposta (es. registro eventi AI, log decisioni) potr√† essere usata contro la banca in cause civili (un tema di liability non risolto: troppa trasparenza potrebbe esporre a contenziosi).</li>
<li>**Ruolo dell&#39;**AI Office <strong>UE vs Autorit√† nazionali:</strong> l&#39;AI Office avr√† poteri di supervisione soprattutto su fondation models e high-risk cross-border. Ma potr√† emanare linee guida vincolanti anche per settori regolati? Le banche dovranno tenere un occhio a possibili indicazioni sovranazionali aggiuntive. La questione: se l&#39;AI Office (Commissione) identifica un modello bancario come <em>&quot;alto impatto&quot;</em> GPAI, potrebbe intervenire direttamente? Va chiarito.</li>
<li><strong>Aggiornamento continuo del tool vs evoluzione norme:</strong> riconoscendo che le interpretazioni e prassi attorno ad AI Act e DPIA evolveranno (giurisprudenza, orientamenti EDPB, nuove modifiche regolamentari), come mantenere il Navigator sempre aggiornato? Il processo di desk research ha limiti - alcune questioni saranno risolte solo con <em>regulatory feedback loop</em> (es. prime FRIA effettivamente svolte, sanzioni inflitte, ecc.). √à aperto il tema di governance del tool: chi in azienda (o consorzio ABI Lab) lo aggiorner√† con le <em>lessons learned</em> e nuove fonti normative man mano che emergono? Questo determina la sostenibilit√† a lungo termine della soluzione proposta.</li>
</ul>
<h1 id="bibliografia-commentata">Bibliografia Commentata</h1>
<ul>
<li><strong>Regolamento (UE) 2024/1689 &quot;AI Act&quot;</strong> - Testo normativo fondamentale adottato nel giugno 2024<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a>. Definisce il quadro regolatorio UE per l&#39;IA con approccio basato sul rischio. <em>Rilevanza:</em> Allegati e articoli citati identificano i casi bancari high-risk (credito, HR) e impongono obblighi (es. art. 14 oversight umano, art. 27 FRIA). Base per molte compliance action del Navigator.</li>
<li><strong>Considerando AI Act (58) e (57)</strong> - Parti introduttive del Regolamento che spiegano le motivazioni per includere credito e occupazione tra gli usi ad alto rischio<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei">[58]</a>. <em>Rilevanza:</em> evidenziano i rischi di discriminazione e impatto sociale di tali sistemi, fornendo giustificazione (da citare in FRIA) e ricordano eccezioni (fraud detection esclusa<a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi">[3]</a>).</li>
<li><strong>Garante Privacy, Provv. n. 467/2018 (Allegato DPIA)</strong> - Delibera italiana che elenca i tipi di trattamento obbligatoriamente soggetti a DPIA<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[25]</a>. Include scoring, decisioni automatizzate, biometria, monitoraggio lavoratori ecc. <em>Rilevanza:</em> √® il riferimento normativo nazionale per capire subito se un progetto AI bancario richiede DPIA. Il Navigator ne incorpora i criteri.</li>
<li><strong>Linee Guida WP29/EDPB WP248 sulla DPIA</strong> - Linee guida europee (2017, confermate EDPB 2018) che dettagliano criteri di rischio elevato e metodologia DPIA<a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a>. <em>Rilevanza:</em> forniscono la checklist dei 9 criteri usata nel wizard per determinare obbligo DPIA e suggeriscono best practice su coinvolgimento stakeholder, aggiornamento DPIA, ecc.</li>
<li><strong>Comunicato Consiglio UE 9 Dec 2023 (Accordo AI Act)</strong> - Nota stampa del Consiglio<a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=Transparency%20and%20protection%20of%20fundamental,rights">[66]</a> che annuncia l&#39;accordo politico. <em>Rilevanza:</em> contiene in linguaggio chiaro le novit√† come l&#39;obbligo di FRIA per i deployer e l&#39;estensione delle trasparenze (es. emotion recognition disclosure). Utile per estrapolare concetti chiave e spiegazioni non tecniche agli utenti.</li>
<li><strong>EBA Risk Assessment Report, &quot;Special topic AI&quot; (Nov 2024)</strong> - Rapporto dell&#39;Autorit√† Bancaria Europea<a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=Regarding%20use%20cases%2C%20AI%20is,banks%20are%20leveraging%20AI%20in">[67]</a><a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent">[16]</a>. Descrive diffusione AI in banche EU, use case comuni (profilazione clienti, supporto, fraud detection, credit scoring) e relative sfide (skill gap, governance). <em>Rilevanza:</em> offre dati e conferme sull&#39;uso di AI nei 5 use case target e raccomandazioni (ad es. human-in-loop nel GPAI<a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent">[16]</a>). Il Navigator lo usa per tarare la sezione &quot;Ambito di utilizzo nel settore&quot; e per supportare raccomandazioni di prudenza.</li>
<li><strong>Lettera EBA Chair Jos√© M. Campa (Nov 2025) - Mapping AI Act vs Regolamentazione bancaria</strong><a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of">[36]</a><a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving">[68]</a> - Documento indirizzato alla Commissione che esamina sovrapposizioni tra obblighi AI Act e normativa finanziaria (CRR, CRD, MiFID, ecc.). <em>Rilevanza:</em> conferma che molti requisiti (es. human oversight, data governance) hanno gi√† equivalenti nelle regole esistenti, ma nessuna esenzione ad hoc √® prevista. Aiuta il Navigator a evidenziare dove un adempimento AI Act pu√≤ essere soddisfatto tramite compliance esistente e dove invece √® addizionale.</li>
<li><strong>Paradigma.it - &quot;AI e concessione del credito: tra innovazione e responsabilit√†&quot; (articolo Ott 2025)</strong><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[11]</a><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano">[35]</a> - Approfondimento giuridico italiano. Sintetizza obblighi bancari (TUB 124-bis: merito creditizio con info adeguate; obbligo verifica umana) e principi delle EBA Guidelines 2020 sul loan origination (trasparenza, tracciabilit√†, non-discriminazione, supervisione costante) con commento Banca d&#39;It 2022 sul ruolo ausiliario dell&#39;AI<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[12]</a>. <em>Rilevanza:</em> fornisce autorevole interpretazione locale su come bilanciare innovazione AI e responsabilit√† nelle decisioni credito. Citato nel Navigator per supportare raccomandazione &quot;AI a supporto, non in sostituzione del giudizio umano&quot; e inquadrare obblighi legali italiani di controllo umano.</li>
<li><strong>Banca d&#39;Italia - Quaderno Economia e Finanza n.721 &quot;Intelligenza artificiale nel credit scoring&quot; (2022)</strong><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove">[32]</a><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in">[13]</a> - Studio empirico e regolatorio. Analizza benefici/rischi AI nel credito, copertura normativa e risultati di survey sulle banche italiane. Conclude che la normativa prudenziale vigente copre gran parte dei rischi AI-ML (governance, controlli), ma evidenzia lacune su principio di non-discriminazione e complessit√† di tradurre i principi etici in prassi<a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove">[32]</a><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti">[23]</a>. <em>Rilevanza:</em> questa fonte interna all&#39;authority italiana rinforza l&#39;importanza di misure di governance (che il Navigator enfatizza) e anticipa possibili richieste del regolatore su fairness. √à utilizzata come base per suggerire nel wizard misure aggiuntive anche se non strettamente richieste dal testo di legge, in quanto &quot;buona prassi prudenziale&quot;.</li>
<li><strong>Garante Privacy - Provv. n. 755/2024 (caso sanzione IA generativa)</strong><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata">[42]</a><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello">[43]</a> - Provvedimento sanzionatorio contro una nota societ√† sviluppatrice di un chatbot generativo (presumibilmente OpenAI). Contesta mancata notifica data breach, carenza di base giuridica per training data personali, violazione trasparenza. <em>Rilevanza:</em> bench√© non settore bancario, segnala posizioni stringenti del Garante su punti cruciali: obbligo di base giuridica identificata prima di usare dati per addestrare modelli, necessit√† di informare gli interessati anche in contesti innovativi. Il Navigator usa questo precedente per avvisare le banche di evitare simili lacune (es. se sviluppano modelli con dati cliente interni, badare a finalit√† dichiarate e informative).</li>
<li><strong>Direttiva (UE) 2019/1152 (&quot;Direttiva Trasparenza Lavoro&quot;) - D.lgs. 104/2022 (art.4)</strong> - Norma che impone ai datori di informare i lavoratori sull&#39;uso di sistemi decisionali automatizzati in ambito lavorativo. <em>Rilevanza:</em> specificamente citata per il case HR: obbliga banche a disclosure verso candidati/dipendenti se impiegano AI in selezione o valutazione, con dettagli su logica, fattori e obiettivi. Il Navigator integra questo vincolo nelle raccomandazioni per l&#39;uso case Recruiting (es. generazione di un&#39;informativa AI per candidati).</li>
<li><strong>D.Lgs 231/2007 (Normativa Antiriciclaggio) e Regole tecniche UIF/Banca d&#39;It.</strong> - Corpus normativo che disciplina KYC, adeguata verifica e controlli antiriciclaggio. Include l&#39;obbligo di identificazione a distanza secondo regole precise e il principio di approccio basato su rischio. <em>Rilevanza:</em> per use case AML/KYC, definisce il perimetro legale entro cui l&#39;AI deve operare (non pu√≤ abbassare gli standard di due diligence). Il Navigator ne ricorda gli obblighi (es. conservazione dati 5 anni, analisi manuale dei sospetti) integrandoli con quelli tecnologici.</li>
<li><strong>ISO/IEC TR 24027:2021 (Bias in AI Systems)</strong> e <strong>NIST AI Risk Management Framework 1.0 (2023)</strong> - Standard e framework internazionali (non normativi) che affrontano metodologie per valutare e mitigare bias nei sistemi AI e gestire i rischi AI. <em>Rilevanza:</em> utili come riferimento tecnico per l&#39;utente pi√π avanzato; il Navigator li cita in bibliografia commentata come risorse per implementare concretamente le indicazioni di fairness e risk management (ad esempio: <em>&quot;ISO 24027 fornisce tassonomia di bias e approcci per misurarli&quot;</em>). Questo aiuta i team IT/Data Science a tradurre le raccomandazioni generali in azioni.</li>
<li><strong>Linee Guida EBA 2020 on Loan Origination &amp; Monitoring (EBA/GL/2020/06)</strong> - Orientamenti vincolanti emanati dall&#39;EBA<a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[11]</a>. Richiedono, tra l&#39;altro, che l&#39;uso di modelli automatici nel processo di concessione crediti rispetti principi di non-discriminazione e incorporate human judgement. <em>Rilevanza:</em> direttamente applicabili alle banche EU, completano il quadro normative credito. Il Navigator li incorpora nella scheda credito (es. sul dovere di spiegabilit√† e controllo umano) e come evidenza in evidence table sulla supervisione costante.</li>
<li><strong>Circolare Banca d&#39;Italia n.285 (agg. 2020), Disposizioni in materia di adeguata verifica a distanza</strong> - Normativa secondaria italiana che permette l&#39;identificazione via audio-video e stabilisce requisiti tecnico-procedurali (es. qualit√† video, conservazione registrazioni, intervento umano in caso di dubbi). <em>Rilevanza:</em> definisce lo <em>standard minimo</em> per use case KYC digitale. Il Navigator ne tiene conto raccomandando misure come liveness detection, verifica manuale se confidenza bassa, etc., conformi a tali disposizioni.</li>
<li><strong>Documentazione Sandbox Regolamentare Italiana (2021-2023)</strong> - In particolare: <em>Comunicato Banca d&#39;Italia su esiti prima finestra sandbox</em><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Informativa%20sulla%20conclusione%20della%20sperimentazione,di%20richiesta%20della%20certificazione%2C%20e">[69]</a><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi">[38]</a>. Descrive progetti Fintech innovativi testati, es. piattaforme di credit scoring e soluzioni KYC in DLT, evidenziando benefici e condizioni di successo (&quot;soluzione idonea a operare fuori sandbox salvo rispetto di tutte le norme&quot;)<a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione">[40]</a>. <em>Rilevanza:</em> fornisce <strong>casi concreti italiani</strong> da citare nelle schede use case, a dimostrazione di fattibilit√† e punti di attenzione emersi. Il Navigator li usa per dare confidenza (es. credit scoring AI testato con successo in sandbox) e sottolineare che compliance full rimane necessaria anche dopo innovazione.</li>
<li><strong>Fonti EDPB/Europrivacy su AI &amp; Privacy:</strong> es. <em>EDPB Statement 2022 on the AI Act</em>, <em>EDPS Opinion on AI Act (June 2021)</em>. Queste esprimono posizioni dei garanti privacy europei sul progetto di AI Act (invocando rigore su FRIA, ban di alcuni usi) e sul rapporto con GDPR. <em>Rilevanza:</em> bench√© anteriori al testo finale, ribadiscono l&#39;importanza di non abbassare gli standard privacy nell&#39;applicare AI (il Navigator ne tiene conto assicurando che DPIA e principi GDPR restino centrali). Inserite in bibliografia come approfondimento per chi volesse la prospettiva delle Autorit√† privacy sul regolamento AI (ad es. EDPS chiedeva FRIA obbligatoria per tutti gli high-risk - poi accolta).</li>
<li><strong>Linee Guida OCSE &amp; Commissione UE su AI Trustworthy (2019-2020)</strong> - Queste non sono norme, ma contengono i 7 principi etici per IA affidabile (trasparenza, controllo umano, diversit√†/non discriminazione, responsabilit√†, ecc.). <em>Rilevanza:</em> costituiscono il fondamento concettuale di molte obbligazioni poi normative. Il Navigator le menziona in background (es. nella evidence table su convergenza principi<a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in">[13]</a>) e le indica in bibliografia come lettura di contesto per comprendere la filosofia della regolamentazione (es. perch√© human oversight √® cruciale).</li>
</ul>
<p><em>(N.B.: Tutte le fonti primarie citate - leggi, linee guida EBA, provvedimenti Garante - sono da considerarsi autorevoli. Le fonti secondarie (articoli, studi) selezionate sono supportate da riferimenti ufficiali e servono a chiarire l&#39;interpretazione pratica. Questo elenco finale contiene 18 voci, rimanendo entro il limite di 20 fonti.)</em></p>
<p><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=utilizzati%20per%20valutare%20il%20merito,previsti%20dal%20diritto%20dell%27Unione%20al">[1]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=i%20sistemi%20di%20IA%20destinati,o%C2%A0filtrare%20le%20candidature%20e%C2%A0valutare%20i%C2%A0candidati">[2]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=o%C2%A0sull%27orientamento%20sessuale%2C%20o%C2%A0possono%20dar%20vita,ad%20alto%20rischio%20ai%20sensi">[3]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=1,che%20comprende%20gli%20elementi%20seguenti">[6]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=una%20descrizione%20dei%20processi%20del,con%20la%20sua%20finalit%C3%A0%20prevista">[7]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=d">[8]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=3,da%20tale%20obbligo%20di%20notifica">[9]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=4,d%27impatto%20sulla%20protezione%20dei%20dati">[10]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=affidata%20la%20sorveglianza%20umana%20dispongano,quale%20%C3%A8%20stata%20affidata%20la">[14]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario">[15]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=di%20IA%20specificamente%20destinati%20a%C2%A0essere,L%27impatto%20dell%27utilizzo">[20]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=considerati%20ad%20alto%20rischio%20anche,di%20IA%20di%20cui%20all%27allegato%C2%A0III">[21]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=5,presente%20articolo%20in%20modo%20semplificato">[22]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,necessarie%20nonch%C3%A9%20del%20sostegno%20necessario">[27]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=sorveglianza%20umana%20affinch%C3%A9%20prenda%20decisioni,significative%20per%20le%20persone%20in">[30]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Sorveglianza%20umana">[31]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=l%27accesso%20di%20tali%20persone%20alle,previsti%20dal%20diritto%20dell%27Unione%20al">[33]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=usati%20nel%20settore%20elencati%20nell%27allegato%C2%A0III%2C,una%20valutazione%20dell%27impatto%20sui%20diritti">[45]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=2,coinvolgimento%20di%20un%20organismo%20notificato">[46]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=match%20at%20L7793%20alto%20rischio,dati%20dell%27UE%20di%20cui%20all%27articolo%C2%A071">[47]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=Anche%20i%C2%A0sistemi%20di%20IA%20utilizzati,carriera%20e%C2%A0sostentamento%20e%C2%A0di%20diritti%20dei">[58]</a> <a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689#:~:text=lavoratori,I%C2%A0sistemi%20di%20IA">[59]</a> Regolamento - UE - 2024/1689 - EN - EUR-Lex</p>
<p><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689">https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32024R1689</a></p>
<p><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=1,ad%20esempio%20con%20la%20videosorveglianza">[4]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[5]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,la%20concessione%20di%20un%20finanziamento">[25]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=7,la%20concessione%20di%20un%20finanziamento">[52]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=particolari%20misure%20di%20carattere%20organizzativo,compresi%20i%20trattamenti%20che%20prevedono">[53]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=profilazione,del%20volume%20dei%20dati">[54]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=8,personali%20raccolti%20per%20finalit%C3%A0%20diverse">[55]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=9,ovvero%20della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di">[56]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=relativi%20a%20condanne%20penali%20e,della%20persistenza%2C%20dell%E2%80%99attivit%C3%A0%20di%20trattamento">[63]</a> <a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto#:~:text=tramite%20reti%20o%20di%20sorveglianza,Big">[64]</a> La valutazione d&#39;impatto</p>
<p><a href="https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto">https://www.latuaprivacy.com/site/approfondimenti/16-la-valutazione-d-impatto</a></p>
<p><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Le%20Linee%20Guida%20EBA%20del,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[11]</a> <a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=chiariscono%20che%20l%E2%80%99uso%20di%20modelli,%E2%80%9Cal%20posto%E2%80%9D%20del%20giudizio%20dell%E2%80%99intermediario">[12]</a> <a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=Un%20altro%20punto%20critico%20riguarda,possibilit%C3%A0%20di%20contestare%20eventuali%20errori">[19]</a> <a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=clientela,economiche%20e%20sociali%20che%20comportano">[24]</a> <a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=A%20questo%20si%20aggiunge%20il,presidi%20tecnici%20e%20giuridici%20solidi">[34]</a> <a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=classifica%20i%20sistemi%20di%20intelligenza,economiche%20e%20sociali%20che%20comportano">[35]</a> <a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/#:~:text=I%20rischi%20non%20sono%20teorici,economiche%20e%20sociali%20che%20comportano">[61]</a> AI e concessione del credito: tra innovazione e responsabilit√†</p>
<p><a href="https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/">https://paradigma.it/2025/10/21/ai-concessione-credito-responsabilita/</a></p>
<p><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cconvergenza%20internazionale%20sui%20principi,traduzione%20di%20questi%20principi%20in">[13]</a> <a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=nazionali%20e%20internazionali%20in%20materia,tutela%20dei%20diritti%20dei%20clienti">[23]</a> <a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=%EF%82%B7%20%E2%80%9Cla%20non%20discriminazione%20nei,concessione%20del%20credito%20riceve%20nuove">[32]</a> <a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf#:~:text=regolamentazione%20specifica%20sugli%20stessi,nelle%20disposizioni%20di%20trasparenza%20sono">[37]</a> bancaditalia.it</p>
<p><a href="https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf">https://www.bancaditalia.it/pubblicazioni/qef/2022-0721/QEF_721_IT.pdf</a></p>
<p><a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=used%2C%20GPAI%20could%20present%20challenges,the%20required%20skills%20and%20talent">[16]</a> <a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=challenges%20in%20consumer%20experiences">[17]</a> <a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=In%20view%20of%20these%20potential,potential%20effects%20and%20necessary%20mitigants">[29]</a> <a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence#:~:text=Regarding%20use%20cases%2C%20AI%20is,banks%20are%20leveraging%20AI%20in">[67]</a> Special topic - Artificial intelligence | European Banking Authority</p>
<p><a href="https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence">https://www.eba.europa.eu/publications-and-media/publications/special-topic-artificial-intelligence</a></p>
<p><a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=provisional%20agreement%20also%20provides%20for,exposed%20to%20such%20a%20system">[18]</a> <a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=The%20provisional%20agreement%20provides%20for,system%20to%20inform%20natural%20persons">[26]</a> <a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=For%20some%20uses%20of%20AI%2C,of%20predictive%20policing%20for%20individuals">[51]</a> <a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=will%20be%20banned%20from%20the,of%20predictive%20policing%20for%20individuals">[60]</a> <a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/#:~:text=Transparency%20and%20protection%20of%20fundamental,rights">[66]</a> Artificial intelligence act: Council and Parliament strike a deal on the first rules for AI in the world - Consilium</p>
<p><a href="https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/">https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/</a></p>
<p><a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=%E2%80%A2%20CRR%3A%20Article%20149,and%20personnel%20responsible%20for%20approving">[28]</a> <a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=requirements%20on%20high,includes%20a%20wide%20range%20of">[36]</a> <a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving">[48]</a> <a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf#:~:text=Human%20oversight%20Article%2014%20,and%20personnel%20responsible%20for%20approving">[68]</a> eba.europa.eu</p>
<p><a href="https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf">https://www.eba.europa.eu/sites/default/files/2025-11/2019d1b5-59f8-4149-ad3b-23cfcd4388a1/EBA%20Chair%20letter%20to%20Mr%20Berrigan%20and%20Mr%20Viola%20on%20outcome%20of%20EBA%E2%80%99s%20AI%20Act%20mapping%20exercise.pdf</a></p>
<p><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20una%20piattaforma%20tecnologica,durata%20massima%20di%2018%20mesi">[38]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20nuova%20modalit%C3%A0%20di,durata%20massima%20di%2018%20mesi">[39]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=la%20validazione%20della%20documentazione%20presentata,una%20futura%20commercializzazione%20della%20soluzione">[40]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=ICCREA%20Banca%20e%20Banca%20Monte,una%20futura%20commercializzazione%20della%20soluzione">[41]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=intermediari%20finanziari%20per%20offrire%20loro,durata%20massima%20di%2018%20mesi">[44]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=finalizzato%20a%20fornire%20alle%20banche,gestione%20del%20rischio%20di%20credito">[49]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=La%20sperimentazione%20%E2%80%93%20nell%27ambito%20della,operare%20al%20di%20fuori%20dell%27ambiente">[50]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Implementazione%20di%20un%20servizio%20evoluto,durata%20massima%20di%2018%20mesi">[57]</a> <a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html#:~:text=Informativa%20sulla%20conclusione%20della%20sperimentazione,di%20richiesta%20della%20certificazione%2C%20e">[69]</a> Banca d&#39;Italia - Progetti ammessi alla prima finestra temporale - sperimentazione conclusa</p>
<p><a href="https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html">https://www.bancaditalia.it/focus/sandbox/progetti-ammessi/index.html</a></p>
<p><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=,trattamento%20e%20deve%20essere%20specificata">[42]</a> <a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/#:~:text=Nel%20caso%20di%20specie%2C%20il,trattamento%20per%20l%E2%80%99addestramento%20del%20modello">[43]</a> Privacy e sviluppo sistemi di IA: trattamento illecito di dati personali - DB</p>
<p><a href="https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/">https://www.dirittobancario.it/art/privacy-e-sviluppo-sistemi-di-ia-trattamento-illecito-di-dati-personali/</a></p>
<p><a href="https://artificialintelligenceact.eu/annex/3/#:~:text=,the%20verification%20of%20travel%20documents">[62]</a> Annex III: High-Risk AI Systems Referred to in Article 6(2) | EU Artificial Intelligence Act</p>
<p><a href="https://artificialintelligenceact.eu/annex/3/">https://artificialintelligenceact.eu/annex/3/</a></p>
<p><a href="https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/#:~:text=EBA%20Factsheet%20%E2%80%93%20AI%20Act%3A,banking%20and%20payments%20sector%2C%20by">[65]</a> EBA Factsheet - AI Act: Implications for the EU banking and ...</p>
<p><a href="https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/">https://www.regulationtomorrow.com/france/fintech-fr/eba-factsheet-ai-act-implications-for-the-eu-banking-and-payments-sector/</a></p>
        </div>
      </section>
    </article>
  </main>
  <footer>
    <span data-it="¬© 2025 Mirko Calcaterra. Tutti i diritti riservati."
          data-en="¬© 2025 Mirko Calcaterra. All rights reserved.">
      ¬© 2025 Mirko Calcaterra. Tutti i diritti riservati.
    </span>
  </footer>
  <script>
    const BLOG_LANG_KEY = 'blogLang';
    const BLOG_THEME_KEY = 'blogTheme';
    const CURRENT_LANG = "it";
    const OTHER_LANG = "en";
    const OTHER_LANG_LINK = "../../../blog/en/ai-banking-dpia-ai-act/index.html";
    (function() {
      const body = document.body;
      const themeToggle = document.querySelector('.theme-toggle');
      const themeThumb = document.querySelector('.theme-toggle .theme-thumb');
      const langBtn = document.querySelector('.lang-btn');
      const tocElement = document.querySelector('.post-toc');
      const tocToggle = tocElement ? tocElement.querySelector('.post-toc__toggle') : null;
      const tocToggleText = tocElement ? tocElement.querySelector('.post-toc__toggle-text') : null;
      const tocTitle = tocElement ? tocElement.querySelector('.post-toc__title') : null;
      const tocLinks = tocElement ? Array.from(tocElement.querySelectorAll('.post-toc__link')) : [];
      const headingEntries = tocLinks
        .map((link) => {
          const id = link.getAttribute('href').slice(1);
          const target = document.getElementById(id);
          return target ? { link, target } : null;
        })
        .filter(Boolean);
      const tocLabels = CURRENT_LANG === 'it'
        ? { title: 'Indice', show: 'Mostra indice', hide: 'Nascondi indice' }
        : { title: 'Table of contents', show: 'Show table of contents', hide: 'Hide table of contents' };
      const tableWrappers = Array.from(document.querySelectorAll('.table-wrapper[data-enhanced-table]'));
      const tableLabels = CURRENT_LANG === 'it'
        ? { expand: 'Apri a schermo intero', close: 'Chiudi' }
        : { expand: 'Open full view', close: 'Close' };
      const codeBlocks = Array.from(document.querySelectorAll('.post-body pre'));
      const codeCopyLabels = {
        it: { copy: 'Copia', copied: 'Copiato!' },
        en: { copy: 'Copy', copied: 'Copied!' },
      };
      let tableOverlay = null;
      let tableOverlayScroll = null;
      let tableOverlayClose = null;
      if (tocTitle) {
        tocTitle.textContent = tocLabels.title;
      }
      if (tocToggleText) {
        tocToggleText.textContent = tocLabels.title;
      }
      let tocCollapsed = false;
      let tocManualOverride = false;
      const tocMediaQuery = window.matchMedia ? window.matchMedia('(max-width: 1024px)') : null;
      function ensureTableOverlay() {
        if (tableOverlay) {
          return;
        }
        tableOverlay = document.createElement('div');
        tableOverlay.className = 'table-overlay';
        tableOverlay.innerHTML =
          '<div class="table-overlay__content">' +
          '<button type="button" class="table-overlay__close">' + tableLabels.close + '</button>' +
          '<div class="table-overlay__scroll"></div>' +
          '</div>';
        body.appendChild(tableOverlay);
        tableOverlayScroll = tableOverlay.querySelector('.table-overlay__scroll');
        tableOverlayClose = tableOverlay.querySelector('.table-overlay__close');
        if (tableOverlayClose) {
          tableOverlayClose.setAttribute('aria-label', tableLabels.close);
          tableOverlayClose.addEventListener('click', closeTableOverlay);
        }
        tableOverlay.addEventListener('click', (event) => {
          if (event.target === tableOverlay) {
            closeTableOverlay();
          }
        });
      }
      function closeTableOverlay() {
        if (!tableOverlay) {
          return;
        }
        tableOverlay.classList.remove('table-overlay--visible');
        body.classList.remove('no-scroll');
        if (tableOverlayScroll) {
          tableOverlayScroll.innerHTML = '';
        }
      }
      function openTableOverlay(wrapper) {
        ensureTableOverlay();
        if (!tableOverlay || !tableOverlayScroll) {
          return;
        }
        tableOverlayScroll.innerHTML = '';
        const table = wrapper.querySelector('table');
        if (table) {
          const clone = table.cloneNode(true);
          const tableSize = table.dataset.tableSize;
          if (tableSize) {
            clone.dataset.tableSize = tableSize;
          }
          tableOverlayScroll.appendChild(clone);
        }
        tableOverlay.classList.add('table-overlay--visible');
        body.classList.add('no-scroll');
        if (tableOverlayClose) {
          tableOverlayClose.focus();
        }
      }
      function enhanceTables() {
        if (!tableWrappers.length) {
          return;
        }
        tableWrappers.forEach((wrapper) => {
          if (wrapper.dataset.enhanced === 'true') {
            return;
          }
          const table = wrapper.querySelector('table');
          if (!table) {
            return;
          }
          const headerCells = table.querySelectorAll('thead th');
          const referenceCells = headerCells.length ? headerCells : table.querySelectorAll('tr:first-child > *');
          const columnCount = referenceCells.length;
          let tableSize = '';
          if (columnCount >= 6) {
            tableSize = 'wide';
          } else if (columnCount >= 4) {
            tableSize = 'medium';
          }
          if (tableSize) {
            wrapper.setAttribute('data-table-size', tableSize);
            table.dataset.tableSize = tableSize;
          }
          const expandBtn = document.createElement('button');
          expandBtn.type = 'button';
          expandBtn.className = 'table-wrapper__expand';
          expandBtn.innerHTML = '<span aria-hidden="true">üîç</span> ' + tableLabels.expand;
          expandBtn.setAttribute('aria-label', tableLabels.expand);
          expandBtn.addEventListener('click', () => openTableOverlay(wrapper));
          wrapper.appendChild(expandBtn);
          wrapper.dataset.enhanced = 'true';
        });
      }
      function fallbackCopy(text) {
        const textarea = document.createElement('textarea');
        textarea.value = text;
        textarea.setAttribute('readonly', '');
        textarea.style.position = 'fixed';
        textarea.style.opacity = '0';
        textarea.style.left = '-9999px';
        document.body.appendChild(textarea);
        textarea.select();
        let successful = false;
        try {
          successful = document.execCommand('copy');
        } catch (error) {
          successful = false;
        }
        textarea.remove();
        return successful;
      }
      function showCopyFeedback(button, labels) {
        if (button._copyTimeout) {
          clearTimeout(button._copyTimeout);
        }
        const labelEl = button.querySelector('.code-copy-btn__text');
        button.classList.add('code-copy-btn--copied');
        if (labelEl) {
          labelEl.textContent = labels.copied;
        }
        button._copyTimeout = window.setTimeout(() => {
          button.classList.remove('code-copy-btn--copied');
          if (labelEl) {
            labelEl.textContent = labels.copy;
          }
        }, 2000);
      }
      function enhanceCodeBlocks() {
        if (!codeBlocks.length) {
          return;
        }
        const labels = codeCopyLabels[CURRENT_LANG] || codeCopyLabels.en;
        codeBlocks.forEach((pre) => {
          if (pre.dataset.copyEnhanced === 'true') {
            return;
          }
          const code = pre.querySelector('code');
          if (!code) {
            return;
          }
          const button = document.createElement('button');
          button.type = 'button';
          button.className = 'code-copy-btn';
          button.setAttribute('aria-label', labels.copy);
          button.innerHTML =
            '<span class="code-copy-btn__icon" aria-hidden="true">üìã</span>' +
            '<span class="code-copy-btn__text">' + labels.copy + '</span>';
          button.addEventListener('click', async () => {
            const text = (code.textContent || '').replace(/s+$/, '');
            if (!text) {
              return;
            }
            let copied = false;
            if (navigator.clipboard && typeof navigator.clipboard.writeText === 'function') {
              try {
                await navigator.clipboard.writeText(text);
                copied = true;
              } catch (error) {
                copied = false;
              }
            }
            if (!copied) {
              copied = fallbackCopy(text);
            }
            if (copied) {
              showCopyFeedback(button, labels);
            }
          });
          pre.appendChild(button);
          pre.dataset.copyEnhanced = 'true';
        });
      }
      function setTocCollapsed(collapsed, { manual = false } = {}) {
        if (!tocElement) {
          return;
        }
        tocCollapsed = Boolean(collapsed);
        if (manual) {
          tocManualOverride = true;
        }
        tocElement.classList.toggle('post-toc--collapsed', tocCollapsed);
        tocElement.setAttribute('data-collapsed', tocCollapsed ? 'true' : 'false');
        if (tocToggle) {
          tocToggle.setAttribute('aria-expanded', tocCollapsed ? 'false' : 'true');
          tocToggle.setAttribute('aria-label', tocCollapsed ? tocLabels.show : tocLabels.hide);
        }
      }
      function initToc() {
        if (!tocElement) {
          return;
        }
        if (tocToggle) {
          tocToggle.addEventListener('click', () => {
            setTocCollapsed(!tocCollapsed, { manual: true });
          });
        }
        if (tocMediaQuery) {
          const handleMediaChange = (event) => {
            if (tocManualOverride) {
              return;
            }
            setTocCollapsed(event.matches);
          };
          if (typeof tocMediaQuery.addEventListener === 'function') {
            tocMediaQuery.addEventListener('change', handleMediaChange);
          } else if (typeof tocMediaQuery.addListener === 'function') {
            tocMediaQuery.addListener(handleMediaChange);
          }
          setTocCollapsed(tocMediaQuery.matches);
        } else {
          setTocCollapsed(false);
        }
      }
      const storedTheme = (localStorage.getItem(BLOG_THEME_KEY) || '').toLowerCase();
      const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
      const initialTheme = storedTheme === 'light' ? 'light' : (storedTheme === 'dark' ? 'dark' : (prefersDark ? 'dark' : 'light'));
      let activeLink = null;
      let ticking = false;
      function applyTheme(theme) {
        const resolved = theme === 'dark' ? 'dark' : 'light';
        body.setAttribute('data-theme', resolved);
        if (themeToggle) {
          themeToggle.classList.toggle('active', resolved === 'dark');
        }
        if (themeThumb) {
          themeThumb.textContent = resolved === 'dark' ? 'üåô' : '‚òÄÔ∏è';
        }
        localStorage.setItem(BLOG_THEME_KEY, resolved);
      }
      function setActive(link) {
        if (activeLink === link) {
          return;
        }
        if (activeLink) {
          activeLink.classList.remove('post-toc__link--active');
        }
        if (link) {
          link.classList.add('post-toc__link--active');
        }
        activeLink = link;
      }
      function updateActiveHeading() {
        if (!headingEntries.length) {
          return;
        }
        const scrollPosition = window.scrollY + 160;
        let current = headingEntries[0];
        for (const item of headingEntries) {
          if (item.target.offsetTop <= scrollPosition) {
            current = item;
          } else {
            break;
          }
        }
        setActive(current.link);
      }
      function onScroll() {
        if (ticking) {
          return;
        }
        ticking = true;
        window.requestAnimationFrame(() => {
          updateActiveHeading();
          ticking = false;
        });
      }
      document.addEventListener('keydown', (event) => {
        if (event.key === 'Escape') {
          closeTableOverlay();
        }
      });
      enhanceTables();
      enhanceCodeBlocks();
      initToc();
      applyTheme(initialTheme);
      if (themeToggle) {
        themeToggle.addEventListener('click', () => {
          applyTheme(body.getAttribute('data-theme') === 'dark' ? 'light' : 'dark');
        });
      }
      if (langBtn) {
        langBtn.textContent = CURRENT_LANG === 'it' ? 'EN' : 'IT';
        if (OTHER_LANG_LINK) {
          langBtn.addEventListener('click', () => {
            localStorage.setItem(BLOG_LANG_KEY, OTHER_LANG);
            window.location.href = OTHER_LANG_LINK;
          });
        } else {
          langBtn.disabled = true;
          langBtn.classList.add('lang-btn--disabled');
        }
      }
      localStorage.setItem(BLOG_LANG_KEY, CURRENT_LANG);
      if (headingEntries.length) {
        headingEntries.sort((a, b) => a.target.offsetTop - b.target.offsetTop);
        updateActiveHeading();
        window.addEventListener('scroll', onScroll, { passive: true });
      }
    })();
  </script>
</body>
</html>