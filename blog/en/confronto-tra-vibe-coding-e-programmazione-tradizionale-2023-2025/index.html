<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Comparison between Vibe Coding and Traditional Programming (2023–2025)</title>
  <style>
    
  :root {
    color-scheme: light dark;
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    line-height: 1.6;
  }
  body {
    margin: 0;
    padding: 0;
    background: var(--bg, #0f172a);
    color: #0f172a;
  }
  @media (prefers-color-scheme: dark) {
    body {
      color: #e2e8f0;
    }
  }
  .page {
    max-width: 960px;
    margin: 0 auto;
    padding: 3rem 1.5rem 4rem;
  }
  header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 2.5rem;
  }
  header nav a {
    margin-left: 1rem;
    text-decoration: none;
    font-weight: 500;
    color: inherit;
  }
  header nav a:hover {
    text-decoration: underline;
  }

    h1 {
      font-size: clamp(2.2rem, 4vw, 3.2rem);
      margin-bottom: 1rem;
    }
    .meta {
      color: #64748b;
      margin-bottom: 2.5rem;
    }
    article {
      font-size: 1.05rem;
    }
    article img {
      max-width: 100%;
      border-radius: 0.75rem;
      margin: 1.5rem 0;
      box-shadow: 0 10px 25px -12px rgba(15, 23, 42, 0.35);
    }
    article pre {
      background: rgba(15, 23, 42, 0.85);
      color: #e2e8f0;
      padding: 1rem 1.25rem;
      overflow-x: auto;
      border-radius: 0.75rem;
    }
    nav {
      margin-bottom: 2rem;
      display: flex;
      gap: 1rem;
    }
    nav a {
      text-decoration: none;
      font-weight: 500;
      color: inherit;
    }
    nav a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="page">
    <nav>
      <a href="../../../blog/index.html">← Blog</a>
      <a href="../../../index.html">Home</a>
      <a href="../../../blog/it/confronto-tra-vibe-coding-e-programmazione-tradizionale-2023-2025/">Leggi in Italiano</a>
    </nav>
    <article>
      <header>
        <h1>Comparison between Vibe Coding and Traditional Programming (2023–2025)</h1>
        <p class="meta">
          Aggiornato il 2025-10-31
        </p>
      </header>
      <h1>Comparison between Vibe Coding and Traditional Programming (2023–2025)</h1>
<h2>Table of Contents</h2>
<ul>
<li><a href="#page-1">Page 1</a></li>
<li><a href="#page-2">Page 2</a></li>
<li><a href="#page-3">Page 3</a></li>
<li><a href="#page-4">Page 4</a></li>
<li><a href="#page-5">Page 5</a></li>
<li><a href="#page-6">Page 6</a></li>
<li><a href="#page-7">Page 7</a></li>
<li><a href="#page-8">Page 8</a></li>
<li><a href="#page-9">Page 9</a></li>
<li><a href="#page-10">Page 10</a></li>
<li><a href="#page-11">Page 11</a></li>
<li><a href="#page-12">Page 12</a></li>
<li><a href="#page-13">Page 13</a></li>
</ul>
<hr>
<h2>Page 1</h2>
<p>Comparison between Vibe Coding and Traditional
Programming (2023–2025)
Abstract
Vibe coding – programming guided entirely by AI – spread between 2023 and 2025
thanks to tools like GitHub Copilot, Cursor, Sourcegraph Cody, Tabnine, Codeium, and AI assistants
integrated into IDEs. This report compares vibe coding with the traditional approach (“old coding”) based on
recent studies, corporate case studies, and real-world experiences. The data shows a nuanced picture: in
controlled corporate environments, AI pair programming can accelerate development (+26% tasks completed on
average) especially for junior developers, without apparent immediate drops in quality.</p>
<p>However, in complex projects, experienced developers encountered unexpected slowdowns (+19%
time with AI) despite the perception of greater speed. On the quality front, AI-generated code
works but tends to have lower maintainability (duplications, doubled code churn) and potential
security flaws if used without supervision. Culturally, AI assistants increase developer satisfaction by
reducing repetitive tasks, but concerns emerge: less experienced programmers risk not developing a deep
understanding of the code, and blind trust in AI (“accept all and go”) can lead to a loss of “craft” and
opaque code. In summary, vibe coding offers unprecedented speed and accessibility, but requires strong
human intervention in architecture, review, and testing to ensure quality, security, and sustainability over
time.</p>
<p>Methodology
For this analysis, sources published between 2023 and 2025 in multiple languages were examined, prioritizing
quantitative and reproducible evidence. Specifically: (1) peer-reviewed experimental studies and technical white papers
with clear methodology (RCT, benchmark) – considered Grade A evidence (high solidity, low
risk of bias); (2) industrial case studies with real metrics on teams (Grade B if conducted internally
with possible context bias); (3) direct testimonies from developers (blogs, videos, forums) that
include concrete experiments or verifiable code (Grade C as anecdotal, medium/high bias risk). We
extracted from each source details on: context and tools used, type of activity performed (e.g.,
new development, refactoring, bugfix, testing), objective metrics (time spent, percentage of bugs
or vulnerabilities, test coverage, performance), and subjective metrics (satisfaction, cognitive load,
perceived learning). During the synthesis, evidence was cross-referenced to highlight
convergences or discrepancies. For example, the results of a large corporate RCT (4,800
developers at Microsoft/Accenture, Grade A evidence) were compared with those of an RCT on experienced OSS
developers (16 open-source maintainers, Grade A evidence), as well as with case studies such as the internal adoption of Copilot
in a company (ZoomInfo, 400 engineers, Grade B evidence). Individual experiences (e.g.,
full-stack prototype developed entirely with AI) were included as Grade C evidence to illuminate
practical and cultural aspects difficult to detect from numbers alone. All sources are cited in Harvard format in
square brackets (e.g., ) with links to original references, and a complete list is provided at the end.</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 2</h2>
<p>Technical Results
Impact on productivity and development speed
Data shows that AI assistants can accelerate software development, but with important
distinctions for context and programmer experience. A large experimental study (3 RCT trials at
Microsoft, Accenture, and another multinational) found a +26% increase in tasks completed on average by
developers with access to GitHub Copilot. In practice, developers with AI complete ~26% more
features/bugs than the traditional group, with also a 13.5% increase in the number of
weekly commits and a 38% increase in compilation frequency (they iterate faster).</p>
<p>Importantly, the study found no deterioration in code quality or more bugs in the AI group –
“no negative impact on quality observed” – suggesting that the extra speed does not
come at the expense of correct functioning (Grade A evidence, low bias). Another positive indicator comes
from a controlled trial at Accenture: with Copilot, an +8.7% increase in pull requests per developer and
a +15% merge rate (more accepted PRs) was observed, along with an +84% increase in successful first-time builds.</p>
<p>This last piece of data is particularly interesting: AI could help write more consistent code
that compiles and passes initial tests, reducing syntax or integration errors (perhaps because the assistant
suggests code that is already correct in many details). Furthermore, the average time to open the first PR on a
project dropped from ~9.6 days to ~2.4 days (–75%) with AI according to DevOps research, indicating a
faster start to new tasks.</p>
<p>However, these benefits are not uniformly distributed. The largest increases concern repetitive or
boilerplate tasks (e.g., writing standard CRUD code, unit tests, generating integration code)
where Copilot performs best – in such activities, up to +34–38% speed was measured. Conversely,
for complex or creative tasks (e.g., development of non-standard algorithms, delicate business logic) the
gain is more modest or null, requiring significant manual intervention and verification of logical
correctness. A field experiment with experienced open-source developers (16
maintainers of mature projects 1M+ LOC) even inverted the paradigm: in that context, the use of
advanced AI (Cursor with Claude 3.5/3.7 models) slowed down times by 19% on average compared to
manual coding. Participants, with ~5 years of experience on their project, estimated that
AI would accelerate them by ~24%, but the data showed an opposite effect (+19% duration). The
Figure 1 illustrates this gap: while both external experts and developers predicted substantial speed-
ups (negative green points), the actual result was a significant slowdown (red point at +19%
time). The causes? Researchers identified various friction factors: time spent writing
prompts, reading and correcting suggestions, adapting the generated code to the complex
existing context. In a large project with rigorous standards, AI ended up producing partial
or misaligned solutions, forcing the developer to make additional iterations to integrate them. In
essence, the initial advantages of “typing less code” were offset (and exceeded) by the cognitive
and temporal overload of guiding the AI and verifying its output. This result (Grade A evidence, low bias)
does not indicate that AI slows down everywhere, but highlights that context is crucial: on large codebases and
complex activities, the current generation of tools still requires a lot of human intervention, to the point of
nullifying speed gains in some cases. Researchers note that with better techniques
(prompting, more evolved AI agents) or for different tasks, the balance could shift in the future.</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 3</h2>
<p>Figure 1: Results of an RCT study (METR 2025) with 16 experienced open-source developers, comparing the
predictions of experts and devs (green = expected speed-up) with the real effect of AI (red = slowdown +19% time).
AI in this context made tasks slower, in contrast to widespread perception.</p>
<p>A key element that emerged in multiple studies is that less experienced programmers derive greater
immediate benefit from AI compared to senior colleagues. In the corporate trial on 4,800 devs, junior
developers and those with fewer years of experience saw productivity increases from 21% up to
~40%, significantly higher than the average. This is explained by greater adoption: juniors
tend to rely more easily on AI (high adoption rate) and trust suggestions, filling
technical gaps with the help of the model. For example, Copilot can immediately suggest
&quot;ready-made&quot; solutions for API problems or structures that a novice would take a long time to find
on their own. Senior developers, on the other hand, already very productive autonomously, see fewer
percentage improvements and often use AI more critically (accepting fewer suggestions: ~15–30%
acceptance rate depending on use). The subjective perception remains positive for almost
everyone: in the open-source trial despite the measured slowdown, developers continued to
believe they had worked faster with AI (estimating a 20% personal speed-up).</p>
<p>This perceptual gap is attributed to the fact that AI &quot;makes one feel&quot; productive – less code is written
manually – but micro-slowdowns (e.g., generation wait, additional debugging) accumulate
without being consciously attributed to the tool. In general, however, the human+AI combination
tends to outperform human alone in many standard scenarios, especially over time.</p>
<p>For example, Microsoft research shows that it takes ~11 weeks of continuous use for a
team to reach full benefit from Copilot (there is a learning curve at the beginning). Many
companies report that once integrated into the workflow, AI coding becomes a fundamental part
of daily development (67% of developers with Copilot use it 5 days a week). It can therefore
be said that vibe coding primarily accelerates prototyping and routine coding phases, while
traditional programming remains competitive (and sometimes more efficient) in the architectural
design, complex algorithms, and deep integration into large-scale systems phases.</p>
<p>Code quality, maintainability, and reusability
Regarding the quality of the code produced, the picture is mixed: the immediate functionality of
AI-generated code is often good, but maintainability problems emerge in the medium term
without disciplined programmer intervention. In terms of out-of-the-box functional correctness,
several studies indicate that AI-assisted code can be just as valid as hand-written code. For</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 4</h2>
<p>example, GitHub reported that code created with Copilot is “significantly more functional,
readable, and reliable” than control code in their 2022 study. In the large 2025 RCT cited
earlier, no significant differences in bugs or malfunctions were observed: AI teams
maintained the same apparent quality standards (passed tests, accepted reviews) as
traditional teams. In fact, as mentioned, in an enterprise environment, AI assistance led to
fewer initial failed builds, suggesting a positive impact on the syntactic/
compilation correctness of the code. This reflects the fact that AI is very good at producing &quot;working&quot; code in
typical cases – for example, it immediately writes the function with the correct syntax and correct API calls,
avoiding careless errors. It also reduces the burden on activities like writing tests: some developers use it to
generate batteries of unit tests, increasing coverage (it&#39;s no coincidence that Copilot increased the speed of writing tests by 38%
in one case study). So far, then, &quot;external&quot; quality (functionality and short-
term quality) appears preserved or even improved in terms of fewer initial errors.</p>
<p>However, analyzing internal quality and maintainability, worrying trends related to
the massive use of AI emerge. An independent white paper (Coding on Copilot, GitClear 2024) analyzed
millions of lines of code on GitHub to measure its evolution after the introduction of Copilot. The result:
code churn – the percentage of code that is modified or removed within two weeks of
its creation – is sharply increasing. In particular, churn was relatively stable until 2021,
while it is estimated to double by 2024 compared to the pre-AI 2021 baseline. This indicates that
twice as much code is discarded shortly after being written, a sign that much rapid AI production
is later reworked or discarded (often for corrections or refactoring). Contextually,
GitClear notes an increase in &quot;added&quot; and &quot;copied/pasted&quot; code compared to reorganized
or deleted code. In other words, developers with AI tend to add new parts of
code (perhaps suggested) instead of reusing or modifying existing ones, generating
duplications. This translates into violations of the DRY (“Don’t Repeat Yourself”) principle and fragmentation
of the codebase. This very phenomenon was experienced in the field by those who tried pure vibe
coding: &quot;spaghetti&quot; or inconsistent code. One programmer recounts his experience
reviewing an app created entirely via vibe coding: “I was shocked at how bad the
code was in many places. It looked like it was written by many junior developers, each with different practices.”
In that app, functions were duplicated in many files and there was no consistency in style, making it difficult
even to track bugs; in the end, the developer decided to discard everything and redesign the architecture
manually before reusing AI, to avoid such chaos. Even an anecdotal experiment with Replit
and Cursor reported similar problems: a prototype created in 20 minutes (React frontend + Express backend</p>
<ul>
<li>PostgreSQL/Pinecone DB) worked, but over a third of the code was duplicated, a single
component had grown 9 times beyond the recommended size causing crashes, and the same
text normalization function appeared in 15 different files. These are not single bugs, but
symptoms of how AI builds systems. Instead of designing reusable abstractions, the model
generates isolated blocks on demand: fast, but lacking an overall vision. Indeed, “AI delivers speed –
but not modularity,” notes the author, explaining that AI operates on local pattern matching, it does not have
a global understanding of the architecture. Therefore, it can solve every micro-problem well, but
it does not realize it is introducing redundancies or disorganization at the system level. An interesting
indicator is the decrease in &quot;moved&quot; code identified by the GitClear report: less
refactoring and reorganization means that the generated code remains as is, even if perhaps
it should be refactored – with the result of having many similar pieces in different places. All this
accumulates technical debt: “AI-generated code often lacks the structure, documentation, and clarity
necessary for long-term maintenance. […] The speed of vibe coding comes at the cost of a codebase
difficult to read, fix, or extend in the future.”</li>
</ul>
<p>In summary, vibe coding tends to produce more code, faster, but with less propensity
for reuse and cleanliness. While an experienced human developer, writing by hand, often reflects on
how to integrate new functionality into the existing design (perhaps refactoring related code parts),</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 5</h2>
<p>an AI on prompt generates a solution ex novo, unaware of possible duplications or similar solutions already
in the project. This is why we see AI-driven applications consuming 3–4 times the server resources
compared to hand-written equivalents: they include more redundant code, fewer
optimizations, repeated calls where unification was possible. It is indicative that in some vibe
coding hackathons, builds of tens of thousands of lines are achieved in a few hours – code that &quot;runs&quot; but is far from
what an experienced team would have produced in terms of cleanliness. Therefore,
traditional programming maintains an advantage in consistency and maintainability, thanks to
stable architectural patterns, better performance, and full understanding of the code by the team. In the long run,
these qualities become vital: a system created with design attention will better withstand
evolution than one put together &quot;by vibes&quot; in a few days. Emerging best practices
suggest a hybrid approach: use AI for rapid prototyping, boilerplate generation, and
testing, but rely on traditional coding (or at least thorough human review) to define
modular architecture, clean up duplications, and enforce consistent style standards.</p>
<p>Errors and security vulnerabilities
A crucial aspect to compare is the incidence of bugs and vulnerabilities in AI-driven code versus
traditional code. The key question: does AI coding lead to more errors or security problems? The
evidence indicates that AI is adept at avoiding syntactic/compilation errors, but can introduce
logical bugs or subtle security vulnerabilities if not carefully controlled. In terms of functional bugs,
controlled studies so far have not reported significantly higher bug rates attributable to AI –
indeed, as seen, initially build errors decrease and tests pass more easily thanks to AI.</p>
<p>However, several experiments show that blindly relying on AI can induce errors that an
experienced human would avoid. For example, models like GPT-4 sometimes tend to &quot;invent&quot; non-existent
functions or calls if the prompt is ambiguous, or to provide plausible but incorrect solutions for
edge cases. A study (Microsoft 2022) found cases where Copilot suggested misleading solutions to
algorithmic problems, solving the base case but failing in boundary situations – errors that a
careful developer would have discovered by writing tests. For this reason, the literature recommends never
letting &quot;AI fly alone&quot;: “Copilot is a powerful tool; however, it should not fly the plane
alone.” The programmer must remain in the loop to verify deep logical correctness. To
confirm, a study on the use of ChatGPT for university programming tasks notes that
inexperienced students tended to trust the output even when it contained errors: if the generated
program “didn’t work, they simply asked the AI to fix it again” without
understanding the real reason. This can create cycles where a bug is &quot;corrected&quot; by AI with
another potentially wrong patch, leading to hidden errors that are difficult to diagnose.</p>
<p>The software security front deserves special attention. Initial research suggests that AI-generated
code often includes known vulnerabilities or insecure practices, if the user does not actively
guide the model towards secure solutions. One of the first studies on the subject (Pearce et al., 2022)
evaluated Copilot on 89 known security scenarios (CWE). The result was that 39% of the generated
programs contained vulnerabilities – for Copilot&#39;s top suggestions, the rate was ~39% in Python and ~50%
in C. In total, out of 1689 programs generated with Copilot, over 40% had at least one weakness
(buffer overflow, SQL injection, hardcoded secrets, etc.). This data (Grade A evidence, low bias)
reveals that without any filtering, AI tends to replicate patterns seen in training data even if insecure. For
example, it might suggest an SQL query built via string concatenation (vulnerable to
injection) because it has often seen such code on GitHub, not knowing that it is a dangerous practice. An
industry report generally found that code produced by models is “full of security flaws and
easily attackable,” defining the result as “insecure spaghetti” if used without controls. An audit
cited by Trickle AI revealed that up to 19% of AI-suggested code snippets presented
known vulnerabilities or potential exploits – Grade B evidence (aggregated corporate data). In addition, the
casual approach of vibe coding means that essential security steps are often skipped: for example,</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 6</h2>
<p>input validation, external data sanitization, robust error handling, and
authentication/permission checks. The model does not &quot;understand&quot; the need for these checks unless
explicitly requested in the prompt; if the human user does not think to ask for them, the generated
code may be superficial from this point of view. A common case: hard-coded credentials or
API keys left in the code – AI might insert them (perhaps drawing from public examples) and the
inexperienced developer might not realize it. It should be noted that AI assistants are improving: Copilot
now tries to recognize and warn about known insecure patterns, and tools like CodeQL can be
integrated for automatic post-generation scans. Furthermore, subsequent studies have tested the use of
Copilot Chat to solve security problems: by providing Copilot with static scanner warnings,
the model is sometimes able to propose fixes, reducing vulnerabilities. So, paradoxically, AI
can introduce flaws but can also help identify and correct them if appropriately directed.</p>
<p>An interesting phenomenon that emerged from a 2025 study is the “iterative degradation” of
security: when a developer makes AI iterate multiple times on a piece of code to improve it, perhaps
alternating efficiency prompts and feature additions, new unexpected vulnerabilities
can appear. In the test, after 5 iterations of requested improvements to GPT, critical vulnerabilities in the code
increased by +37.6%. This challenges the assumption that iterating with AI automatically refines code:
without careful human supervision, each automatic &quot;improvement&quot; can introduce a new security
problem elsewhere. This reinforces the recommendation that human security expertise remains in
the loop.</p>
<p>Ultimately, compared to the traditional approach, AI-only coding presents greater risks of
latent vulnerabilities if used naively. With traditional coding, an experienced developer
follows security procedures (code review, OWASP checklist, pen testing) and has the context to know
where risks might lurk. A generative AI does not have the moral or causal notion of &quot;security,&quot;
so it can violate basic principles without alarm. On the other hand, AI can act as a catalyst if
used synergistically: speed up the implementation of known fixes, suggest secure patterns once the team
defines them, and lighten the load on details, leaving critical review to developers. The best practice
that has emerged is to treat AI as a very fast but potentially naive junior developer.</p>
<p>That is: never trust blindly, always peer review generated code, run security scanners
(e.g., OWASP ZAP, Snyk), keep keys and sensitive data out of reach, and provide feedback to AI
 (via prompts or manual corrections) to keep it on the right track. In critical contexts (e.g.,
financial modules, healthcare, national security), many companies choose a hybrid or conservative
approach: use AI for non-critical parts or as a suggester, but hand-write core parts where security
errors are not an option. This prudence reflects a fundamental principle: the ultimate
responsibility for code is human. As one manager notes, when AI generates code, the
“traditional sense of ownership fades” and there is a risk that engineers overlook flaws they
would have noticed if they had written it themselves. Maintaining a high sense of responsibility and human
control is essential to combine AI speed and security.</p>
<p>Performance and scalability of applications
A final technical comparison concerns the runtime performance and scalability of systems created
primarily via AI compared to those traditionally designed. Evidence indicates that vibe
coding often prioritizes the simplest working solution, neglecting optimizations that
experienced developers would implement for scalability. For example, a typical scenario: AI generates a
function that processes data in memory sub-optimally. With small test data it &quot;works,&quot; but in
production it could become a bottleneck. As reported, an AI assistant had created DB access logic
that did not scale – unindexed and repeated queries – suitable for the prototype but not for
real loads. Another cited case: generation of a 39 MB JSON blob on the client side, which
obviously crashes the browser – AI did not &quot;understand&quot; that this approach was not scalable,</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 7</h2>
<p>while the human developer recognized it immediately. In general, architectures
created &quot;by improvising&quot; with AI tend to be monolithic and not very modular. The model does not
impose separation into services, does not think about asynchronous queues, caching, streaming, etc., unless
the user explicitly instructs it. This means that a project born via vibe coding can
work well for a limited number of users or requests, but under high load it shows
serious inefficiencies, often requiring redesign. An observed effect is that the time
initially saved could be lost later in scaling the application: “code created
‘winging it’ with AI is fine for a quick demo, but that same code might not hold up in production
under load, requiring massive refactoring afterwards.” Developing in a traditional way,
although slower, allows for performance requirements to be taken into account as you go (e.g., choosing
efficient data structures, avoiding redundant computations). In pure vibe coding, these aspects
only emerge when the app &quot;breaks,&quot; leading to reactive adjustments instead of a
proactive design.</p>
<p>That said, it must be acknowledged that AI can also help optimize if used well: for example, it can
suggest vectorization of a loop or indicate the complexity of an algorithm if queried. Furthermore, with
the maturation of models and fine-tuning for performance, it is plausible that in the future, assistants
themselves will highlight bottlenecks (e.g., &quot;this function has O(n^2) complexity, you could use
an index to improve&quot;). But in the 2023-2025 period, AI does not have this innate awareness. Therefore,
mission-critical or high-scale projects have largely remained dominated by traditional approaches or
at least by strong intervention of human engineers in the performance hardening phase. For example,
in the corporate feedback collected, it is suggested not to entrust performance-sensitive parts to AI
(intensive algorithms, memory management, real-time components) without robust manual tuning.</p>
<p>Here too, the winning model is hybrid: AI quickly creates a functional base, then experienced
developers profile and optimize where needed, as they would on code written by juniors.</p>
<p>Summarizing the technical results: vibe coding revolutionizes writing speed and convenience (less
manual effort on many tasks), but does not guarantee &quot;structural&quot; quality of the final product. AI-driven
code tends to be more verbose and redundant, subsequently requiring a greater cleaning/
optimization phase compared to code designed from the start with a traditional method. With proper
control practices (review, refactoring, intensive testing), well-organized teams can mitigate these
defects and leverage the best of both worlds – but without such controls, the risk is to get &quot;a castle
built quickly on weak foundations.&quot; As we will see, this also has implications for the
cultural and organizational aspects of software development.</p>
<p>Cultural and Team Results
Developer satisfaction and adoption of AI tools
The introduction of AI coding assistants has had a significant impact on developer morale and
satisfaction. Numerous surveys and internal studies indicate that the majority of
developers appreciate AI as a pair programmer and would not want to go back. For example, in
Accenture&#39;s research, 90% of developers reported being more satisfied with their work using
Copilot, and as many as 95% said they enjoy coding more with AI assistance. This
is due to the fact that AI removes many tedious or frustrating parts: 70% reported less
mental effort in repetitive tasks, and over half reduced the time spent searching for information on
Stack Overflow or documentation. In practice, by being able to delegate boilerplate writing or
syntax lookup to AI, developers focus on more creative or interesting aspects – which improves
engagement. A Microsoft survey of early Copilot adopters (2023) found that 77% no
longer want to do without it after trying it, and ~70% report an increase in perceived productivity and</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 8</h2>
<p>quality of their work. This data shows great enthusiasm: many see AI as a
power-up that makes them more effective and frees them from the most mundane tasks.</p>
<p>At the level of corporate adoption, the trend is also strong: by 2025, over 80% of developers
regularly use AI assistants, and Copilot in particular has been adopted by 90% of Fortune 100
companies. This happened not only out of &quot;fashion,&quot; but driven by positive feedback in the
field. For example, ZoomInfo – a tech company with ~400 engineers – conducted an internal pilot: Copilot
contributed hundreds of thousands of lines of code to their codebase, with an estimated ~20% of
time saved on coding activities. Furthermore, internal surveys at ZoomInfo showed
about 72% positive sentiment towards AI. Engineers particularly appreciated the speed
in generating utility functions and boilerplate, while recognizing the need to dedicate time to reviewing
the output to ensure its correctness and consistency. This highlights a point: developers
recognize the limitations of AI (lack of specific domain knowledge, variable quality), but are
happy to use it as a fast helper. In general, adoption in enterprise teams has been accompanied by
training and onboarding to learn how to use the tool well: teams that deeply integrate AI into
workflows report the greatest benefits and satisfaction, creating a virtuous cycle (the more you use it, the more you
like using it).</p>
<p>That said, there are also critical voices and cultural resistances to the indiscriminate use of AI. Some
developers – especially the more senior or purists – express frustration with the excessive emphasis
on AI. For example, forum discussions indicate that part of the community fears that IDEs are
becoming overloaded with AI functionalities at the expense of basic stability (&quot;I don&#39;t want your AI, I want
an IDE that works well&quot; is a refrain that has appeared online). JetBrains faced criticism when its AI
Assistant, launched in 2023, received negative reviews (2.3/5 rating) due to bugs and perceived
hindrances in the traditional workflow. This suggests that not all developers find
AI useful in the same way: those who work very methodically and deeply on complex problems
may see AI as a &quot;distractor&quot; or even distrust its solutions. Some fear losing control over the code:
if AI writes most of it, the developer feels they no longer &quot;own&quot; the complete understanding. This can
generate professional anxiety, especially in engineers who care about the craftsmanship of their work.
However, the general trend seems to be acceptance: as tools improve and developers learn to
guide them, even traditionalists begin to integrate them into their processes, perhaps limiting
themselves to the features they find most useful (e.g., short auto-completions, code summaries, etc.).</p>
<p>In the open-source domain, a heated cultural debate concerns the use of AI to contribute to projects. On
one hand, GitHub made Copilot free for OS maintainers to encourage its use, convinced that it increases
productivity. And indeed, a study (Bakal et al. 2025) on open-source repositories found an
increase in the volume of contributions after the introduction of Copilot (developers who previously made
few PRs now make more). On the other hand, some maintainers have expressed concern about
AI-generated PRs potentially of low quality: feature requests or fixes generated via
ChatGPT without sufficient reflection. There has even been an appeal from maintainers to have the option
on GitHub to block AI-generated contributions if they are not compliant. This denotes a tension between
quantity (many rapid AI contributions) and quality/controllability in collaborative communities. Some
projects have started requiring contributors to declare if they used AI to generate the
code, somewhat as an indication for reviewers to pay extra attention. Culturally, therefore, if in
companies AI is encouraged by management (also because it is seen as increasing productivity and
reducing costs), in open-source communities and generally among developers, the need for
new norms and guidelines on how to correctly use these tools without compromising
standards emerges.</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 9</h2>
<p>Training, skills, and impact on roles (junior vs. senior)
Perhaps the most discussed cultural issue is: how does vibe coding affect the learning and
skills of programmers? Is there a risk of having developers who only know how to &quot;prompt&quot; AI without
really understanding the code? From various reports, the risk exists, but it is mitigable. A viral post – “New
Junior Developers Can’t Actually Code” (Goel, 2025) – argues that a generation of
new devs is emerging who write code faster than ever thanks to AI, but without really understanding what
they are doing. The author, a tech lead, tells of young hires who have Copilot/ChatGPT
always active and who deliver seemingly functional features; but to questions like “Why does your
code do that? Considered edge cases?” he gets “blank stares.” He observes that the basic
knowledge that was once built by &quot;struggling&quot; with problems is missing: many cannot explain
possible alternatives or the reasoning behind the code, because they limited themselves to accepting the
solution suggested by AI. This leads to superficial understanding and dependence on AI for debugging: if
something goes wrong, instead of introspection, they ask AI again to solve it. In educational contexts,
university professors confirm similar phenomena: students who, with ChatGPT, submit seemingly
correct assignments but cannot answer simple questions about why the code is the way it is (a symptom
that they did not write it themselves or understand it deeply). A discussion on HackerNews
compares this to the &quot;StackOverflow copy-pasters&quot; of a few years ago: it&#39;s a known problem, but AI scales it
to a new level.</p>
<p>However, it should be noted that AI can also be a powerful educational tool if used judiciously. For
example, a junior can ask AI to explain a piece of code, provide examples, or suggest
how to solve a bug, getting immediate and adaptive answers (something StackOverflow did not do).</p>
<p>Some developers report having filled gaps faster thanks to AI: instead of searching on
Google, they received instant mentoring. The crucial difference is the user&#39;s attitude: if the
programmer uses AI in learning mode – asking &quot;why,&quot; actively verifying
answers, trying variants – they can learn a lot in a short time. Conversely, if they use AI in
autopilot mode, merely pasting solutions, they will learn very little. In the community, the advice for
juniors is emerging: “use Copilot/ChatGPT, but with discipline: question the solutions,
study the whys, try to reconstruct the reasoning.” Some educational paths are beginning to integrate
AI not as a shortcut to do less, but as a tool to do more and better: for example,
rapidly generating different approaches to a problem and then analyzing them with the teacher. In
corporate contexts, senior mentors are called upon to guide juniors on how to use AI responsibly – to
avoid skill atrophy. This includes conducting pair reviews where the senior asks the junior to explain the
generated code, or setting policies where the junior must try to implement something themselves before
asking AI, in order to train human problem-solving.</p>
<p>An important role change is observed for senior developers and architects. With vibe
coding, their experience is more necessary than ever, but the focus shifts: less code written line by
line, more review, orchestration, and big picture. As highlighted by Simon Greenman&#39;s case,
effectively using AI tools requires what AI lacks: “architectural vision,
scale awareness, debugging instinct, perceptive judgment.” Senior devs must intervene in
deciding appropriate data structures, anticipating bottlenecks, seeing beyond &quot;it works locally&quot;
to understand if it will hold up in production. In practice, the senior becomes an orchestra conductor:
they let AI play the easy parts, but keep the baton on timing and overall harmony. This can increase the
senior&#39;s leverage: an experienced developer with AI can undertake larger or multiple projects, because
they delegate low-level manual work to the machine and intervene only where their expertise is needed.</p>
<p>Some teams report precisely this effect: the best results with AI are obtained when the
most experienced developers embrace it and integrate it into their process. Conversely, the less
experienced without guidance risk getting lost if something goes wrong: “AI lowers the barriers to entry but raises
the bar for mastery.” Those with little experience can now build a full-stack demo app in</p>
<p><img src="Assets/image_001.png" alt="Immagine"></p>
<hr>
<h2>Page 10</h2>
<p>a few</p>

    </article>
  </div>
</body>
</html>