<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GeoAI Stack: Where to start to become a GeoAI engineer? | Mirko Calcaterra</title>
  <meta name="description" content="GeoAI Stack: Where to start to become a GeoAI engineer? Second installment of our long journey to become a GeoAI engineer. Last time we described very generally what an AI engineer does and what differentiates them. Now let&#39;s understand what an aspiring GeoAI‚Ä¶">
  <meta name="author" content="Mirko Calcaterra">
  <link rel="canonical" href="https://rkomi98.github.io/MyBlog/blog/en/geoai-startingpoint/">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9EVQ8G9W48"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9EVQ8G9W48');
  </script>

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://rkomi98.github.io/MyBlog/blog/en/geoai-startingpoint/">
  <meta property="og:title" content="GeoAI Stack: Where to start to become a GeoAI engineer?">
  <meta property="og:description" content="GeoAI Stack: Where to start to become a GeoAI engineer? Second installment of our long journey to become a GeoAI engineer. Last time we described very generally what an AI engineer does and what differentiates them. Now let&#39;s understand what an aspiring GeoAI‚Ä¶">
  <meta property="og:image" content="https://rkomi98.github.io/MyBlog/Assets/Logo.png">
  <meta property="article:published_time" content="2026-01-08T00:00:00.000Z">
  <meta property="article:author" content="Mirko Calcaterra">
  <meta property="article:section" content="AI Engineering Path">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:title" content="GeoAI Stack: Where to start to become a GeoAI engineer?">
  <meta property="twitter:description" content="GeoAI Stack: Where to start to become a GeoAI engineer? Second installment of our long journey to become a GeoAI engineer. Last time we described very generally what an AI engineer does and what differentiates them. Now let&#39;s understand what an aspiring GeoAI‚Ä¶">
  <meta property="twitter:image" content="https://rkomi98.github.io/MyBlog/Assets/Logo.png">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "GeoAI Stack: Where to start to become a GeoAI engineer?",
    "image": "https://rkomi98.github.io/MyBlog/Assets/Logo.png",
    "datePublished": "2026-01-08T00:00:00.000Z",
    "dateModified": "2026-01-08T09:02:50.465Z",
    "author": {
      "@type": "Person",
      "name": "Mirko Calcaterra",
      "url": "https://rkomi98.github.io/MyBlog/"
    },
    "publisher": {
      "@type": "Person",
      "name": "Mirko Calcaterra"
    },
    "description": "GeoAI Stack: Where to start to become a GeoAI engineer? Second installment of our long journey to become a GeoAI engineer. Last time we described very generally what an AI engineer does and what differentiates them. Now let&#39;s understand what an aspiring GeoAI‚Ä¶"
  }
  </script>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    * {
      box-sizing: border-box;
    }
    html {
      scroll-behavior: smooth;
    }
    body {
      margin: 0;
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      background: radial-gradient(120% 120% at 50% 0%, rgba(59, 130, 246, 0.18) 0%, transparent 65%), var(--bg-primary);
      color: var(--text-primary);
      transition: background 0.3s ease, color 0.3s ease;
      --bg-primary: #0f172a;
      --bg-secondary: #111c33;
      --bg-card: rgba(15, 23, 42, 0.78);
      --bg-card-strong: rgba(15, 23, 42, 0.9);
      --border: rgba(148, 163, 184, 0.24);
      --text-primary: #e2e8f0;
      --text-secondary: #cbd5f5;
      --text-muted: #94a3b8;
      --accent: #60a5fa;
      --accent-strong: #38bdf8;
      --shadow-lg: 0 28px 60px -36px rgba(15, 23, 42, 0.9);
      --code-inline-bg: rgba(6, 11, 19, 0.92);
      --code-block-bg: #050912;
      --code-border: rgba(148, 163, 184, 0.35);
      --code-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.12);
      --code-text: #f8fafc;
    }
    body[data-theme="light"] {
      --bg-primary: #f8fafc;
      --bg-secondary: #ffffff;
      --bg-card: rgba(255, 255, 255, 0.96);
      --bg-card-strong: rgba(248, 250, 252, 0.98);
      --border: rgba(148, 163, 184, 0.18);
      --text-primary: #0f172a;
      --text-secondary: #334155;
      --text-muted: #64748b;
      --accent: #2563eb;
      --accent-strong: #1d4ed8;
      --shadow-lg: 0 28px 50px -38px rgba(15, 23, 42, 0.18);
      background: radial-gradient(120% 120% at 50% 0%, rgba(59, 130, 246, 0.12) 0%, transparent 60%), var(--bg-primary);
    }
    body[data-theme="light"] .post-toc {
      background: rgba(255, 255, 255, 0.96);
    }
    body[data-theme="light"] .post-body {
      background: rgba(255, 255, 255, 0.96);
      color: var(--text-secondary);
    }
    body[data-theme="light"] .post-hero__category {
      background: rgba(37, 99, 235, 0.12);
      color: var(--accent-strong);
    }
    body[data-theme="light"] .post-body blockquote {
      background: rgba(37, 99, 235, 0.1);
      color: var(--text-primary);
    }
    a {
      color: inherit;
      text-decoration: none;
    }
    header.site-header {
      position: sticky;
      top: 0;
      z-index: 12;
      backdrop-filter: blur(14px);
      background: rgba(15, 23, 42, 0.85);
      border-bottom: 1px solid var(--border);
      transition: background 0.3s ease;
    }
    body[data-theme="light"] header.site-header {
      background: rgba(248, 250, 252, 0.9);
    }
    .site-header__inner {
      max-width: 1200px;
      margin: 0 auto;
      padding: 1.15rem clamp(1.5rem, 3vw, 3rem);
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 1.5rem;
    }
    .site-header__left {
      display: flex;
      align-items: center;
      gap: 1.75rem;
    }
    .logo {
      display: inline-flex;
      align-items: center;
      gap: 0.7rem;
      font-weight: 600;
      color: var(--text-primary);
      font-size: 1.05rem;
      letter-spacing: 0.01em;
    }
    .logo-img {
      width: 38px;
      height: 38px;
      border-radius: 12px;
      object-fit: cover;
      box-shadow: 0 8px 18px -12px rgba(15, 23, 42, 0.6);
    }
    .site-nav {
      display: flex;
      gap: 1.1rem;
      font-size: 0.95rem;
      font-weight: 500;
      color: var(--text-muted);
    }
    .site-nav a:hover {
      color: var(--accent);
    }
    .header-controls {
      display: flex;
      align-items: center;
      gap: 0.75rem;
    }
    .lang-btn {
      border: 1px solid var(--border);
      background: var(--bg-card);
      color: var(--text-primary);
      padding: 0.45rem 0.9rem;
      border-radius: 12px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, border 0.2s ease, transform 0.2s ease;
    }
    .lang-btn:hover:not(.lang-btn--disabled) {
      background: var(--accent);
      color: #ffffff;
      transform: translateY(-1px);
      border-color: transparent;
    }
    .lang-btn--disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }
    .theme-toggle {
      position: relative;
      width: 52px;
      height: 28px;
      border-radius: 999px;
      border: 1px solid var(--border);
      background: var(--bg-card);
      cursor: pointer;
      padding: 0;
      transition: background 0.3s ease, border 0.3s ease;
      display: flex;
      align-items: center;
    }
    .theme-toggle .theme-thumb {
      position: absolute;
      top: 50%;
      left: 4px;
      transform: translateY(-50%);
      width: 22px;
      height: 22px;
      border-radius: 50%;
      background: #ffffff;
      color: #1f2937;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.9rem;
      transition: transform 0.3s ease, background 0.3s ease, color 0.3s ease;
      box-shadow: 0 6px 18px -8px rgba(15, 23, 42, 0.6);
    }
    body[data-theme="dark"] .theme-toggle .theme-thumb {
      transform: translate(20px, -50%);
      background: #1f2937;
      color: #f8fafc;
    }
    body[data-theme="dark"] .theme-toggle {
      background: rgba(37, 99, 235, 0.2);
      border-color: rgba(37, 99, 235, 0.3);
    }
    main.page {
      max-width: 1200px;
      margin: 0 auto;
      padding: 3.5rem clamp(1.5rem, 3vw, 3rem) 4.5rem;
    }
    .post-hero {
      position: relative;
      overflow: hidden;
      background: linear-gradient(135deg, rgba(99, 102, 241, 0.22) 0%, rgba(14, 165, 233, 0.08) 60%), var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 28px;
      padding: 2.75rem;
      box-shadow: var(--shadow-lg);
      margin-bottom: 3rem;
    }
    .post-hero::after {
      content: '';
      position: absolute;
      inset: 0;
      background: radial-gradient(circle at 20% 20%, rgba(59, 130, 246, 0.22) 0%, transparent 55%);
      pointer-events: none;
    }
    .post-hero__icon {
      position: relative;
      font-size: 3.1rem;
      margin-bottom: 1.5rem;
      display: inline-flex;
      align-items: center;
      justify-content: center;
    }
    .post-hero__category {
      position: relative;
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 0.4rem 1rem;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.35);
      color: #ffffff;
      font-weight: 600;
      letter-spacing: 0.02em;
      margin-bottom: 1.25rem;
      text-transform: uppercase;
      font-size: 0.8rem;
    }
    .post-hero__title {
      position: relative;
      margin: 0 0 1.25rem;
      font-size: clamp(2.4rem, 4vw, 3.2rem);
      letter-spacing: -0.025em;
      line-height: 1.2;
      color: var(--text-primary);
    }
    .post-hero__meta {
      position: relative;
      display: flex;
      flex-wrap: wrap;
      gap: 1.25rem;
      color: var(--text-muted);
      font-size: 0.95rem;
      font-weight: 500;
    }
    .post-hero__meta span {
      display: inline-flex;
      align-items: center;
      gap: 0.45rem;
    }
    .post-layout {
      display: grid;
      grid-template-columns: minmax(220px, 300px) minmax(0, 1fr);
      gap: 2.75rem;
      align-items: flex-start;
    }
    .post-layout--single {
      grid-template-columns: minmax(0, 1fr);
    }
    .post-toc {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 22px;
      padding: 1.5rem 1.6rem 1.8rem;
      box-shadow: var(--shadow-lg);
      position: sticky;
      top: 120px;
      max-height: calc(100vh - 160px);
      overflow: hidden;
      display: flex;
      flex-direction: column;
    }
    .post-toc__header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 0.75rem;
      margin-bottom: 0.5rem;
    }
    .post-toc__title {
      text-transform: uppercase;
      font-size: 0.78rem;
      letter-spacing: 0.18em;
      font-weight: 700;
      color: var(--text-muted);
    }
    .post-toc__toggle {
      display: none;
      border: 1px solid var(--border);
      background: transparent;
      color: var(--text-secondary);
      border-radius: 999px;
      padding: 0.25rem 0.8rem;
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      align-items: center;
      gap: 0.4rem;
      transition: background 0.2s ease, border 0.2s ease, color 0.2s ease;
    }
    .post-toc__toggle:hover {
      background: rgba(96, 165, 250, 0.15);
      border-color: transparent;
      color: var(--accent);
    }
    .post-toc__content {
      margin-top: 0.6rem;
      overflow-y: auto;
      padding-right: 0.4rem;
      transition: max-height 0.25s ease, opacity 0.25s ease;
      max-height: calc(100vh - 220px);
    }
    .post-toc--collapsed .post-toc__content {
      max-height: 0;
      opacity: 0;
      margin-top: 0;
      pointer-events: none;
    }
    .post-toc__list {
      list-style: none;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      gap: 0.45rem;
    }
    .post-toc__sublist {
      margin-left: 0.85rem;
      padding-left: 0.85rem;
      border-left: 1px solid rgba(148, 163, 184, 0.35);
      margin-top: 0.4rem;
      gap: 0.35rem;
    }
    .post-toc__item {
      margin: 0;
    }
    .post-toc__link {
      color: var(--text-secondary);
      font-size: 0.95rem;
      line-height: 1.45;
      display: flex;
      align-items: flex-start;
      gap: 0.5rem;
      border-bottom: 1px dashed transparent;
      transition: color 0.2s ease, border-bottom 0.2s ease, transform 0.2s ease;
    }
    .post-toc__link:hover {
      color: var(--accent);
      border-bottom-color: rgba(96, 165, 250, 0.4);
      transform: translateX(2px);
    }
    .post-toc__link--active {
      color: var(--accent-strong);
      border-bottom-color: var(--accent-strong);
    }
    .post-toc__number {
      font-variant-numeric: tabular-nums;
      font-size: 0.85rem;
      color: var(--text-muted);
      min-width: 2.5ch;
      display: inline-flex;
      justify-content: flex-end;
      padding-top: 0.15rem;
    }
    .post-toc__text {
      flex: 1;
    }
    .post-body {
      background: var(--bg-card-strong);
      border: 1px solid var(--border);
      border-radius: 26px;
      padding: 2.5rem;
      box-shadow: var(--shadow-lg);
      font-size: 1.04rem;
      line-height: 1.75;
      color: var(--text-secondary);
    }
    .post-body h2 {
      margin-top: 2.75rem;
      margin-bottom: 1.25rem;
      font-size: clamp(1.9rem, 3vw, 2.35rem);
      color: var(--text-primary);
      letter-spacing: -0.01em;
    }
    .post-body h3 {
      margin-top: 2.2rem;
      margin-bottom: 1rem;
      font-size: 1.5rem;
      color: var(--text-primary);
    }
    .post-body h4 {
      margin-top: 1.8rem;
      margin-bottom: 0.75rem;
      font-size: 1.2rem;
      color: var(--text-primary);
    }
    .post-body p {
      margin-bottom: 1.4rem;
    }
    .post-body .post-warning {
      margin: 2rem 0;
      border-radius: 18px;
      border: 1px solid rgba(250, 204, 21, 0.35);
      background: rgba(254, 243, 199, 0.9);
      color: #4a3b0a;
      padding: 0 1.25rem 1rem;
      box-shadow: inset 0 0 0 1px rgba(255, 255, 255, 0.35);
    }
    body[data-theme="dark"] .post-body .post-warning {
      background: rgba(253, 230, 138, 0.12);
      border-color: rgba(251, 191, 36, 0.5);
      color: #f6e6b2;
      box-shadow: inset 0 0 0 1px rgba(250, 200, 88, 0.3);
    }
    .post-body .post-warning summary {
      list-style: none;
      cursor: pointer;
      font-weight: 600;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      padding: 1rem 0;
      color: inherit;
    }
    .post-body .post-warning summary::-webkit-details-marker {
      display: none;
    }
    .post-body .post-warning summary::before {
      content: '‚ö†Ô∏è';
      font-size: 1rem;
    }
    .post-body .post-warning[open] {
      padding-bottom: 1.25rem;
    }
    .post-body .post-warning p:last-child {
      margin-bottom: 0;
    }
    .post-body ul,
    .post-body ol {
      margin: 1.4rem 0 1.4rem 1.4rem;
      padding: 0;
    }
    .post-body li {
      margin-bottom: 0.8rem;
    }
    .post-body a {
      color: var(--accent);
      text-decoration: none;
      border-bottom: 1px solid rgba(96, 165, 250, 0.35);
      transition: color 0.2s ease, border-bottom 0.2s ease;
    }
    .post-body a:hover {
      color: var(--accent-strong);
      border-bottom-color: var(--accent-strong);
    }
    .post-body blockquote {
      margin: 2rem 0;
      padding: 1.5rem 1.75rem;
      border-left: 4px solid var(--accent);
      border-radius: 0 18px 18px 0;
      background: rgba(37, 99, 235, 0.12);
      color: var(--text-primary);
    }
    .post-body code {
      background: var(--code-inline-bg);
      color: var(--code-text);
      padding: 0.2rem 0.45rem;
      border-radius: 6px;
      font-family: 'JetBrains Mono', 'Fira Code', monospace;
      font-size: 0.9rem;
    }
    .post-body pre code {
      background: transparent;
      padding: 0;
      display: block;
      font-size: inherit;
      line-height: inherit;
    }
    .hljs {
      color: #e2e8f0;
      background: transparent;
    }
    .hljs-comment,
    .hljs-quote {
      color: #7dd79d;
      font-style: italic;
    }
    .hljs-keyword,
    .hljs-selector-tag,
    .hljs-literal,
    .hljs-name,
    .hljs-strong,
    .hljs-built_in {
      color: #7dd3fc;
      font-weight: 600;
    }
    .hljs-title,
    .hljs-section,
    .hljs-function,
    .hljs-meta .hljs-keyword {
      color: #38bdf8;
      font-weight: 600;
    }
    .hljs-string,
    .hljs-doctag,
    .hljs-addition,
    .hljs-attribute,
    .hljs-template-tag,
    .hljs-template-variable {
      color: #facc15;
    }
    .hljs-number,
    .hljs-symbol,
    .hljs-bullet,
    .hljs-link,
    .hljs-meta,
    .hljs-type {
      color: #f472b6;
    }
    .hljs-variable,
    .hljs-params {
      color: #cbd5f5;
    }
    .post-body pre {
      background: var(--code-block-bg);
      color: var(--code-text);
      padding: 1.2rem 1.4rem;
      padding-right: 3.6rem;
      border-radius: 18px;
      overflow-x: auto;
      font-family: 'JetBrains Mono', 'Fira Code', monospace;
      font-size: 0.95rem;
      box-shadow: var(--code-shadow);
      border: 1px solid var(--code-border);
      margin: 2rem 0;
      position: relative;
    }
    .code-copy-btn {
      position: absolute;
      top: 0.75rem;
      right: 0.75rem;
      background: rgba(15, 23, 42, 0.8);
      color: #e2e8f0;
      border: 1px solid rgba(148, 163, 184, 0.35);
      border-radius: 999px;
      padding: 0.25rem 0.85rem;
      font-size: 0.85rem;
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, border-color 0.2s ease, transform 0.2s ease;
    }
    .code-copy-btn:hover {
      background: rgba(96, 165, 250, 0.85);
      color: #ffffff;
      border-color: transparent;
      transform: translateY(-1px);
    }
    .code-copy-btn--copied {
      background: rgba(34, 197, 94, 0.85);
      color: #ffffff;
      border-color: transparent;
    }
    .code-copy-btn__icon {
      font-size: 0.95rem;
    }
    .code-copy-btn__text {
      display: inline-block;
    }
    body[data-theme="light"] .code-copy-btn {
      background: rgba(248, 250, 252, 0.85);
      color: #0f172a;
      border-color: rgba(148, 163, 184, 0.4);
    }
    body[data-theme="light"] .code-copy-btn--copied {
      background: rgba(34, 197, 94, 0.92);
      color: #ffffff;
    }
    .post-body img {
      max-width: 100%;
      border-radius: 18px;
      margin: 2.2rem 0;
      box-shadow: 0 24px 45px -28px rgba(15, 23, 42, 0.55);
    }
    .post-body .table-wrapper {
      margin: 2rem 0;
      border-radius: 18px;
      border: 1px solid var(--border);
      background: rgba(15, 23, 42, 0.55);
      box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.12);
      position: relative;
      overflow: hidden;
    }
    .post-body .table-wrapper__scroll {
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }
    .post-body .table-wrapper__scroll::-webkit-scrollbar {
      height: 10px;
    }
    .post-body .table-wrapper__scroll::-webkit-scrollbar-thumb {
      background: rgba(96, 165, 250, 0.4);
      border-radius: 999px;
    }
    .post-body .table-wrapper table {
      width: 100%;
      border-collapse: collapse;
      background: transparent;
    }
    .post-body .table-wrapper[data-table-size="medium"] table {
      min-width: 720px;
    }
    .post-body .table-wrapper[data-table-size="wide"] table {
      min-width: 960px;
    }
    .post-body .table-wrapper thead th {
      background: rgba(96, 165, 250, 0.12);
      color: var(--text-primary);
      font-weight: 600;
    }
    .post-body .table-wrapper th,
    .post-body .table-wrapper td {
      padding: 0.9rem 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(148, 163, 184, 0.18);
      white-space: nowrap;
    }
    .post-body .table-wrapper td {
      white-space: normal;
    }
    .post-body .table-wrapper tr:last-child td {
      border-bottom: none;
    }
    .post-body .table-wrapper__expand {
      position: absolute;
      top: 0.75rem;
      right: 0.75rem;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.3);
      color: var(--accent);
      border-radius: 999px;
      padding: 0.35rem 0.9rem;
      font-size: 0.85rem;
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, transform 0.2s ease;
      z-index: 2;
    }
    .post-body .table-wrapper__expand:hover {
      background: rgba(37, 99, 235, 0.35);
      color: #ffffff;
      transform: translateY(-1px);
      border-color: transparent;
    }
    .table-overlay {
      position: fixed;
      inset: 0;
      background: rgba(15, 23, 42, 0.85);
      backdrop-filter: blur(6px);
      display: none;
      align-items: center;
      justify-content: center;
      padding: 2rem;
      z-index: 999;
    }
    .table-overlay--visible {
      display: flex;
    }
    .table-overlay__content {
      background: var(--bg-card-strong);
      border: 1px solid var(--border);
      border-radius: 24px;
      max-width: min(1080px, 92vw);
      max-height: 85vh;
      width: 100%;
      box-shadow: 0 32px 80px -40px rgba(15, 23, 42, 0.9);
      position: relative;
      overflow: hidden;
    }
    .table-overlay__close {
      position: absolute;
      top: 0.85rem;
      right: 0.85rem;
      background: rgba(15, 23, 42, 0.9);
      border: 1px solid rgba(148, 163, 184, 0.35);
      color: var(--text-primary);
      border-radius: 999px;
      padding: 0.4rem 1rem;
      font-size: 0.9rem;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease;
    }
    .table-overlay__close:hover {
      background: rgba(37, 99, 235, 0.4);
      color: #ffffff;
      border-color: transparent;
    }
    .table-overlay__scroll {
      overflow: auto;
      max-height: 85vh;
      padding: 2.5rem 2rem 2rem;
    }
    .table-overlay__scroll table {
      width: 100%;
      border-collapse: collapse;
      background: transparent;
    }
    .table-overlay__scroll table[data-table-size="medium"] {
      min-width: 720px;
    }
    .table-overlay__scroll table[data-table-size="wide"] {
      min-width: 960px;
    }
    .table-overlay__scroll thead th {
      background: rgba(96, 165, 250, 0.12);
      color: var(--text-primary);
      font-weight: 600;
    }
    .table-overlay__scroll th,
    .table-overlay__scroll td {
      padding: 0.9rem 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(148, 163, 184, 0.18);
      white-space: nowrap;
    }
    .table-overlay__scroll td {
      white-space: normal;
    }
    .table-overlay__scroll tr:last-child td {
      border-bottom: none;
    }
    body[data-theme="light"] .post-body .table-wrapper {
      background: rgba(255, 255, 255, 0.96);
      box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.16);
    }
    body[data-theme="light"] .post-body .table-wrapper__expand {
      background: rgba(248, 250, 252, 0.9);
    }
    body[data-theme="light"] .table-overlay {
      background: rgba(15, 23, 42, 0.25);
    }
    body[data-theme="light"] .table-overlay__content {
      background: rgba(255, 255, 255, 0.98);
    }
    body.no-scroll {
      overflow: hidden;
    }
    footer {
      margin-top: 4rem;
      padding: 2rem 0;
      text-align: center;
      color: var(--text-muted);
      font-size: 0.92rem;
      border-top: 1px solid var(--border);
      background: rgba(15, 23, 42, 0.35);
    }
    body[data-theme="light"] footer {
      background: rgba(255, 255, 255, 0.72);
    }
    @media (max-width: 1024px) {
      .site-header__inner {
        padding: 1rem clamp(1.25rem, 4vw, 2rem);
      }
      main.page {
        padding: 2.75rem clamp(1.25rem, 4vw, 2rem) 4rem;
      }
      .post-layout {
        grid-template-columns: minmax(0, 1fr);
      }
      .post-toc {
        position: sticky;
        top: 88px;
        z-index: 6;
        max-height: calc(100vh - 140px);
        margin-bottom: 2rem;
        padding: 1.1rem 1.25rem 1.35rem;
      }
      .post-toc__toggle {
        display: inline-flex;
      }
      .post-toc__content {
        max-height: none;
        margin-top: 0.4rem;
        overflow: visible;
      }
    }
    @media (max-width: 720px) {
      .post-hero {
        padding: 2.1rem 1.65rem;
      }
      .post-body {
        padding: 1.9rem 1.5rem;
      }
      .site-header__inner {
        flex-direction: column;
        align-items: stretch;
        gap: 1rem;
      }
      .site-header__left {
        justify-content: space-between;
      }
      .header-controls {
        align-self: flex-end;
      }
      .post-hero__title {
        font-size: clamp(2rem, 6vw, 2.6rem);
      }
      .post-body .table-wrapper {
        margin: 1.6rem 0;
      }
      .post-body .table-wrapper__expand {
        top: 0.6rem;
        right: 0.6rem;
        font-size: 0.78rem;
        padding: 0.25rem 0.75rem;
      }
      .table-overlay__scroll {
        padding: 1.8rem 1.25rem 1.5rem;
      }
    }
  </style>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      },
    };
  </script>
  <script id="mathjax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body data-theme="dark">
  <header class="site-header">
    <div class="site-header__inner">
      <div class="site-header__left">
        <a class="logo" href="../../../index.html">
          <img src="../../../Assets/Logo.png" alt="Mirko Calcaterra logo" class="logo-img">
          <span class="logo-text">Mirko Calcaterra</span>
        </a>
        <nav class="site-nav">
          <a href="../../../index.html" data-it="Home" data-en="Home">Home</a>
          <a href="../../../blog/index.html" data-it="Blog" data-en="Blog">Blog</a>
        </nav>
      </div>
      <div class="header-controls">
        <button class="lang-btn" type="button">EN</button>
        <button class="theme-toggle" type="button" aria-label="Toggle theme">
          <span class="theme-thumb">‚òÄÔ∏è</span>
        </button>
      </div>
    </div>
  </header>
  <main class="page">
    <article class="post">
      <section class="post-hero">
        <div class="post-hero__icon">üß†</div>
        <span class="post-hero__category">AI Engineering Path</span>
        <h1 class="post-hero__title">GeoAI Stack: Where to start to become a GeoAI engineer?</h1>
        <div class="post-hero__meta">
          <span>üìÖ January 8, 2026</span>
          <span>‚è±Ô∏è 25 min</span>
        </div>
      </section>
      <section class="post-layout">
        <aside class="post-toc" data-collapsed="false">
        <div class="post-toc__header">
          <div class="post-toc__title" data-it="Indice" data-en="Table of contents">Table of contents</div>
          <button class="post-toc__toggle" type="button" aria-expanded="true" aria-label="Hide table of contents">
            <span class="post-toc__toggle-text">Table of contents</span>
            <span class="post-toc__toggle-icon" aria-hidden="true">‚ñæ</span>
          </button>
        </div>
        <div class="post-toc__content">
          <ul class="post-toc__list">
    <li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#1-architectural-overview-of-the-geospatial-ai-stack">
            <span class="post-toc__number">1</span>
            <span class="post-toc__text">1\. Architectural Overview of the Geospatial AI Stack</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#local-vs-production-architecture">
            <span class="post-toc__number">1.1</span>
            <span class="post-toc__text">Local vs. Production Architecture</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#2-map-of-tools-and-resources-2025">
            <span class="post-toc__number">2</span>
            <span class="post-toc__text">2\. Map of Tools and Resources (2025)</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#21-python-environment-management-venv-conda-poetry-etc">
            <span class="post-toc__number">2.1</span>
            <span class="post-toc__text">2.1 Python Environment Management (venv, conda, poetry, etc.)</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#gpugeo-reproducibility">
            <span class="post-toc__number">2.1.1</span>
            <span class="post-toc__text">GPU/Geo Reproducibility</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#22-essential-ai-engineer-tooling">
            <span class="post-toc__number">2.2</span>
            <span class="post-toc__text">2.2 Essential AI Engineer Tooling</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#does-all-this-make-sense-today-with-ai">
            <span class="post-toc__number">2.2.1</span>
            <span class="post-toc__text">Does all this make sense today with AI?</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#23-docker-and-aigeo-containerization">
            <span class="post-toc__number">2.3</span>
            <span class="post-toc__text">2.3 Docker and AI+GEO Containerization</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#example-1-dockerfile-for-llmrag-fastapi-service-cpu">
            <span class="post-toc__number">2.3.1</span>
            <span class="post-toc__text">Example 1: Dockerfile for LLM/RAG + FastAPI Service (CPU)</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#example-2-dockerfile-for-geospatial-pipeline-with-gdal-optional-gpu">
            <span class="post-toc__number">2.3.2</span>
            <span class="post-toc__text">Example 2: Dockerfile for geospatial pipeline (with GDAL, optional GPU)</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#gpu-management">
            <span class="post-toc__number">2.3.3</span>
            <span class="post-toc__text">GPU Management</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#24-managing-passwords-and-configurations">
            <span class="post-toc__number">2.4</span>
            <span class="post-toc__text">2.4 Managing &quot;passwords&quot; and configurations</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#env-approach-local">
            <span class="post-toc__number">2.4.1</span>
            <span class="post-toc__text">.env approach (local)</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#yamlini-file-approach-other-classes">
            <span class="post-toc__number">2.4.2</span>
            <span class="post-toc__text">YAML/INI file approach + other classes</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#secret-management-in-production">
            <span class="post-toc__number">2.4.3</span>
            <span class="post-toc__text">Secret Management in Production</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#25-open-datasets-and-staccog-catalogs-for-disasters">
            <span class="post-toc__number">2.5</span>
            <span class="post-toc__text">2.5 Open Datasets and STAC/COG Catalogs for Disasters</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#active-stac-catalogs">
            <span class="post-toc__number">2.5.1</span>
            <span class="post-toc__text">Active STAC Catalogs</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#other-notable-datasets-2025">
            <span class="post-toc__number">2.5.2</span>
            <span class="post-toc__text">Other Notable Datasets 2025</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#26-geospatial-and-remote-sensing-libraries">
            <span class="post-toc__number">2.6</span>
            <span class="post-toc__text">2.6 Geospatial and Remote Sensing Libraries</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#when-to-use-what">
            <span class="post-toc__number">2.6.1</span>
            <span class="post-toc__text">When to use what</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#example-pipeline-optical-sar">
            <span class="post-toc__number">2.6.2</span>
            <span class="post-toc__text">Example Pipeline (Optical + SAR)</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#27-project-templates-reference-repositories-and-best-practices">
            <span class="post-toc__number">2.7</span>
            <span class="post-toc__text">2.7 Project Templates, Reference Repositories, and Best Practices</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#conclusion-an-ideal-repository-aigeo">
            <span class="post-toc__number">3</span>
            <span class="post-toc__text">Conclusion: An Ideal Repository (AI+Geo)</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#how-to-get-started-immediately">
            <span class="post-toc__number">3.1</span>
            <span class="post-toc__text">How to get started immediately</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#bibliography">
            <span class="post-toc__number">4</span>
            <span class="post-toc__text">Bibliography</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#models-papers-and-architectures">
            <span class="post-toc__number">4.1</span>
            <span class="post-toc__text">Models, papers, and architectures</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#geoai-libraries-and-frameworks">
            <span class="post-toc__number">4.2</span>
            <span class="post-toc__text">GeoAI Libraries and Frameworks</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#services-and-apis">
            <span class="post-toc__number">4.3</span>
            <span class="post-toc__text">Services and APIs</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#agents-and-rag-frameworks">
            <span class="post-toc__number">4.4</span>
            <span class="post-toc__text">Agents and RAG frameworks</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#python-tooling-and-quality">
            <span class="post-toc__number">4.5</span>
            <span class="post-toc__text">Python Tooling and Quality</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#containers-and-base-images">
            <span class="post-toc__number">4.6</span>
            <span class="post-toc__text">Containers and Base Images</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#stac-and-data-access">
            <span class="post-toc__number">4.7</span>
            <span class="post-toc__text">STAC and Data Access</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#datasets-benchmarks-and-open-catalogs">
            <span class="post-toc__number">4.8</span>
            <span class="post-toc__text">Datasets, Benchmarks, and Open Catalogs</span>
          </a>
          
        </li>
  </ul>
        </li>
  </ul>
        </div>
      </aside>
        <div class="post-body">
          <p>Second installment of our long journey to become a GeoAI engineer. Last time we described very generally what an AI engineer does and what differentiates them.</p>
<p>Now let&#39;s understand what an aspiring GeoAI engineer needs to get started, namely the tools, datasets, and necessary setup.</p>
<h2 id="1-architectural-overview-of-the-geospatial-ai-stack">1. Architectural Overview of the Geospatial AI Stack</h2>
<p>Let&#39;s start with the main objective and keep it in mind.</p>
<blockquote>
<p><strong>Objective:</strong> Integrate <strong>Language (LLM)</strong> models and <strong>Geospatial Vision</strong> pipelines in a reproducible environment, from local development to production.</p>
</blockquote>
<p>The typical architecture combines:</p>
<ul>
<li><strong>Geospatial data ingestion:</strong> access to optical satellite imagery (e.g., Sentinel-2) and SAR radar (e.g., Sentinel-1) via <strong>STAC/COG</strong> catalogs (Planetary Computer, Earth Data, etc.). We will discuss this in detail later.</li>
<li><strong>Pre-processing and remote sensing analysis:</strong> Python pipelines to read, align, and process large rasters (with <em>rasterio</em>/<em>GDAL</em>, <em>rioxarray</em>/<em>dask</em> for voluminous data) and vectors (with <em>geopandas</em>/<em>shapely</em>). This produces <em>features</em> such as damage maps, flood extent, extracted buildings, etc.</li>
<li><strong>Vision and geospatial models:</strong> application of specialized deep learning models on pre-processed data. For example, <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=Our%20main%20algorithm%20of%20choice,algorithm%20for%20semantic%20image%20segmentation">IBM used <strong>U-Net</strong></a> in one of its research activities for damage segmentation post-natural disaster. Or, models derived from <a href="https://arxiv.org/abs/2105.15203"><strong>SegFormer</strong></a> for change detection, such as <a href="https://arxiv.org/abs/2407.15317v1">Open-CD</a>.<br>Libraries like <a href="https://github.com/torchgeo/torchgeo"><strong>TorchGeo</strong></a> provide ready-to-use geospatial-specific datasets and pre-trained models.</li>
</ul>
<p><img src="../../../Assets/Unet.png" alt="Image">
<em>Figure 1 ‚Äì U-Net architecture with encoder/decoder and skip connection for segmenting damage and classes in satellite images.</em></p>
<p><img src="../../../Assets/TorchGeo.png" alt="Image">
<em>Figure 2 ‚Äì TorchGeo collects ready-to-use datasets and pre-trained models designed for geospatial computer vision scenarios.</em></p>
<ul>
<li><p><strong>LLM/RAG Integration:</strong> a <strong>Retrieval-Augmented Generation (RAG)</strong> module connects geospatial results with an <strong>LLM</strong> (e.g., GPT-5 or Mistral Large) to enable Q&amp;A and reporting. The LLM can draw upon updated knowledge bases (documents, place descriptions) in addition to extracted data. This reduces the problem of hallucinations by providing verifiable context. For example, a user can ask <em>&quot;How many buildings were destroyed by the earthquake in Turkey?&quot;</em> and the system uses data extracted from the CV model + textual descriptions to generate an answer citing sources.</p>
</li>
<li><p><strong>Agents and automation:</strong> agent-based components (built with frameworks like <strong>LangChain</strong>, <strong>Haystack</strong>, or Datapizza-AI) orchestrate the steps and calls to specific tools. <br>Specifically, an <em>agent</em> can:</p>
<ol>
<li>query a <em>geospatial database</em> to find relevant post-disaster images;</li>
<li>execute the computer vision model to obtain metrics (number of damaged buildings, flooded area, etc.);</li>
<li>call the LLM to explain the results.</li>
</ol>
<p>  This enables complex &quot;question -&gt; actions -&gt; answer&quot; workflows in a modular way.</p>
</li>
<li><p><strong>Services and deployment:</strong> everything is containerized (Docker) and can be exposed via REST APIs (e.g., with <a href="https://fastapi.tiangolo.com/"><strong>FastAPI</strong></a>) or lightweight graphical interfaces. For example, a <em>Streamlit</em> dashboard can display interactive maps with damage layers and offer an LLM chat for questions about the disaster.</p>
</li>
</ul>
<h3 id="local-vs-production-architecture">Local vs. Production Architecture</h3>
<p>When developing the solution, it is good practice to work on test datasets (for example, on a few representative satellite scenes) using notebooks (like Jupyter Lab, Colab, etc.) or modular scripts (in src/).</p>
<p>In production, however, individual <strong>components</strong> are orchestrated into <strong>microservices</strong>: there can be a service for geospatial analysis (e.g., calculating risk maps) and a service for the LLM (e.g., generating responses), with logging and monitoring. Raw data in this case, such as images, resides in storage (either a local file system or a cloud bucket), while intermediate results, such as generated COGs (Cloud Optimized GeoTIFFs), shapefiles, vector embeddings, can be cached to speed up repeated requests.</p>
<p>I would like to point out that language models are typically used via external APIs (OpenAI, etc.) or, in the case of optimized open-source models (e.g., gemma3n 4B), inference occurs on-premise.</p>
<p>As I read in <a href="https://arxiv.org/html/2502.18470v5#:~:text=On%20the%20other%20hand%2C%20large,zhang2024bb%20%2C%20but%20the%20resulting">this paper</a>, this <strong>hybrid AI + GIS</strong> architecture overcomes the limitations of individual systems: classic GIS systems struggle with natural language input, while <em>&quot;Large Language Models show strong linguistic capabilities but struggle with spatial reasoning and geospatial ground truth&quot;</em>. By combining them, we obtain a system where visual models provide &quot;eyes&quot; and structured data, and LLMs provide &quot;linguistic reasoning&quot; on such data, with the ability to consult real-time knowledge bases. In summary, the stack embraces the full cycle: <strong>Data (Geo) ‚Üí AI Vision ‚Üí Knowledge ‚Üí LLM</strong>. The following figure illustrates the key components and data flow in the system (from data collection to user response):</p>
<p><img src="../../../Assets/framework.png" alt="Image"></p>
<p><em>Figure 3 ‚Äì Summary diagram of the GeoAI stack connecting geospatial ingestion and analysis, CV models, RAG/LLM components, and agent-orchestrated deployment services.</em></p>
<h2 id="2-map-of-tools-and-resources-2025">2. Map of Tools and Resources (2025)</h2>
<p>In this chapter, we will look at the main options for each aspect of the stack: from Python environment management to development tools, covering container basics, open geospatial datasets, and key libraries. The goal will not only be to understand all of this but also to keep in mind a comparison of features, outlining the advantages of each method.</p>
<h3 id="21-python-environment-management-venv-conda-poetry-etc">2.1 Python Environment Management (venv, conda, poetry, etc.)</h3>
<p>One thing we all agree on: to have a solid foundation, a <strong>reproducible Python environment</strong> is necessary, with all dependencies, including GPU packages and geospatial libraries.</p>
<p>The following table compares the most common approaches today (in 2025):</p>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th>Approach</th>
<th>Type</th>
<th>Advantages (pros)</th>
<th>Disadvantages (cons)</th>
<th>Update</th>
</tr>
</thead>
<tbody><tr>
<td><strong>pip + venv</strong></td>
<td>Installer + isolated env</td>
<td>Simple and native; fast direct installation</td>
<td>Solver <a href="https://debuglab.net/2024/01/26/resolving-new-pip-backtracking-runtime-issue/"><strong>no longer greedy</strong></a> but still less powerful than SAT solvers; no native lockfile; requires <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=The%20OG%20of%20Python%20package,be%20completely%20decoupled%20from%20a">manual removal of unused sub-deps</a></td>
<td>pip 25.3 (2025)</td>
</tr>
<tr>
<td><strong>conda / mamba</strong></td>
<td>Package manager with C++ SAT solver</td>
<td><a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=Python%20GDAL%20requires%20,while%20using%20Conda%20package%20manager">Manages <strong>non-Python</strong> libraries</a> (e.g., GDAL, PROJ) in isolated envs; complete and fast resolution thanks to the <em>libmamba</em> solver</td>
<td>Heavy base environment (hundreds MB); lacks out-of-the-box lockfile; sometimes requires mixing pip ‚Üí possible conflicts</td>
<td>Conda 25.9.1 (2025)</td>
</tr>
<tr>
<td><strong>Poetry</strong></td>
<td>PyPI manager with lockfile</td>
<td>Uses unified <em>pyproject.toml</em> standard; generates multi-platform <strong>lockfile</strong> for reproducibility; automatically manages virtual env</td>
<td>Python-based solver relatively slow on large reqs (DFS backtracking); voluminous lockfile; beware of excessive constraints (^version) which can cause conflicts in large teams.</td>
<td>Poetry 2.1 (2025)</td>
</tr>
<tr>
<td><strong>PDM / Hatch</strong></td>
<td>Modern PyPI managers</td>
<td><strong>PDM</strong> supports PEP 582 (local environment without activate); <strong>Hatch</strong> also acts as a complete build system and allows multi-version Python testing</td>
<td>Less common than the pip/conda/poetry triad; Hatch has a higher learning curve and focuses on packaging (not just env)</td>
<td>PDM 2.26 (2025), Hatch 1.15 (2025)</td>
</tr>
<tr>
<td><strong>uv (Astral)</strong></td>
<td>Unified all-in-one tool (Rust)</td>
<td><a href="https://docs.astral.sh/uv/"><strong>Extremely fast</strong></a> (10-100√ó pip) thanks to its Rust core; replaces pip, pipx, poetry, pyenv with a single tool; universal lockfile support and multi-project workspace; transparent integration with existing venvs (you can activate uv, then use normal pip if you want)</td>
<td>API and CLI still unstable (young project); emerging community</td>
<td>uv 0.9.9 (2025)</td>
</tr>
<tr>
<td><strong>pixi (prefix.dev)</strong></td>
<td>Conda-like package manager (Rust)</td>
<td><a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=benchmarks%20show%20that%20pixi%20is,on%20a%20M2%20MacBook%20Pro"><strong>Speed</strong></a> <del>4√ó micromamba, &gt;10√ó conda (resolution + install); supports its own cross-platform lockfile (solves one of conda&#39;s limitations); integrates PyPI packages into the unified solver (<a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative">uses uv libraries</a>); no conda base env to install ([standalone executable](<a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#">https://prefix.dev/blog/pixi_a_fast_conda_alternative#</a>:</del>:text=Reason%203%3A%20No%20more%20Miniconda,base%20environment))</td>
<td>New ecosystem (releases &lt;1 year); fewer precompiled packages compared to conda-forge; some commands are still evolving</td>
<td>pixi 0.59 (2025)</td>
</tr>
</tbody></table>
</div></figure><p>| <strong>pyenv</strong> | Python version manager | Allows easy installation and switching of different Python versions per project (useful for multi-version or legacy testing) | Does not manage packages; used in combination with venv/poetry; if used globally, can cause confusion about active versions | pyenv 2.6.12 (2025) |</p>
<p><em>Table notes:</em> <strong>mamba</strong> is the C++ implementation of conda. Today, <em>conda</em> itself incorporates <em>libmamba</em> by default, so the two converge in speed. In data science environments, conda/mamba remains popular for its ease with complex scientific packages, while in production, <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=Verdict">pip/poetry is often preferred</a> to avoid extra dependencies and have more control. Emerging tools like uv and pixi aim to unify the best of both worlds (speed and completeness). For example, <strong>uv</strong> is developed in Rust by the creators of Ruff and aims to become the &quot;Cargo for Python&quot; (a single tool for managing Python versions, dependencies, virtualenvs, publishing). <strong>Pixi</strong>, created by the mamba team, is a drop-in conda replacement written in Rust: it uses <em>uv</em> to resolve pip packages, generates lockfiles, and removes the need for a conda base environment, drastically improving speed and ergonomics for bringing conda environments to production.</p>
<h4 id="gpugeo-reproducibility">GPU/Geo Reproducibility</h4>
<p>For a local AI/RS project on GPU, conda often offers the simplest path (e.g., <code>conda install pytorch cudatoolkit gdal rasterio</code> in an env) because it manages compatible &quot;binaries&quot; (CUDA, GDAL). Alternatively, in containerized environments, you can opt for <strong>pip + Docker</strong> using base images with appropriate drivers (we&#39;ll discuss this shortly).</p>
<p>In all cases, it is recommended to <strong>pin versions</strong> in a lockfile (for example, using poetry.lock) or requirements files with hashes, and document the setup (e.g., by providing a conda environment.yml + pip requirements.txt for safety).</p>
<p>An ideal project layout includes a structure similar to:</p>
<pre><code class="hljs language-bash">proj-root/
‚îú‚îÄ‚îÄ src/ <span class="hljs-comment"># application code (main package)</span>
‚îú‚îÄ‚îÄ notebooks/ <span class="hljs-comment"># exploratory Jupyter notebooks</span>
‚îú‚îÄ‚îÄ data/ <span class="hljs-comment"># raw or sample data (gitignored if large)</span>
‚îú‚îÄ‚îÄ models/ <span class="hljs-comment"># saved models or weights</span>
‚îú‚îÄ‚îÄ tests/ <span class="hljs-comment"># unit/integrated tests</span>
‚îú‚îÄ‚îÄ configs/ <span class="hljs-comment"># configs for Hydra/pydantic (e.g., dev.yaml, prod.yaml)</span>
‚îú‚îÄ‚îÄ Dockerfile <span class="hljs-comment"># container definition</span>
‚îú‚îÄ‚îÄ pyproject.toml <span class="hljs-comment"># package/dep specifications (poetry/pdm)</span>
‚îú‚îÄ‚îÄ requirements.txt <span class="hljs-comment"># dependencies (if pip)</span>
‚îú‚îÄ‚îÄ Makefile <span class="hljs-comment"># useful commands (setup, run, lint, test, deploy)</span>
‚îî‚îÄ‚îÄ README.md <span class="hljs-comment"># project documentation</span></code></pre><p>This organization partly follows the <a href="https://cookiecutter-data-science.drivendata.org/#directory-structure"><em>Cookiecutter Data Science</em></a> model (to clearly separate code, data, docs) and facilitates the transition <strong>from local development to production</strong>: the code is packaged (in src/ with an optional _<em>init</em>_.py), tests ensure functionality, and separate configs/secrets per environment allow for consistent deployments.</p>
<p>Let&#39;s say it&#39;s good practice, costs nothing, and provides an order that is useful both to you and to those who work with you.</p>
<h3 id="22-essential-ai-engineer-tooling">2.2 Essential AI Engineer Tooling</h3>
<p>To ensure <strong>code quality and development speed</strong>, every developer adopts a set of lightweight DevOps/MLOps tools, which we will look at shortly.</p>
<p>Before giving advice on this, I will provide some definitions that may be obvious to some but perhaps not to others.</p>
<blockquote>
<p>Linting is an automatic check of source code for:</p>
<ul>
<li>potential errors: unused variables, suspicious syntax, common bugs</li>
<li>style issues: formatting, inconsistent names, unrespected conventions.</li>
<li>&quot;code smell&quot;: patterns that are not errors but can cause problems later
In practice, files are analyzed without executing them, and points to fix are flagged, often with suggestions or automatic corrections.</li>
</ul>
</blockquote>
<ul>
<li><strong>Linting &amp; Format:</strong> <em>Black</em> for automatic code formatting (opinionated, PEP8) and <strong>Ruff</strong> for ultra-fast linting in Rust. Ruff includes hundreds of rules (replaces flake8, isort, etc.) and automatically fixes many issues. It has practically supplanted traditional linters in a short time thanks to its speed and coverage.<blockquote>
<p><strong>Tip:</strong> configure Black and Ruff to avoid overlapping rules (e.g., disable rules in Ruff that Black already handles, such as line length) - this can be done by <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac">centralizing the configuration</a> in pyproject.toml.</p>
</blockquote>
</li>
<li><strong>Type Checking:</strong> use <strong>static typing</strong> to prevent bugs. <em>Mypy</em> is the standard for type-checking in Python; alternatively, <em>Pyright</em> (integrated into VSCode/Pylance) offers very fast incremental analysis during writing. Setting a &quot;strict&quot; level (e.g., <code>warn_unused_configs</code>, <code>disallow_untyped_defs</code> in mypy) helps maintain robust code.</li>
<li><strong>Testing:</strong> <em>Pytest</em> is the de facto standard for unit and functional tests in Python. Organize tests in <code>tests/</code> and perhaps use useful plugins: e.g., <strong>pytest-snapshot</strong> (or <a href="https://github.com/syrupy-project/syrupy"><strong>Syrupy</strong></a>) for <em>snapshot testing</em> of complex outputs (automatically compares current output with a previously saved one). This can be convenient for validating, for example, API response JSON or raster analysis results. For geospatial pipelines, it might be useful to generate small synthetic datasets to test functions (e.g., create a 100x100 raster and a known geometry and verify that the overlay produces expected results).</li>
<li><strong>Pre-commit hooks:</strong> configure <em>pre-commit</em> (file <code>.pre-commit-config.yaml</code>) to automatically run quality tools before each commit. A recommended set of hooks: <strong>black</strong>, <strong>ruff</strong>, <strong>mypy</strong>, <em>isort</em> (if not using ruff for import sorting), <em>end-of-file-fixer</em> and <em>trailing-whitespace</em> (simple cleanups), possibly <strong>nbstripout</strong> or <strong>nbqa</strong> to normalize notebooks. This ensures that every commit adheres to code style standards and passes static tests. For example, a combined ruff/black hook makes the code consistent - a developer notes <em>&quot;Ruff + Black + isort configured together provide friction-free quality&quot;</em> in CI and editor.</li>
<li><strong>Minimal CI/CD:</strong> set up a simple GitHub Action that on every push runs: lint (ruff/black), type-check (mypy), and tests (pytest) on an environment matrix (at least Python 3.x). This automates quality control. For ML projects, a test on sample data can be included (e.g., run inference on a small image) to ensure that pipelines and models work. A minimal YAML workflow would include steps to install dependencies (using poetry/mamba for speed in CI) and then <code>pre-commit run --all-files</code> followed by <code>pytest</code>.</li>
<li><strong>Notebooks and collaboration:</strong> use <strong>JupyterLab 4</strong> (modern, supports plugins and real-time collaboration) or the <strong>VSCode notebook interface</strong> for prototyping. In shared or cloud contexts: <em>Deepnote</em>, <em>Google Colab</em>, or JupyterHub offer ready-to-use environments (Colab, for example, provides limited free GPUs). It&#39;s good practice to keep notebooks and code synchronized: <em>Jupytext</em> (notebooks as paired scripts) can be used to easily version them. To visualize geospatial data in notebooks, tools like <strong>folium</strong> (interactive Leaflet maps) or <strong>ipyleaflet/leafmap</strong> allow direct visualization of tiles, shapefiles, and model results (this will be explored in a couple of sections).</li>
</ul>
<blockquote>
<p><strong>Tip:</strong> configure all these tools from the start. For example, add black, ruff, isort, mypy with consistent configuration (same line-length, etc.) to pyproject.toml. Install pre-commit and activate it (<code>pre-commit install</code>) so that every git commit triggers the checks. These automations make development <strong>&quot;no drama&quot;</strong>: the developer can focus on AI/Geo logic, while the tools keep the code clean and functional.</p>
</blockquote>
<h4 id="does-all-this-make-sense-today-with-ai">Does all this make sense today with AI?</h4>
<p>In recent years, the introduction of AI into development tools has revolutionized how we write, maintain, and version code. Yet, this revolution has not eliminated the need for good practices: it has simply shifted the focus of what truly matters. AI accelerates, suggests, generates, but does not guarantee quality and structural consistency. For this reason, many classic tools not only remain relevant but, in some cases, become more important than before.</p>
<p>Let&#39;s start with linting and formatting. It&#39;s no longer a matter of aesthetics, but of having more substance.</p>
<p>Formatters like <strong>Black</strong> remain indispensable. Even if AI produces already readable code, consistent formatting is essential for clean diffs and friction-free reviews.</p>
<p>As for linters, tools like <strong>Ruff</strong> become fundamental not for aesthetics (which AI already handles well), but for catching errors, unused imports, dead code, or risky patterns. The goal is not to make the code &quot;prettier,&quot; but to prevent bugs introduced by somewhat overly optimistic generations.</p>
<p>Let&#39;s move on to the guardrails introduced by type checking.</p>
<p>With AI proposing complex code or plausible but not always correct functions, tools like <strong>Mypy</strong> or <strong>Pyright</strong> become essential as a safety net. Static typing is not just a quality measure, but a structural guide that AI itself uses to generate more precise solutions. In particular, in a project&#39;s core modules, a semi-strict type checking profile enormously reduces the risk of silent errors (I mean errors that are not visible now but emerge later).</p>
<p>Now let&#39;s analyze what I believe is the most important point: <strong>testing</strong>.</p>
<p>With well-structured tests and snapshot tests for complex outputs, it&#39;s possible to ensure that AI-driven optimizations or refactoring do not alter, shall we say, delicate behaviors.</p>
<p>We&#39;re almost at the end; now it&#39;s time for pre-commit, which I define as the sentinel between us and Git.</p>
<p><strong>Pre-commit</strong> hooks continue to be useful as a final filter before code enters Git (here, by Git, I mean both GitHub and GitLab).</p>
<p>Black, Ruff, and some light cleanups like whitespace removal are often sufficient. Heavier tools, like Mypy, can remain in CI to avoid local slowdowns. In personal projects, you might even do without them, but in a team, it remains a mechanism that prevents small, unnecessary errors.</p>
<p>Finally, for independence and reproducibility, there&#39;s the classic CI/CD.</p>
<p>With AI facilitating the generation of entire code blocks, the probability of unintentionally destructive changes increases. A minimal CI pipeline, including linting, type checking, and testing, ensures that every push is valid in a clean and controlled environment.</p>
<p>This is a fundamental step to prevent AI-generated code from breaking remote functions or introducing regressions that are difficult to identify.</p>
<p>In conclusion, in a world where AI supports development, &quot;code safety&quot; tools don&#39;t disappear: they reposition themselves. Black standardizes, Ruff protects, Mypy or Pyright guide, Pytest ensures stability, CI guarantees reproducibility.</p>
<p>So AI doesn&#39;t eliminate the need for these practices. On the contrary, it makes them even more relevant because it increases the volume and speed of code produced.</p>
<p>Quality is no longer just a matter of writing, but of the ecosystem. And in this ecosystem, AI is a powerful accelerator, but not a substitute!</p>
<h3 id="23-docker-and-aigeo-containerization">2.3 Docker and AI+GEO Containerization</h3>
<p>This is a part that is very dear to me (so much so that I want to dedicate an entire series to Docker).</p>
<p>Containerizing the app allows for standardizing the development environment (especially for native libraries and GPU drivers) and thus facilitating the deployment phase. We present two examples of optimized <strong>Dockerfiles</strong>, one for a lightweight LLM/RAG service, the other for a heavy geospatial pipeline, and a table of recommended base images.</p>
<h4 id="example-1-dockerfile-for-llmrag-fastapi-service-cpu">Example 1: Dockerfile for LLM/RAG + FastAPI Service (CPU)</h4>
<pre><code class="hljs language-docker"><span class="hljs-comment"># Usiamo la versione di Python leggera con Debian¬†12 &quot;bookworm&quot;</span>
<span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.12</span>-slim-bookworm as base

<span class="hljs-comment"># Aggiorniamo e installiamo solo git, senza raccomandati, quindi puliamo la cache</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends git \
    &amp;&amp; <span class="hljs-built_in">rm</span> -rf /var/lib/apt/lists/*</span>

<span class="hljs-comment"># Creiamo un utente non‚Äëroot per eseguire il servizio</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> useradd -m appuser</span>

<span class="hljs-comment"># Impostiamo la cartella di lavoro</span>
<span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span>

<span class="hljs-comment"># Installiamo una versione stabile di Poetry e configuriamo l‚Äôinstallazione dei pacchetti</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> pip install --no-cache-dir poetry==1.8.2</span>

<span class="hljs-comment"># Copiamo i file di progetto e sfruttiamo la cache Docker per velocizzare gli aggiornamenti</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> pyproject.toml poetry.lock ./</span>
<span class="hljs-comment"># Installiamo le dipendenze (solo di produzione) direttamente nel global site‚Äëpackages</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> poetry config virtualenvs.create <span class="hljs-literal">false</span> \
    &amp;&amp; poetry install --no-dev</span>

<span class="hljs-comment"># Copiamo il codice applicativo</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> src/ ./src/</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> main.py ./</span>

<span class="hljs-comment"># Eseguiamo come utente non root</span>
<span class="hljs-keyword">USER</span> appuser

<span class="hljs-comment"># Avviamo Uvicorn esponendo la FastAPI all‚Äôesterno</span>
<span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">&quot;uvicorn&quot;</span>, <span class="hljs-string">&quot;main:app&quot;</span>, <span class="hljs-string">&quot;--host&quot;</span>, <span class="hljs-string">&quot;0.0.0.0&quot;</span>, <span class="hljs-string">&quot;--port&quot;</span>, <span class="hljs-string">&quot;8000&quot;</span>]</span>
</code></pre><p><strong>Note:</strong> Here we use python:3.12-slim (~50MB) because it&#39;s a good compromise between size and functionality, as it avoids issues arising from using Alpine. I decided to propose version 3.12 for maximum compatibility with other libraries.</p>
<p>Dependency installation is done in a separate layer by copying only <code>pyproject/lock</code> (to leverage Docker cache: if only the code changes and not the dependencies, all packages are not reinstalled).</p>
<p>Uvicorn serves the FastAPI app. This container is CPU-only (suitable for LLMs via external API or small models). If we wanted to include a local model (e.g., Transformers), it would be enough to add <code>RUN pip install transformers</code> or include it directly in poetry.</p>
<h4 id="example-2-dockerfile-for-geospatial-pipeline-with-gdal-optional-gpu">Example 2: Dockerfile for geospatial pipeline (with GDAL, optional GPU)</h4>
<p>Let&#39;s now look at a more complex example, but also more suitable for a GeoAI engineer.</p>
<p>For complex geospatial pipelines (raster analysis, photogrammetry, deep
learning on satellite images), an environment with many
native libraries (GDAL, PROJ, Rasterio) and often GPU support is necessary.</p>
<pre><code class="hljs language-docker"><span class="hljs-comment"># Stage 1 ‚Äì builder with Miniconda and mamba</span>
<span class="hljs-keyword">FROM</span> continuumio/miniconda3:latest as builder

<span class="hljs-comment"># The latest version of mamba is 2.3.3. Then we remove temporary files</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> conda install -n base -c conda-forge mamba==2.3.3 \
    &amp;&amp; conda clean -afy</span>

<span class="hljs-comment"># Copy the environment listing geospatial packages (Python 3.12, GDAL,</span>
<span class="hljs-comment"># Rasterio, Geopandas, etc.)</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> environment.yaml /tmp/environment.yaml</span>

<span class="hljs-comment"># Update the base environment with the libraries specified in environment.yaml</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> mamba <span class="hljs-built_in">env</span> update -n base -f /tmp/environment.yaml \
    &amp;&amp; conda clean -afy</span>

<span class="hljs-comment"># Install additional packages with pip (e.g., Raster Vision)</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> pip install --no-cache-dir rastervision==0.31.2</span>

<span class="hljs-comment"># Stage 2 ‚Äì production image with CUDA 13.0.2 support</span>
<span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">13.0</span>.<span class="hljs-number">2</span>-runtime-ubuntu22.<span class="hljs-number">04</span> AS production

<span class="hljs-comment"># Copy the conda installation from the builder</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> --from=builder /opt/conda /opt/conda</span>

<span class="hljs-comment"># Update the PATH variable to include conda</span>
<span class="hljs-keyword">ENV</span> PATH=<span class="hljs-string">&quot;/opt/conda/bin:$PATH&quot;</span>

<span class="hljs-comment"># Set the working directory</span>
<span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span>

<span class="hljs-comment"># Copy the source code</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> src/ ./src/</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> entrypoint.py ./</span>

<span class="hljs-comment"># Pipeline startup command</span>
<span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;entrypoint.py&quot;</span>]</span>
</code></pre><p>As an <code>environment.yaml</code> file, you can use:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">geoenv</span>
<span class="hljs-attr">channels:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">conda-forge</span>
<span class="hljs-attr">dependencies:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">python=3.12</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">gdal=3.12</span>       <span class="hljs-comment"># GDAL 3.12 is available via OSGeo containers</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">rasterio</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">geopandas</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">numpy</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">pandas</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">pyproj</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">pip</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">pip:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">rastervision==0.31.2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">torch==2.7.0</span></code></pre><p>We have considered an example here taken from a <a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application">source</a> from which, at the time, I also studied. I have tried to update it based on the library versions current as of this article&#39;s date.</p>
<p>In this multi-stage Dockerfile, we use the <strong>Miniconda</strong> image with mamba as a builder to resolve dependencies (in <code>environment.yaml</code> we specify, for example: gdal, rasterio, geopandas, pytorch, etc. with the conda-forge channel). This approach automatically manages native libraries (GEOS, PROJ, etc.), avoiding pip errors (e.g., <code>pip install gdal</code> would fail without GDAL dev installed).</p>
<p>In the second part, we start from a very slim nvidia <strong>CUDA</strong> runtime, which includes only the necessary drivers for PyTorch Tensor GPU.</p>
<p>We copy the conda installation from the builder, avoiding carrying over compilation layers. The result is a ready image with GPU support and geospatial libs.</p>
<p>Below is a table of common <strong>base images</strong> for various scenarios:</p>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th>Base Image</th>
<th>Content</th>
<th>Recommended Use</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://hub.docker.com/layers/library/python/3.12-slim/images/"><strong>python:3.12-slim</strong></a></td>
<td>Debian slim + Python 3.x</td>
<td>Lightweight Python services (APIs, agents) - minimal (&lt;50MB)</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/continuumio/miniconda3"><strong>continuumio/miniconda3</strong></a></td>
<td>Miniconda + conda (base env)</td>
<td>Data science/<a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=Unlike%20pip%2C%20Conda%20package%20manager,python%2C%20we%20get%20following%20error">Full Geo</a>; easy to install complex packages (e.g., GDAL)</td>
</tr>
<tr>
<td><a href="https://micromamba-docker.readthedocs.io/en/latest/"><strong>mambaorg/micromamba</strong></a></td>
<td>Micromamba in Alpine/CentOS</td>
<td>Build images with slim conda envs; ideal for multi-stage (downloads only required packages)</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/nvidia/cuda"><strong>nvidia/cuda:13.0.2-runtime-ubuntu22.04</strong></a></td>
<td>CUDA libraries + base runtime</td>
<td>Add GPU support. To be used with pip/conda to install PyTorch/TF with compatible CUDA. Note that we are at version 13.0.2 now</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/pytorch/pytorch"><strong>pytorch/pytorch:2.7.0-cuda12.8-cudnn8-devel-ubuntu20.04</strong></a></td>
<td>Python + PyTorch 2.0 + pre-installed CUDA</td>
<td>DL training/inference on GPU - avoids manual CUDA/cuDNN configurations. However, it also includes various libs (image ~&gt;10GB).</td>
</tr>
<tr>
<td><a href="https://github.com/OSGeo/gdal/pkgs/container/gdal"><strong>ghcr.io/osgeo/gdal:ubuntu-full-3.12.0</strong></a></td>
<td>Ubuntu + pre-compiled GDAL (full drivers)</td>
<td>Intensive GIS/RS pipelines.</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/jupyter/scipy-notebook"><strong>jupyter/scipy-notebook</strong></a></td>
<td>Python with Jupyter Notebook + SciPy stack</td>
<td>Ready-to-use notebook environments (CPU). Includes numpy, pandas, matplotlib, etc. Useful for interactive development, also on cloud (e.g., JupyterHub Docker Stacks).</td>
</tr>
</tbody></table>
</div></figure><p>I don&#39;t exclude that there are</p>
<h4 id="gpu-management">GPU Management</h4>
<p>In on-prem deployment, enable GPU runtime with <code>--gpus all</code> on <code>Docker run</code> (using NVIDIA runtimes). In Kubernetes, use NVIDIA device plugins. If the host does not have a GPU, it will be sufficient for the image to still contain the correct libraries, and we can execute on CPU without errors, or delegate to Colab for GPU execution.</p>
<h3 id="24-managing-passwords-and-configurations">2.4 Managing &quot;passwords&quot; and configurations</h3>
<p>Today, the word AI often calls for the word <strong>API</strong>, and consequently also <strong>credentials (or config)</strong> for services (e.g., Mapbox token, OpenAI key, database URL). It is fundamental <strong>not to embed</strong> these values <strong>in the source code</strong>, but to <strong>use config systems</strong>.</p>
<h4 id="env-approach-local">.env approach (local)</h4>
<p>The simplest and most functional approach is certainly to use the classic <code>.env</code> file, which means putting the keys in a <code>.env</code> file excluded from git commands, and then loading it with <a href="https://pypi.org/project/python-dotenv/">python-dotenv</a>.</p>
<p>For <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration">example</a>, the <code>config.py</code> file could be written as:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> BaseSettings  

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Settings</span>(<span class="hljs-title class_ inherited__">BaseSettings</span>):  
    openai_api_key: <span class="hljs-built_in">str</span>  
    db_url: <span class="hljs-built_in">str</span>  
    debug: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>  
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Config</span>:  
    env_file = <span class="hljs-string">&quot;.env&quot;</span> <span class="hljs-comment"># reads variables from .env  </span>
    env_prefix = <span class="hljs-string">&quot;MYAPP_&quot;</span> <span class="hljs-comment"># optional: prefix required in env vars  </span>
settings = Settings()</code></pre><p>This code uses <strong>Pydantic BaseSettings</strong>. Using this code, at each startup, the application would read environment variables (or from the .env file) and build a settings object.</p>
<p>This offers the advantage of <strong>type validation</strong> (e.g., if db_url must be a URL, Pydantic can validate it). Defaults can be defined, and Pydantic automatically converts types (int, bool, etc.) and handles nested configurations. Pydantic is excellent when <em>&quot;the configuration structure is relatively simple and based on env var/.env&quot;</em>, i.e., with few main parameters.</p>
<p>Furthermore, by integrating it with FastAPI, settings can be used as dependencies.</p>
<h4 id="yamlini-file-approach-other-classes">YAML/INI file approach + other classes</h4>
<p>In larger projects or with many configurations, using YAML or TOML files for different environments can be organized. For example, a schema:</p>
<pre><code class="hljs language-1c">config/
<span class="hljs-string">|_default.yaml  </span>
<span class="hljs-string">|_dev.yaml  </span>
<span class="hljs-string">|_prod.yaml</span></code></pre><p>Libraries like <strong>Dynaconf</strong> support <em>multi-layer configuration</em> by loading multiple files and merging them based on an environment key.</p>
<p><a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration">Dynaconf</a> allows defining configs in different formats (YAML, TOML, Python) and distinguishes <em>development</em> vs <em>production</em> contexts, in addition to supporting encrypted secrets. It is indicated when <em>&quot;complex, multi-source configurations with clear separation between environments are needed&quot;</em></p>
<p>Alternatively, <strong>Hydra</strong> (by Facebook) allows composing configurations from modular files and overriding them via CLI. Hydra is common in research contexts for managing many parameters (e.g., model architecture, hyperparameters) and varying experiments by simply launching. Hydra automatically creates versioned output directories with the used config.</p>
<p><strong>Practical config tips:</strong> If the app is relatively simple (few parameters and secrets), <strong>Pydantic BaseSettings</strong> offers simplicity and robustness (type-safe). If it grows in complexity (e.g., dozens of entries, multiple files), <strong>Dynaconf</strong> might be useful to avoid boilerplate and manage multiple sources. <strong>Hydra</strong> is excellent if you anticipate many run variations (typical in ML model training), but for a web service, it might be overkill.</p>
<h4 id="secret-management-in-production">Secret Management in Production</h4>
<p>Never commit credentials. In cloud environments, use dedicated services: AWS Secrets Manager, GCP Secret Manager, Hashicorp Vault.</p>
<p>For example, on AWS, you can store the OPENAI_API_KEY and retrieve it dynamically in the ECS container or inject it as an environment variable through the configuration system (like Terraform).</p>
<p>Many CI/CD services (GitHub Actions, GitLab CI) offer an integrated vault to save secrets and make them available as env vars during deployment. Therefore, the recommended pattern is: <strong>locally .env</strong>, in CI/prod <strong>env var</strong> or encrypted config.</p>
<p>In summary, investing time in solid config/secret management ensures that the app can transition from dev to prod without manual code changes, minimizing leak risks (no keys in the repository, please).</p>
<h3 id="25-open-datasets-and-staccog-catalogs-for-disasters">2.5 Open Datasets and STAC/COG Catalogs for Disasters</h3>
<p>Before showcasing the datasets, I&#39;d like to clarify a few points and provide a couple of definitions:</p>
<blockquote>
<p>‚ÄúChip‚Äù = a cutout/tile (‚Äúpatch/tile‚Äù) extracted from a large satellite scene. In practice: instead of using an entire satellite image, it is broken into many smaller squares, each used as a training example.
For more information on satellites, I refer you to the article <a href="../../../blog/it/geodata/">What data do satellites record?</a>, where I provided a detailed overview of satellite data types.</p>
</blockquote>
<p>For <strong>Disaster Intelligence</strong> projects, drawing from updated open datasets is crucial. Fortunately, between 2022-2025, <strong>STAC</strong> (SpatioTemporal Asset Catalog) catalogs and specialized datasets have increased. Here is a selection of <em>state-of-the-art</em> datasets and resources:</p>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th>Dataset / Catalog</th>
<th>Data (type and resolution)</th>
<th>Coverage/Size</th>
<th>Task / Usage</th>
<th>Source (link)</th>
</tr>
</thead>
<tbody><tr>
<td><strong><a href="https://arxiv.org/abs/1911.09296">xBD</a> / <a href="https://xview2.org/">xView2</a></strong> (2019)</td>
<td>Maxar pre- and post-event satellite imagery (RGB, ~0.3 m/px, 1024√ó1024 tile); building annotations + damage class</td>
<td><a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20images%20capture%2019%20natural,from%20all%20over%20the%20world">19 events of 5 different types</a> (earthquakes, hurricanes, fires); <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20data%20used%20for%20the,resolution%20color%20images">850k annotated buildings over ~45k km¬≤</a>; 9k pre/post image pairs for training</td>
<td><strong>Building Damage Assessment</strong> - building segmentation and damage classification (none, minor, moderate, major, destroyed)</td>
<td><a href="https://xview2.org/">DIU xView2</a> - <a href="https://arxiv.org/abs/1911.09296">xBD Paper</a>)</td>
</tr>
<tr>
<td><strong><a href="https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html">C2SMS Floods</a></strong> (ended in 2023)</td>
<td>Sentinel-1 (SAR) + Sentinel-2 (Optical) co-registered, 512√ó512 px; binary masks with water (flood vs permanent water)</td>
<td>~900 chip pairs from 18 global flood events</td>
<td><strong><a href="https://github.com/microsoft/PlanetaryComputerExamples/blob/main/competitions/s1floods/benchmark_tutorial.ipynb">Flood segmentation (multimodal)</a></strong>: training models to detect floodwater by combining SAR and optical data</td>
<td><a href="https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf#:~:text=The%20C2S,oz32gz">Microsoft/Cloud to Street</a></td>
</tr>
<tr>
<td><a href="https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1"><strong>Sen1Floods11</strong></a> (2020)</td>
<td>Sentinel-1 GRD (SAR) chips 512√ó512 px; binary water mask</td>
<td>4,831 chips from 11 flood events across 5 continents (including Japan Tsunami 2011, Harvey 2017, etc.)</td>
<td><strong>Flood segmentation (SAR)</strong> - benchmarking on radar only (thus robust even with clouds)</td>
<td>Cloud to Street - <a href="https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1">IEEE Paper</a> or <a href="https://github.com/cloudtostreet/Sen1Floods11">Github</a></td>
</tr>
</tbody></table>
</div></figure><p>| <strong>UN Flood Extents</strong> (2023) | Sentinel-1 Raster ($\sigma_0$ backscatter, <del>10 m) with vector flood delineation polygons | Recent global events - data released via <a href="https://unosat.org/">UNOSAT</a>. The number of rasters varies | <strong>Rapid Flood Mapping</strong> - water segmentation via automated pipelines (Google, UN) | UNOSAT|
| <a href="https://xpress.maxar.com/?lat=0&lon=0&zoom=3.0"><strong>Maxar Open Data</strong></a> (2017-today) | High-resolution optical satellite imagery (30-50 cm) pre and post-disaster (GeoTIFF) | $&gt;100$ global events (Nepal Earthquake 2015, Beirut Explosion 2020, Libya Flood 2023, etc.) - coverage varies per event (tens of images each) | <strong>Visual Damage Mapping</strong> - rapidly provides post-event CC BY 4.0 imagery for humanitarian uses, e.g., mapping collapsed buildings | Maxar Open Data Program (AWS OpenData) (includes STAC index on registry.opendata.aws) |
| <a href="https://emergency.copernicus.eu/"><strong>Copernicus EMS</strong></a> (2015-today) | <em>Analyst-derived</em> products from imagery (GeoTIFF and shapefile): e.g., polygons of destroyed buildings, flood extent, damage grade maps (EMS-98) | Emergency activations not only in Europe but worldwide (thousands of maps for earthquakes, floods, fires) - resolution depends on imagery (Sentinel-2 10 m, Pl√©iades 0.5 m, etc.) | <strong>Rapid Mapping</strong> - official results from analysts in a few hours/days (ground truth useful for model training or verification) | <a href="https://mapping.emergency.copernicus.eu/">Copernicus Emergency Mgt Service</a> (public downloads per activation) |
| [<strong>Landslide4Sense / GDCLD</strong>](<a href="https://essd.copernicus.org/articles/16/4817/2024/#">https://essd.copernicus.org/articles/16/4817/2024/#</a>:</del>:text=globally%20distributed%2C%20event,art%20semantic%20segmentation%20algorithms.%20These) (2024) | HR multi-source imagery (PlanetScope <del>3 m, Gaofen-6 2 m, UAV ~0.1 m) with landslide masks (pixel polygons) | 9 global seismic events (Nepal 2015, Amatrice 2016, etc.) with various geological contexts; ~60,000 labeled landslides. + Test on 1 rainfall event. | <strong>Landslide detection</strong> - landslide segmentation in mountainous contexts. Allows training robust models on global datasets. | <em>Globally Distributed Coseismic Landslide Dataset</em> - [DOI Zenodo] [(Fang et al, ESSD 2024)](<a href="https://essd.copernicus.org/articles/16/4817/2024/#">https://essd.copernicus.org/articles/16/4817/2024/#</a>:</del>:text=globally%20distributed%2C%20event,art%20semantic%20segmentation%20algorithms.%20These) |
| <a href="https://github.com/microsoft/PlanetaryComputerExamples"><strong>Planetary Computer STAC</strong></a> (ongoing) | Cloud catalog of <del>100 environmental/EO datasets: Sentinel-1, Sentinel-2, Landsat, MODIS, NAIP collections, etc., with STAC API and Azure storage for on-demand tiles | Global or national coverage depending on the dataset. Examples: Sentinel-2 L2A global (2017-2025), NAIP USA (aerial 60 cm). | <strong>Base data hub</strong> - standardized access to open geo imagery and data for custom analytics (subset, mosaic, etc.). Allows spatial and temporal queries via API. | Microsoft Planetary Computer - [[Docs](<a href="https://github.com/microsoft/PlanetaryComputerExamples#">https://github.com/microsoft/PlanetaryComputerExamples#</a>:</del>:text=If%20you%27re%20viewing%20this%20repository,quickstarts%2C%20dataset%20examples%2C%20and%20tutorials)](free API key required for high throughput) |
| <strong>NOAA NGS Aerial</strong> (various) | Post-event nadir oblique aerial imagery (<del>10-20 cm/px) | USA (coasts and areas affected by hurricanes, tornadoes, fires). [Example](<a href="https://storms.ngs.noaa.gov/storms/ian/index.html#">https://storms.ngs.noaa.gov/storms/ian/index.html#</a>:</del>:text=Hurricane%20IAN%20Imagery%20Hurricane%20IAN,by%20the%20NOAA%20Remote): <del>15k aerial photos after Hurricane Ian 2022 Florida. | <strong>Damage inspection</strong> - used for visual damage assessment immediately after events (also input for CV - e.g., damaged roof detection). | [NOAA NGS Emergency Response Imagery](<a href="https://storms.ngs.noaa.gov/storms/ian/index.html#">https://storms.ngs.noaa.gov/storms/ian/index.html#</a>:</del>:text=Hurricane%20IAN%20Imagery%20Hurricane%20IAN,by%20the%20NOAA%20Remote) (data downloadable via API) |
| <a href="https://esri-disasterresponse.hub.arcgis.com/pages/imagery"><strong>Planet Disaster Data</strong></a> (ongoing) | PlanetScope (<del>3 m) and SkySat (</del>0.5 m) imagery with temporary access for eligible entities (upon request/approval) | Global, selective. Variable coverage: may include multiple acquisitions in the days/weeks post-event, depending on the event and availability. | <strong>Rapid Monitoring</strong> - high-frequency (daily) optical imagery for monitoring disaster evolution. Useful for before/after time series not available elsewhere. | Planet Disaster Data: access for eligible entities via request/DRP program/partners |</p>
<p>These datasets allow training and validating models for <em>damage assessment</em>, <em>flood mapping</em>, <em>change detection</em>, etc., leveraging real data from past events. We note the trend towards multi-modal datasets (SAR+optical combined), multi-event, and with <strong>rich labeling</strong> (not just binary, but damage grades, land cover type, etc.).</p>
<p>For example, xBD remains the benchmark for damage detection with its unprecedented <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20data%20used%20for%20the,resolution%20color%20images">quality</a> and <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20images%20capture%2019%20natural,from%20all%20over%20the%20world">coverage</a>.</p>
<h4 id="active-stac-catalogs">Active STAC Catalogs</h4>
<p>The aforementioned Planetary Computer and Radiant MLHub offer uniform APIs for searching data by area and date. For example, with <strong>pystac-client</strong> we can query Planetary Computer:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> pystac_client <span class="hljs-keyword">import</span> Client  
catalog = Client.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;https://planetarycomputer.microsoft.com/api/stac/v1&quot;</span>)  
search = catalog.search(
    collections=[<span class="hljs-string">&quot;sentinel-2-l2a&quot;</span>],
    intersects={<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;Point&quot;</span>, <span class="hljs-string">&quot;coordinates&quot;</span>: [<span class="hljs-number">30.5</span>,<span class="hljs-number">50.5</span>]},  
    datetime=<span class="hljs-string">&quot;2023-06-01/2023-06-30&quot;</span>
    )  
items = <span class="hljs-built_in">list</span>(search.get_items())
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Found <span class="hljs-subst">{<span class="hljs-built_in">len</span>(items)}</span> Sentinel-2 images in June 2023 in the AOI.&quot;</span>)  
<span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items[:<span class="hljs-number">5</span>]:  
  <span class="hljs-built_in">print</span>(item.<span class="hljs-built_in">id</span>, item.assets[<span class="hljs-string">&quot;B04&quot;</span>].href) <span class="hljs-comment"># prints link to band 4 (red)</span></code></pre><p><em>Snippet 01: Example of using pystac-client to search for Sentinel-2 images within an AOI and data period. It allows us to obtain URLs (often signed) to directly access the COG file, or download subsets.</em></p>
<p>Similarly, Radiant MLHub provides SDKs and APIs for downloading ML datasets. For example, with the radiant_mlhub package, we can query collections by name and download annotations.</p>
<h4 id="other-notable-datasets-2025">Other Notable Datasets 2025</h4>
<p>SpaceNet Challenges (1-8) have produced open datasets on building footprints, road network extraction, multi-year building change detection (SpaceNet 7), etc. Although somewhat dated, they remain valuable for benchmarking. For example, SpaceNet8 (2022) provides pre/post-flood PlanetScope images + flood masks and building footprints, useful for multimodal approaches (similar to xBD but for floods). Additionally, datasets like <a href="https://www.nature.com/articles/s41597-023-02847-z"><strong>CAS Landslide</strong></a> and <a href="https://www.mdpi.com/2072-4292/16/11/1886"><strong>DMLD</strong></a> (China) offer thousands of examples of landslides mapped in different regions.</p>
<p>The open community is also moving towards <strong>geospatial foundation models</strong>: e.g., <a href="https://bigearth.net/"><em>BigEarthNet</em></a> (520k Sentinel-2 patches labeled with land cover) is used to pre-train ResNet/ViT-like models on satellite data and is directly available on <a href="https://www.tensorflow.org/datasets/catalog/bigearthnet?hl=it">Tensorflow</a>.</p>
<h3 id="26-geospatial-and-remote-sensing-libraries">2.6 Geospatial and Remote Sensing Libraries</h3>
<p>The geospatial world in Python is layered: I/O, geometries, analysis, visualization, and serving. Below are the most common libraries with a practical description.</p>
<ul>
<li><strong>GDAL (osgeo)</strong>: is the base for reading/writing rasters and vectors in many formats; it includes CLIs like <code>gdal_translate</code> and <code>gdalwarp</code>. In Python, it&#39;s usually accessed using rasterio or fiona.</li>
<li><strong>rasterio</strong>: reading/writing (I/O) of rasters (GeoTIFF, COG) and basic operations like clipping, masking with polygons, reprojecting, and windowed reading. Useful when you need to process many files one at a time without loading them entirely into memory.</li>
<li><strong>fiona</strong>: vector I/O (Shapefile, GeoJSON, GeoPackage). Reads/writes features without going through GeoPandas.</li>
<li><strong>GeoPandas</strong>: manages DataFrames with geometries. In particular, it can perform overlays, spatial joins, buffers, and dissolves. Works well on small/medium datasets.</li>
<li><strong>Shapely 2.0</strong>: geometric operations (intersection, distance, buffer). Does not manage CRS; to be combined with pyproj/GeoPandas.</li>
<li><strong>pyproj</strong>: CRS definition and coordinate transformations. Base for reliable rejections.</li>
<li><strong>xarray</strong>: N-D arrays with labels (dimensions and coordinates). Useful for multi-band time series or image stacks; with dask, it manages large datasets lazily.</li>
<li><strong>rioxarray</strong>: adds CRS and transformations to xarray and allows geospatial raster I/O. Useful if you work with DataArray and want to clip/reproject without losing georeferencing.</li>
<li><strong>rio-tiler</strong>: reads COG even via HTTP and generates tiles or previews. To be used when you need to serve maps or images via API.</li>
<li><strong>folium</strong>: creates static HTML maps based on Leaflet. Ideal for reports or quick sharing.</li>
<li><strong>ipyleaflet</strong>: Leaflet widget for Jupyter with live callbacks. Useful for drawing polygons, clicking, and getting input in Python.</li>
<li><strong>leafmap</strong>: high-level API for maps with less boilerplate; also integrates services like Google Earth Engine.</li>
<li><strong>TorchGeo</strong>: geospatial datasets and samplers for PyTorch, with utilities for patching and training.</li>
<li><strong>scikit-image</strong>: filters and operations on arrays (denoise, thresholds, morphology). Not geospatial, but convenient for pre/post-processing.</li>
<li><strong>OpenCV</strong>: optimized CV primitives (resize, warp, feature). Practical choice when speed is needed.</li>
<li><strong>pySAR / Gamma / SNAP</strong>: tools for SAR. SNAP covers calibration, speckle, and terrain correction; Gamma is more specialized (interferometry).</li>
<li><strong>PDAL / pylas</strong>: LiDAR point cloud management. PDAL for pipelines and filters, pylas for lightweight I/O.</li>
<li><strong>PostGIS</strong>: spatial database for large vector volumes; spatial indexes and queries when you no longer fit in memory.</li>
</ul>
<h4 id="when-to-use-what">When to use what</h4>
<p>Let&#39;s summarize:</p>
<ul>
<li>For simple rasters or file-by-file batches, <em>rasterio</em> + <em>numpy</em> is sufficient.</li>
<li>For large stacks or time series: <em>xarray</em> + dask (with <em>rioxarray</em> if CRS is needed).</li>
<li>For small/medium vectors: <em>GeoPandas</em>; for large ones: <em>PostGIS</em>.</li>
<li>For zonal statistics or clipping rasters with polygons: <em>rasterstats</em> (based on rasterio).</li>
<li>For tile servers or web maps: <em>rio-tiler</em> for the backend, <em>folium/ipyleaflet</em> for the front-end.</li>
</ul>
<h4 id="example-pipeline-optical-sar">Example Pipeline (Optical + SAR)</h4>
<p>To identify flooded areas with Sentinel-1 and Sentinel-2:</p>
<ol>
<li><strong>Data loading:</strong> download assets (pystac-client) and open with rasterio.</li>
<li><strong>Pre-process:</strong> align CRS and resolution with <code>rasterio.warp.reproject</code>.</li>
<li><strong>Stack &amp; filter:</strong> normalize bands and create a stack (NIR, Red, SAR); filter SAR speckle.</li>
<li><strong>Inference:</strong> pass the stack to the segmentation model.</li>
<li><strong>Post-process:</strong> vectorize the mask with <code>rasterio.features</code> and save with GeoPandas.</li>
<li><strong>Visualize:</strong> overlay on map with folium.</li>
</ol>
<p><img src="../../../Assets/geospatial_pipeline.svg" alt="Geospatial pipeline">
<em>Figure: synthetic diagram of the geospatial pipeline.</em></p>
<p>This pipeline uses <strong>rasterio</strong> for I/O, <strong>numpy/scikit-image</strong> for filters, <strong>PyTorch</strong> for inference, and <strong>GeoPandas</strong> for vector output. For large-scale operations, it&#39;s advisable to read with rioxarray + dask and process by tile.</p>
<h3 id="27-project-templates-reference-repositories-and-best-practices">2.7 Project Templates, Reference Repositories, and Best Practices</h3>
<p>To build a <strong>production-ready</strong> stack, it&#39;s useful to study existing open-source projects that address similar problems. Here are <strong>10 reference repositories and templates (2025)</strong> from which to draw lessons, with motivation:</p>
<ul>
<li><p><a href="https://www.langchain.com/"><strong>LangChain</strong></a> and <a href="https://datapizza.tech/en/ai-framework/"><strong>Datapizza-AI</strong></a>: two frameworks for agents and LLM apps. <strong>Why:</strong> These are two de facto standard ways to orchestrate LLMs with memory, tools, and data augmentation. Study how LangChain structures modular code (chains, agents), its <strong>pyproject.toml</strong> with <em>uv</em> (they maintain it with uv and release frequently, ~14k commits), and how they manage integrations (e.g., vectorstores). Additionally, I recommend studying how to create <a href="https://youtu.be/Mmu5f7jLBds?si=eJrqIwp62B8bXyko">very simple applications</a> with Datapizza-AI.</p>
</li>
<li><p><strong>LlamaIndex (GPT Index)</strong>: a framework for building RAG systems. <strong>Why:</strong> It provides patterns for indexing documents and images and performing efficient retrieval for LLMs. It has components for spatial queries (e.g., you could index coordinates and perform filtering). You can see how it implements various Storage options and <strong>readers for heterogeneous data</strong>. <a href="https://github.com/run-llama/llama_index/releases">Very active repo</a> (0.14.x releases in 2025).</p>
</li>
<li><p><a href="https://haystack.deepset.ai/"><strong>Haystack</strong></a> - <a href="https://github.com/deepset-ai/haystack">GitHub</a>. <strong>Why:</strong> It allows building end-to-end open-source pipelines with support for document retrieval, generation, and even agents. They have a design oriented towards <strong>producing APIs</strong> (e.g., GraphQL) and composable YAML pipelines. I recommend studying how they define components (Reader, Retriever, Generator) and config management. Very production-friendly (used in companies).</p>
</li>
<li><p><a href="https://www.osgeo.org/projects/torchgeo/"><strong>TorchGeo</strong></a>. <strong>Why:</strong> This is an example of a well-designed <em>domain-specific</em> library: you can see the structure (modularity in datasets, samplers, models), in-depth tests, and use of CI (they have integration with OSGeo, etc.). Actively maintained by Microsoft Research. Useful for understanding best practices in <em>packaging</em> geospatial models (e.g., in HuggingFace Hub).</p>
</li>
<li><p><strong>Segment Anything (Meta AI)</strong> - <a href="https://github.com/facebookresearch/sam3">GitHub</a>. <strong>Why:</strong> Although not originally designed for the geospatial world, <em>SAM</em> has had a strong impact in the Remote Sensing world as well (used for segmenting generic objects in satellite images). In my opinion, it&#39;s important to study how to use it, leveraging the notebooks in the repo.</p>
</li>
<li><p><strong>xView2 First Place Solution</strong> - (<a href="https://github.com/vdurnov/xview2_1st_place_solution">DIUx-xView GitHub</a>). <strong>Why:</strong> It contains the code of a winning team for building damage analysis. It allows seeing a complete project: data preprocessing from xBD, model (U-Net ensemble), inference and post-processing, and as final packaging, they have a script for inference on image directories. Although it&#39;s a 2019 project, many principles remain valid. PyTorch structure with JSON config, separate training and inference. Useful for understanding <em>tricks</em> to improve performance (e.g., split model into two stages for localization vs classification).</p>
</li>
<li><p><a href="https://rastervision.io/"><strong>Raster Vision</strong></a>. <strong>Why:</strong> General framework for CV on geospatial images, written with a plugin architecture. It allows defining training pipelines with JSON config. I recommend studying its <em>config-driven design</em>, the use of Docker to isolate environments, and the components (scenario, dataset, backend). It demonstrates how a tool can abstract common tasks (clip, retiling, train, predict) for various problems.</p>
</li>
<li><p><a href="https://cookiecutter-data-science.drivendata.org/"><strong>Cookiecutter MLOps</strong></a> - (DrivenData Cookiecutter Data Science 2.0) - <a href="https://github.com/drivendata/cookiecutter-data-science">GitHub</a>. <strong>Why:</strong> A project template that emphasizes the lifecycle of a complete DS project: data, modeling, deployment. Although more data-science oriented, it provides a reference on how to structure repositories with folders for raw data, intermediate data, saved models, reports, etc., and how to document decisions. This is useful for not forgetting anything (e.g., it also includes CI setup and docs scaffolding).</p>
</li>
</ul>
<p>In addition to these, monitor tech company repositories on <em>disaster response</em>, such as <a href="https://sites.research.google/gr/floodforecasting/"><strong>Google&#39;s flood forecasting</strong></a>, which has a proprietary open-source dataset, in addition to several detailed publications, or also <a href="https://unosat.org/products/"><strong>UNOSAT open products</strong></a>. Furthermore, I would like to remind you that communities like <strong>OSGeo</strong> and <a href="https://learning.hotosm.org/course/introduction-to-open-mapping"><strong>HOT (Humanitarian OpenStreetMap Team)</strong></a> exist for automatic mapping tools.</p>
<h2 id="conclusion-an-ideal-repository-aigeo">Conclusion: An Ideal Repository (AI+Geo)</h2>
<p>Drawing inspiration from the above, we therefore recommend:</p>
<ul>
<li><p><strong>Modularize</strong> by components: e.g., <code>src/models/</code> for ML models, <code>src/data/</code> with all loading modules, <code>src/utils/geo.py</code> for geospatial functions (buffer, reproject, etc.), src/api/ for FastAPI code.</p>
</li>
<li><p><strong>Configs outside the code</strong>: use Hydra or Pydantic as discussed. Provide example configs for datasets (especially if open) so that new contributors can easily replicate results by pointing to the correct paths.</p>
</li>
<li><p><strong>Makefile/CLI</strong>: provide quick commands: <code>make data</code> (downloads open dataset), <code>make train</code> (starts training if planned), <code>make infer</code> (performs inference on example input), <code>make serve</code> (launches API). This reduces problems in executing parts of the project.</p>
</li>
<li><p><strong>Comprehensive README</strong>: explain architecture, how to set up env, run tests, etc.</p>
</li>
<li><p><strong>Test &amp; CI</strong>: include some tests also for geospatial functions (e.g., a test that creates a geometry, applies a buffer, and compares the expected area). Or, a test that the loaded model produces output with correct dimensions on a dummy input.</p>
</li>
</ul>
<p>Finally, general <strong>best practices</strong>: version models (use <code>semver</code> or <code>data</code> to know with which data they were trained), keep track of <em>cron jobs</em> if any (e.g., daily data update), and include monitoring tools if in production (Prometheus exporter for resources and maybe a counter for how many times a certain inference function is called).</p>
<h3 id="how-to-get-started-immediately">How to get started immediately</h3>
<p>To get started immediately, here&#39;s a quick checklist of the first actions to take:</p>
<ul>
<li><strong>1. Prepare the development environment:</strong> Install Anaconda/Miniconda or Poetry on your operating system (if Linux, even better XD). Update NVIDIA drivers if you plan to test on a remote GPU (Colab). Create a new clean Python 3.11 env.</li>
<li><strong>2. Initialize the project:</strong> Structure folders as described and start a git repo. Create a remote repository (GitHub) and make the first commit with <code>README.md</code> and <code>pyproject.toml/requirements.txt</code>.</li>
<li><strong>3. Configure Dev tools:</strong> Immediately set up pre-commit and Black/Ruff. Run <code>pre-commit install</code> so every file you create will already be formatted and linted on commit. This prevents you from accumulating technical style debt.</li>
<li><strong>4. Retrieve sample datasets:</strong> Identify a small event (even just a pre/post image from Maxar Open Data or Copernicus EMS). Download them manually for now (via browser or AWS CLI) and put them in <code>data/raw/</code>. In parallel, request any necessary API keys (e.g., OpenAI API if you intend to use it).</li>
<li><strong>5. Study references:</strong> Dedicate a few hours to reading documentation for 1-2 key tools you&#39;ll use first. For example: basic rasterio syntax (read the official tutorial), basic FastAPI usage (how to define an endpoint and run uvicorn), and if you&#39;re not familiar, the pystac-client guide to understand how to query <a href="https://pystac-client.readthedocs.io/en/stable/usage.html#:~:text=The%20following%20code%20creates%20an,Microsoft%20Planetary%20Computer%20root%20catalog">STAC</a> data. This initial time investment will save you some time later.</li>
<li><strong>6. Set up config variables:</strong> Create an .env.example file with expected keys (e.g., OPENAI_API_KEY=) and add it to git. Copy it to .env and populate for dev. Include in the README how to obtain and set these variables (e.g., link to the OpenAI page to create a key).</li>
<li><strong>7. Plan computational resources:</strong> Since you don&#39;t have a local GPU, decide how to test heavy parts. For example, configure a notebook on Colab with a linked GitHub repo, so you can run inference on a GPU there if needed. Or ensure the code is parameterized to use <code>.to(&#39;cpu&#39;)</code> as default and still functions.</li>
<li><strong>8. Communicate and document:</strong> even if you are the sole developer, get used to maintaining clear changelogs or commit messages. For example, start by writing a first commit &quot;setup environment, add base deps (rasterio, torch, etc.) - verify imports OK&quot;. This will help you track progress and facilitate potential involvement of other collaborators in the future.</li>
</ul>
<p>By following these actions at the beginning, you will lay the groundwork to become a good GeoAI Engineer: a ready environment, a repo configured with quality gates, and data available to start developing the <strong>core functionality</strong>. From then on, you can iterate by gradually building the pipeline, knowing you have a professional <em>scaffolding</em> around (tests, lint, containers) that supports you in producing reliable and reproducible code.</p>
<h2 id="bibliography">Bibliography</h2>
<h3 id="models-papers-and-architectures">Models, papers, and architectures</h3>
<ul>
<li><strong>xView2 AI Challenge (IBM, U-Net for damage assessment)</strong>: <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge">https://www.ibm.com/think/insights/the-xview2-ai-challenge</a></li>
<li><strong>SegFormer (paper)</strong>: <a href="https://arxiv.org/abs/2105.15203">https://arxiv.org/abs/2105.15203</a></li>
<li><strong>Open-CD (change detection, paper)</strong>: <a href="https://arxiv.org/abs/2407.15317v1">https://arxiv.org/abs/2407.15317v1</a></li>
<li><strong>Spatial-RAG (paper)</strong>: <a href="https://arxiv.org/html/2502.18470v5">https://arxiv.org/html/2502.18470v5</a></li>
</ul>
<h3 id="geoai-libraries-and-frameworks">GeoAI Libraries and Frameworks</h3>
<ul>
<li><strong>TorchGeo (GitHub)</strong>: <a href="https://github.com/torchgeo/torchgeo">https://github.com/torchgeo/torchgeo</a></li>
<li><strong>TorchGeo (OSGeo project)</strong>: <a href="https://www.osgeo.org/projects/torchgeo/">https://www.osgeo.org/projects/torchgeo/</a></li>
<li><strong>Segment Anything - SAM3 (GitHub)</strong>: <a href="https://github.com/facebookresearch/sam3">https://github.com/facebookresearch/sam3</a></li>
<li><strong>Raster Vision</strong>: <a href="https://rastervision.io/">https://rastervision.io/</a></li>
<li><strong>xView2 First Place Solution (GitHub)</strong>: <a href="https://github.com/vdurnov/xview2_1st_place_solution">https://github.com/vdurnov/xview2_1st_place_solution</a></li>
</ul>
<h3 id="services-and-apis">Services and APIs</h3>
<ul>
<li><strong>FastAPI (docs)</strong>: <a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a></li>
</ul>
<h3 id="agents-and-rag-frameworks">Agents and RAG frameworks</h3>
<ul>
<li><strong>LangChain</strong>: <a href="https://www.langchain.com/">https://www.langchain.com/</a></li>
<li><strong>Datapizza-AI framework</strong>: <a href="https://datapizza.tech/en/ai-framework/">https://datapizza.tech/en/ai-framework/</a></li>
<li><strong>Datapizza-AI: simple apps (YouTube video)</strong>: <a href="https://youtu.be/Mmu5f7jLBds?si=eJrqIwp62B8bXyko">https://youtu.be/Mmu5f7jLBds?si=eJrqIwp62B8bXyko</a></li>
<li><strong>LlamaIndex (releases)</strong>: <a href="https://github.com/run-llama/llama_index/releases">https://github.com/run-llama/llama_index/releases</a></li>
<li><strong>Haystack (docs)</strong>: <a href="https://haystack.deepset.ai/">https://haystack.deepset.ai/</a></li>
<li><strong>Haystack (GitHub)</strong>: <a href="https://github.com/deepset-ai/haystack">https://github.com/deepset-ai/haystack</a></li>
</ul>
<h3 id="python-tooling-and-quality">Python Tooling and Quality</h3>
<ul>
<li><strong>pip resolver/backtracking (debuglab)</strong>: <a href="https://debuglab.net/2024/01/26/resolving-new-pip-backtracking-runtime-issue/">https://debuglab.net/2024/01/26/resolving-new-pip-backtracking-runtime-issue/</a></li>
<li><strong>Python has too many package managers (blog)</strong>: <a href="https://dublog.net/blog/so-many-python-package-managers/">https://dublog.net/blog/so-many-python-package-managers/</a></li>
<li><strong>uv (docs)</strong>: <a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a></li>
<li><strong>Pixi: fast conda alternative (blog)</strong>: <a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative">https://prefix.dev/blog/pixi_a_fast_conda_alternative</a></li>
<li><strong>Cookiecutter Data Science (docs)</strong>: <a href="https://cookiecutter-data-science.drivendata.org/">https://cookiecutter-data-science.drivendata.org/</a></li>
<li><strong>Cookiecutter Data Science (GitHub)</strong>: <a href="https://github.com/drivendata/cookiecutter-data-science">https://github.com/drivendata/cookiecutter-data-science</a></li>
<li><strong>Mypy/Ruff/Black/Isort combos (Medium)</strong>: <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac">https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac</a></li>
<li><strong>Syrupy (pytest snapshot plugin)</strong>: <a href="https://github.com/syrupy-project/syrupy">https://github.com/syrupy-project/syrupy</a></li>
<li><strong>python-dotenv (PyPI)</strong>: <a href="https://pypi.org/project/python-dotenv/">https://pypi.org/project/python-dotenv/</a></li>
<li><strong>Pydantic BaseSettings vs Dynaconf (article)</strong>: <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration">https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration</a></li>
</ul>
<h3 id="containers-and-base-images">Containers and Base Images</h3>
<ul>
<li><strong>Docker image for geospatial Python app (blog)</strong>: <a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application">https://www.geosynopsis.com/posts/docker-image-for-geospatial-application</a></li>
<li><strong>Python 3.12-slim (Docker Hub)</strong>: <a href="https://hub.docker.com/layers/library/python/3.12-slim/images/">https://hub.docker.com/layers/library/python/3.12-slim/images/</a></li>
<li><strong>Miniconda3 (Docker Hub)</strong>: <a href="https://hub.docker.com/r/continuumio/miniconda3">https://hub.docker.com/r/continuumio/miniconda3</a></li>
<li><strong>micromamba (docs)</strong>: <a href="https://micromamba-docker.readthedocs.io/en/latest/">https://micromamba-docker.readthedocs.io/en/latest/</a></li>
<li><strong>NVIDIA CUDA runtime (Docker Hub)</strong>: <a href="https://hub.docker.com/r/nvidia/cuda">https://hub.docker.com/r/nvidia/cuda</a></li>
<li><strong>PyTorch (Docker Hub)</strong>: <a href="https://hub.docker.com/r/pytorch/pytorch">https://hub.docker.com/r/pytorch/pytorch</a></li>
<li><strong>GDAL container (OSGeo)</strong>: <a href="https://github.com/OSGeo/gdal/pkgs/container/gdal">https://github.com/OSGeo/gdal/pkgs/container/gdal</a></li>
<li><strong>Jupyter scipy-notebook (Docker Hub)</strong>: <a href="https://hub.docker.com/r/jupyter/scipy-notebook">https://hub.docker.com/r/jupyter/scipy-notebook</a></li>
</ul>
<h3 id="stac-and-data-access">STAC and Data Access</h3>
<ul>
<li><strong>pystac-client (usage)</strong>: <a href="https://pystac-client.readthedocs.io/en/stable/usage.html">https://pystac-client.readthedocs.io/en/stable/usage.html</a></li>
<li><strong>Planetary Computer Examples (GitHub)</strong>: <a href="https://github.com/microsoft/PlanetaryComputerExamples">https://github.com/microsoft/PlanetaryComputerExamples</a></li>
<li><strong>Planetary Computer: Sen1Floods11 benchmark notebook</strong>: <a href="https://github.com/microsoft/PlanetaryComputerExamples/blob/main/competitions/s1floods/benchmark_tutorial.ipynb">https://github.com/microsoft/PlanetaryComputerExamples/blob/main/competitions/s1floods/benchmark_tutorial.ipynb</a></li>
</ul>
<h3 id="datasets-benchmarks-and-open-catalogs">Datasets, Benchmarks, and Open Catalogs</h3>
<ul>
<li><strong>xBD dataset (paper)</strong>: <a href="https://arxiv.org/abs/1911.09296">https://arxiv.org/abs/1911.09296</a></li>
<li><strong>xView2 (official site)</strong>: <a href="https://xview2.org/">https://xview2.org/</a></li>
<li><strong>Cloud to Street (CMR)</strong>: <a href="https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html">https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html</a></li>
<li><strong>C2SMS Floods (documentation PDF)</strong>: <a href="https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf">https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf</a></li>
<li><strong>Sen1Floods11 (paper, ResearchGate)</strong>: <a href="https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1">https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1</a></li>
<li><strong>Sen1Floods11 (GitHub)</strong>: <a href="https://github.com/cloudtostreet/Sen1Floods11">https://github.com/cloudtostreet/Sen1Floods11</a></li>
<li><strong>Coseismic landslide dataset (ESSD 2024)</strong>: <a href="https://essd.copernicus.org/articles/16/4817/2024/">https://essd.copernicus.org/articles/16/4817/2024/</a></li>
<li><strong>CAS Landslide dataset (Scientific Data)</strong>: <a href="https://www.nature.com/articles/s41597-023-02847-z">https://www.nature.com/articles/s41597-023-02847-z</a></li>
<li><strong>DMLD dataset (MDPI)</strong>: <a href="https://www.mdpi.com/2072-4292/16/11/1886">https://www.mdpi.com/2072-4292/16/11/1886</a></li>
<li><strong>BigEarthNet (project site)</strong>: <a href="https://bigearth.net/">https://bigearth.net/</a></li>
<li><strong>BigEarthNet (TF Datasets)</strong>: <a href="https://www.tensorflow.org/datasets/catalog/bigearthnet?hl=it">https://www.tensorflow.org/datasets/catalog/bigearthnet?hl=it</a></li>
<li><strong>Google Flood Forecasting</strong>: <a href="https://sites.research.google/gr/floodforecasting/">https://sites.research.google/gr/floodforecasting/</a></li>
<li><strong>Hurricane Ian imagery (NOAA)</strong>: <a href="https://storms.ngs.noaa.gov/storms/ian/index.html">https://storms.ngs.noaa.gov/storms/ian/index.html</a></li>
<li><strong>Esri Disaster Response imagery</strong>: <a href="https://esri-disasterresponse.hub.arcgis.com/pages/imagery">https://esri-disasterresponse.hub.arcgis.com/pages/imagery</a></li>
<li><strong>Maxar Open Data portal</strong>: <a href="https://xpress.maxar.com/?lat=0&lon=0&zoom=3.0">https://xpress.maxar.com/?lat=0&amp;lon=0&amp;zoom=3.0</a></li>
<li><strong>Copernicus Emergency Management Service</strong>: <a href="https://emergency.copernicus.eu/">https://emergency.copernicus.eu/</a></li>
<li><strong>Copernicus Emergency Mapping Service</strong>: <a href="https://mapping.emergency.copernicus.eu/">https://mapping.emergency.copernicus.eu/</a></li>
<li><strong>UNOSAT</strong>: <a href="https://unosat.org/">https://unosat.org/</a></li>
<li><strong>UNOSAT products</strong>: <a href="https://unosat.org/products/">https://unosat.org/products/</a></li>
<li><strong>HOTOSM: introduction to open mapping</strong>: <a href="https://learning.hotosm.org/course/introduction-to-open-mapping">https://learning.hotosm.org/course/introduction-to-open-mapping</a></li>
</ul>
        </div>
      </section>
    </article>
  </main>
  <footer>
    <span data-it="¬© 2026 Mirko Calcaterra. Tutti i diritti riservati."
          data-en="¬© 2026 Mirko Calcaterra. All rights reserved.">
      ¬© 2026 Mirko Calcaterra. Tutti i diritti riservati.
    </span>
  </footer>
  <script>
    const BLOG_LANG_KEY = 'blogLang';
    const BLOG_THEME_KEY = 'blogTheme';
    const CURRENT_LANG = "en";
    const OTHER_LANG = "it";
    const OTHER_LANG_LINK = "../../../blog/it/geoai-startingpoint/index.html";
    (function() {
      const body = document.body;
      const themeToggle = document.querySelector('.theme-toggle');
      const themeThumb = document.querySelector('.theme-toggle .theme-thumb');
      const langBtn = document.querySelector('.lang-btn');
      const tocElement = document.querySelector('.post-toc');
      const tocToggle = tocElement ? tocElement.querySelector('.post-toc__toggle') : null;
      const tocToggleText = tocElement ? tocElement.querySelector('.post-toc__toggle-text') : null;
      const tocTitle = tocElement ? tocElement.querySelector('.post-toc__title') : null;
      const tocLinks = tocElement ? Array.from(tocElement.querySelectorAll('.post-toc__link')) : [];
      const headingEntries = tocLinks
        .map((link) => {
          const id = link.getAttribute('href').slice(1);
          const target = document.getElementById(id);
          return target ? { link, target } : null;
        })
        .filter(Boolean);
      const tocLabels = CURRENT_LANG === 'it'
        ? { title: 'Indice', show: 'Mostra indice', hide: 'Nascondi indice' }
        : { title: 'Table of contents', show: 'Show table of contents', hide: 'Hide table of contents' };
      const tableWrappers = Array.from(document.querySelectorAll('.table-wrapper[data-enhanced-table]'));
      const tableLabels = CURRENT_LANG === 'it'
        ? { expand: 'Apri a schermo intero', close: 'Chiudi' }
        : { expand: 'Open full view', close: 'Close' };
      const codeBlocks = Array.from(document.querySelectorAll('.post-body pre'));
      const codeCopyLabels = {
        it: { copy: 'Copia', copied: 'Copiato!' },
        en: { copy: 'Copy', copied: 'Copied!' },
      };
      let tableOverlay = null;
      let tableOverlayScroll = null;
      let tableOverlayClose = null;
      if (tocTitle) {
        tocTitle.textContent = tocLabels.title;
      }
      if (tocToggleText) {
        tocToggleText.textContent = tocLabels.title;
      }
      let tocCollapsed = false;
      let tocManualOverride = false;
      const tocMediaQuery = window.matchMedia ? window.matchMedia('(max-width: 1024px)') : null;
      function ensureTableOverlay() {
        if (tableOverlay) {
          return;
        }
        tableOverlay = document.createElement('div');
        tableOverlay.className = 'table-overlay';
        tableOverlay.innerHTML =
          '<div class="table-overlay__content">' +
          '<button type="button" class="table-overlay__close">' + tableLabels.close + '</button>' +
          '<div class="table-overlay__scroll"></div>' +
          '</div>';
        body.appendChild(tableOverlay);
        tableOverlayScroll = tableOverlay.querySelector('.table-overlay__scroll');
        tableOverlayClose = tableOverlay.querySelector('.table-overlay__close');
        if (tableOverlayClose) {
          tableOverlayClose.setAttribute('aria-label', tableLabels.close);
          tableOverlayClose.addEventListener('click', closeTableOverlay);
        }
        tableOverlay.addEventListener('click', (event) => {
          if (event.target === tableOverlay) {
            closeTableOverlay();
          }
        });
      }
      function closeTableOverlay() {
        if (!tableOverlay) {
          return;
        }
        tableOverlay.classList.remove('table-overlay--visible');
        body.classList.remove('no-scroll');
        if (tableOverlayScroll) {
          tableOverlayScroll.innerHTML = '';
        }
      }
      function openTableOverlay(wrapper) {
        ensureTableOverlay();
        if (!tableOverlay || !tableOverlayScroll) {
          return;
        }
        tableOverlayScroll.innerHTML = '';
        const table = wrapper.querySelector('table');
        if (table) {
          const clone = table.cloneNode(true);
          const tableSize = table.dataset.tableSize;
          if (tableSize) {
            clone.dataset.tableSize = tableSize;
          }
          tableOverlayScroll.appendChild(clone);
        }
        tableOverlay.classList.add('table-overlay--visible');
        body.classList.add('no-scroll');
        if (tableOverlayClose) {
          tableOverlayClose.focus();
        }
      }
      function enhanceTables() {
        if (!tableWrappers.length) {
          return;
        }
        tableWrappers.forEach((wrapper) => {
          if (wrapper.dataset.enhanced === 'true') {
            return;
          }
          const table = wrapper.querySelector('table');
          if (!table) {
            return;
          }
          const headerCells = table.querySelectorAll('thead th');
          const referenceCells = headerCells.length ? headerCells : table.querySelectorAll('tr:first-child > *');
          const columnCount = referenceCells.length;
          let tableSize = '';
          if (columnCount >= 6) {
            tableSize = 'wide';
          } else if (columnCount >= 4) {
            tableSize = 'medium';
          }
          if (tableSize) {
            wrapper.setAttribute('data-table-size', tableSize);
            table.dataset.tableSize = tableSize;
          }
          const expandBtn = document.createElement('button');
          expandBtn.type = 'button';
          expandBtn.className = 'table-wrapper__expand';
          expandBtn.innerHTML = '<span aria-hidden="true">üîç</span> ' + tableLabels.expand;
          expandBtn.setAttribute('aria-label', tableLabels.expand);
          expandBtn.addEventListener('click', () => openTableOverlay(wrapper));
          wrapper.appendChild(expandBtn);
          wrapper.dataset.enhanced = 'true';
        });
      }
      function fallbackCopy(text) {
        const textarea = document.createElement('textarea');
        textarea.value = text;
        textarea.setAttribute('readonly', '');
        textarea.style.position = 'fixed';
        textarea.style.opacity = '0';
        textarea.style.left = '-9999px';
        document.body.appendChild(textarea);
        textarea.select();
        let successful = false;
        try {
          successful = document.execCommand('copy');
        } catch (error) {
          successful = false;
        }
        textarea.remove();
        return successful;
      }
      function showCopyFeedback(button, labels) {
        if (button._copyTimeout) {
          clearTimeout(button._copyTimeout);
        }
        const labelEl = button.querySelector('.code-copy-btn__text');
        button.classList.add('code-copy-btn--copied');
        if (labelEl) {
          labelEl.textContent = labels.copied;
        }
        button._copyTimeout = window.setTimeout(() => {
          button.classList.remove('code-copy-btn--copied');
          if (labelEl) {
            labelEl.textContent = labels.copy;
          }
        }, 2000);
      }
      function enhanceCodeBlocks() {
        if (!codeBlocks.length) {
          return;
        }
        const labels = codeCopyLabels[CURRENT_LANG] || codeCopyLabels.en;
        codeBlocks.forEach((pre) => {
          if (pre.dataset.copyEnhanced === 'true') {
            return;
          }
          const code = pre.querySelector('code');
          if (!code) {
            return;
          }
          const button = document.createElement('button');
          button.type = 'button';
          button.className = 'code-copy-btn';
          button.setAttribute('aria-label', labels.copy);
          button.innerHTML =
            '<span class="code-copy-btn__icon" aria-hidden="true">üìã</span>' +
            '<span class="code-copy-btn__text">' + labels.copy + '</span>';
          button.addEventListener('click', async () => {
            const text = (code.textContent || '').replace(/s+$/, '');
            if (!text) {
              return;
            }
            let copied = false;
            if (navigator.clipboard && typeof navigator.clipboard.writeText === 'function') {
              try {
                await navigator.clipboard.writeText(text);
                copied = true;
              } catch (error) {
                copied = false;
              }
            }
            if (!copied) {
              copied = fallbackCopy(text);
            }
            if (copied) {
              showCopyFeedback(button, labels);
            }
          });
          pre.appendChild(button);
          pre.dataset.copyEnhanced = 'true';
        });
      }
      function setTocCollapsed(collapsed, { manual = false } = {}) {
        if (!tocElement) {
          return;
        }
        tocCollapsed = Boolean(collapsed);
        if (manual) {
          tocManualOverride = true;
        }
        tocElement.classList.toggle('post-toc--collapsed', tocCollapsed);
        tocElement.setAttribute('data-collapsed', tocCollapsed ? 'true' : 'false');
        if (tocToggle) {
          tocToggle.setAttribute('aria-expanded', tocCollapsed ? 'false' : 'true');
          tocToggle.setAttribute('aria-label', tocCollapsed ? tocLabels.show : tocLabels.hide);
        }
      }
      function initToc() {
        if (!tocElement) {
          return;
        }
        if (tocToggle) {
          tocToggle.addEventListener('click', () => {
            setTocCollapsed(!tocCollapsed, { manual: true });
          });
        }
        if (tocMediaQuery) {
          const handleMediaChange = (event) => {
            if (tocManualOverride) {
              return;
            }
            setTocCollapsed(event.matches);
          };
          if (typeof tocMediaQuery.addEventListener === 'function') {
            tocMediaQuery.addEventListener('change', handleMediaChange);
          } else if (typeof tocMediaQuery.addListener === 'function') {
            tocMediaQuery.addListener(handleMediaChange);
          }
          setTocCollapsed(tocMediaQuery.matches);
        } else {
          setTocCollapsed(false);
        }
      }
      const storedTheme = (localStorage.getItem(BLOG_THEME_KEY) || '').toLowerCase();
      const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
      const initialTheme = storedTheme === 'light' ? 'light' : (storedTheme === 'dark' ? 'dark' : (prefersDark ? 'dark' : 'light'));
      let activeLink = null;
      let ticking = false;
      function applyTheme(theme) {
        const resolved = theme === 'dark' ? 'dark' : 'light';
        body.setAttribute('data-theme', resolved);
        if (themeToggle) {
          themeToggle.classList.toggle('active', resolved === 'dark');
        }
        if (themeThumb) {
          themeThumb.textContent = resolved === 'dark' ? 'üåô' : '‚òÄÔ∏è';
        }
        localStorage.setItem(BLOG_THEME_KEY, resolved);
      }
      function setActive(link) {
        if (activeLink === link) {
          return;
        }
        if (activeLink) {
          activeLink.classList.remove('post-toc__link--active');
        }
        if (link) {
          link.classList.add('post-toc__link--active');
        }
        activeLink = link;
      }
      function updateActiveHeading() {
        if (!headingEntries.length) {
          return;
        }
        const scrollPosition = window.scrollY + 160;
        let current = headingEntries[0];
        for (const item of headingEntries) {
          if (item.target.offsetTop <= scrollPosition) {
            current = item;
          } else {
            break;
          }
        }
        setActive(current.link);
      }
      function onScroll() {
        if (ticking) {
          return;
        }
        ticking = true;
        window.requestAnimationFrame(() => {
          updateActiveHeading();
          ticking = false;
        });
      }
      document.addEventListener('keydown', (event) => {
        if (event.key === 'Escape') {
          closeTableOverlay();
        }
      });
      enhanceTables();
      enhanceCodeBlocks();
      initToc();
      applyTheme(initialTheme);
      if (themeToggle) {
        themeToggle.addEventListener('click', () => {
          applyTheme(body.getAttribute('data-theme') === 'dark' ? 'light' : 'dark');
        });
      }
      if (langBtn) {
        langBtn.textContent = CURRENT_LANG === 'it' ? 'EN' : 'IT';
        if (OTHER_LANG_LINK) {
          langBtn.addEventListener('click', () => {
            localStorage.setItem(BLOG_LANG_KEY, OTHER_LANG);
            window.location.href = OTHER_LANG_LINK;
          });
        } else {
          langBtn.disabled = true;
          langBtn.classList.add('lang-btn--disabled');
        }
      }
      localStorage.setItem(BLOG_LANG_KEY, CURRENT_LANG);
      if (headingEntries.length) {
        headingEntries.sort((a, b) => a.target.offsetTop - b.target.offsetTop);
        updateActiveHeading();
        window.addEventListener('scroll', onScroll, { passive: true });
      }
    })();
  </script>
</body>
</html>