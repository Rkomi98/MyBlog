<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GeoAI Stack: A Guide for 2025 | Mirko Calcaterra</title>
  <meta name="description" content="GeoAI Stack: A Guide for 2025 &lt;details class=&quot;postwarning&quot; &lt;summary&lt;strongArticle under review&lt;/strong (click to open)&lt;/summary This article is still being written and is under technical review. Some content may be incomplete or subject to change with short n‚Ä¶">
  <meta name="author" content="Mirko Calcaterra">
  <link rel="canonical" href="https://rkomi98.github.io/MyBlog/blog/en/geoai-startingpoint/">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9EVQ8G9W48"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9EVQ8G9W48');
  </script>

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://rkomi98.github.io/MyBlog/blog/en/geoai-startingpoint/">
  <meta property="og:title" content="GeoAI Stack: A Guide for 2025">
  <meta property="og:description" content="GeoAI Stack: A Guide for 2025 &lt;details class=&quot;postwarning&quot; &lt;summary&lt;strongArticle under review&lt;/strong (click to open)&lt;/summary This article is still being written and is under technical review. Some content may be incomplete or subject to change with short n‚Ä¶">
  <meta property="og:image" content="https://rkomi98.github.io/MyBlog/Assets/Logo.png">
  <meta property="article:published_time" content="2025-11-23T00:00:00.000Z">
  <meta property="article:author" content="Mirko Calcaterra">
  <meta property="article:section" content="AI Engineering Path">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:title" content="GeoAI Stack: A Guide for 2025">
  <meta property="twitter:description" content="GeoAI Stack: A Guide for 2025 &lt;details class=&quot;postwarning&quot; &lt;summary&lt;strongArticle under review&lt;/strong (click to open)&lt;/summary This article is still being written and is under technical review. Some content may be incomplete or subject to change with short n‚Ä¶">
  <meta property="twitter:image" content="https://rkomi98.github.io/MyBlog/Assets/Logo.png">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "GeoAI Stack: A Guide for 2025",
    "image": "https://rkomi98.github.io/MyBlog/Assets/Logo.png",
    "datePublished": "2025-11-23T00:00:00.000Z",
    "dateModified": "2025-12-19T14:48:16.293Z",
    "author": {
      "@type": "Person",
      "name": "Mirko Calcaterra",
      "url": "https://rkomi98.github.io/MyBlog/"
    },
    "publisher": {
      "@type": "Person",
      "name": "Mirko Calcaterra"
    },
    "description": "GeoAI Stack: A Guide for 2025 &lt;details class=&quot;postwarning&quot; &lt;summary&lt;strongArticle under review&lt;/strong (click to open)&lt;/summary This article is still being written and is under technical review. Some content may be incomplete or subject to change with short n‚Ä¶"
  }
  </script>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    * {
      box-sizing: border-box;
    }
    html {
      scroll-behavior: smooth;
    }
    body {
      margin: 0;
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      background: radial-gradient(120% 120% at 50% 0%, rgba(59, 130, 246, 0.18) 0%, transparent 65%), var(--bg-primary);
      color: var(--text-primary);
      transition: background 0.3s ease, color 0.3s ease;
      --bg-primary: #0f172a;
      --bg-secondary: #111c33;
      --bg-card: rgba(15, 23, 42, 0.78);
      --bg-card-strong: rgba(15, 23, 42, 0.9);
      --border: rgba(148, 163, 184, 0.24);
      --text-primary: #e2e8f0;
      --text-secondary: #cbd5f5;
      --text-muted: #94a3b8;
      --accent: #60a5fa;
      --accent-strong: #38bdf8;
      --shadow-lg: 0 28px 60px -36px rgba(15, 23, 42, 0.9);
      --code-inline-bg: rgba(6, 11, 19, 0.92);
      --code-block-bg: #050912;
      --code-border: rgba(148, 163, 184, 0.35);
      --code-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.12);
      --code-text: #f8fafc;
    }
    body[data-theme="light"] {
      --bg-primary: #f8fafc;
      --bg-secondary: #ffffff;
      --bg-card: rgba(255, 255, 255, 0.96);
      --bg-card-strong: rgba(248, 250, 252, 0.98);
      --border: rgba(148, 163, 184, 0.18);
      --text-primary: #0f172a;
      --text-secondary: #334155;
      --text-muted: #64748b;
      --accent: #2563eb;
      --accent-strong: #1d4ed8;
      --shadow-lg: 0 28px 50px -38px rgba(15, 23, 42, 0.18);
      background: radial-gradient(120% 120% at 50% 0%, rgba(59, 130, 246, 0.12) 0%, transparent 60%), var(--bg-primary);
    }
    body[data-theme="light"] .post-toc {
      background: rgba(255, 255, 255, 0.96);
    }
    body[data-theme="light"] .post-body {
      background: rgba(255, 255, 255, 0.96);
      color: var(--text-secondary);
    }
    body[data-theme="light"] .post-hero__category {
      background: rgba(37, 99, 235, 0.12);
      color: var(--accent-strong);
    }
    body[data-theme="light"] .post-body blockquote {
      background: rgba(37, 99, 235, 0.1);
      color: var(--text-primary);
    }
    a {
      color: inherit;
      text-decoration: none;
    }
    header.site-header {
      position: sticky;
      top: 0;
      z-index: 12;
      backdrop-filter: blur(14px);
      background: rgba(15, 23, 42, 0.85);
      border-bottom: 1px solid var(--border);
      transition: background 0.3s ease;
    }
    body[data-theme="light"] header.site-header {
      background: rgba(248, 250, 252, 0.9);
    }
    .site-header__inner {
      max-width: 1200px;
      margin: 0 auto;
      padding: 1.15rem clamp(1.5rem, 3vw, 3rem);
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 1.5rem;
    }
    .site-header__left {
      display: flex;
      align-items: center;
      gap: 1.75rem;
    }
    .logo {
      display: inline-flex;
      align-items: center;
      gap: 0.7rem;
      font-weight: 600;
      color: var(--text-primary);
      font-size: 1.05rem;
      letter-spacing: 0.01em;
    }
    .logo-img {
      width: 38px;
      height: 38px;
      border-radius: 12px;
      object-fit: cover;
      box-shadow: 0 8px 18px -12px rgba(15, 23, 42, 0.6);
    }
    .site-nav {
      display: flex;
      gap: 1.1rem;
      font-size: 0.95rem;
      font-weight: 500;
      color: var(--text-muted);
    }
    .site-nav a:hover {
      color: var(--accent);
    }
    .header-controls {
      display: flex;
      align-items: center;
      gap: 0.75rem;
    }
    .lang-btn {
      border: 1px solid var(--border);
      background: var(--bg-card);
      color: var(--text-primary);
      padding: 0.45rem 0.9rem;
      border-radius: 12px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, border 0.2s ease, transform 0.2s ease;
    }
    .lang-btn:hover:not(.lang-btn--disabled) {
      background: var(--accent);
      color: #ffffff;
      transform: translateY(-1px);
      border-color: transparent;
    }
    .lang-btn--disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }
    .theme-toggle {
      position: relative;
      width: 52px;
      height: 28px;
      border-radius: 999px;
      border: 1px solid var(--border);
      background: var(--bg-card);
      cursor: pointer;
      padding: 0;
      transition: background 0.3s ease, border 0.3s ease;
      display: flex;
      align-items: center;
    }
    .theme-toggle .theme-thumb {
      position: absolute;
      top: 50%;
      left: 4px;
      transform: translateY(-50%);
      width: 22px;
      height: 22px;
      border-radius: 50%;
      background: #ffffff;
      color: #1f2937;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.9rem;
      transition: transform 0.3s ease, background 0.3s ease, color 0.3s ease;
      box-shadow: 0 6px 18px -8px rgba(15, 23, 42, 0.6);
    }
    body[data-theme="dark"] .theme-toggle .theme-thumb {
      transform: translate(20px, -50%);
      background: #1f2937;
      color: #f8fafc;
    }
    body[data-theme="dark"] .theme-toggle {
      background: rgba(37, 99, 235, 0.2);
      border-color: rgba(37, 99, 235, 0.3);
    }
    main.page {
      max-width: 1200px;
      margin: 0 auto;
      padding: 3.5rem clamp(1.5rem, 3vw, 3rem) 4.5rem;
    }
    .post-hero {
      position: relative;
      overflow: hidden;
      background: linear-gradient(135deg, rgba(99, 102, 241, 0.22) 0%, rgba(14, 165, 233, 0.08) 60%), var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 28px;
      padding: 2.75rem;
      box-shadow: var(--shadow-lg);
      margin-bottom: 3rem;
    }
    .post-hero::after {
      content: '';
      position: absolute;
      inset: 0;
      background: radial-gradient(circle at 20% 20%, rgba(59, 130, 246, 0.22) 0%, transparent 55%);
      pointer-events: none;
    }
    .post-hero__icon {
      position: relative;
      font-size: 3.1rem;
      margin-bottom: 1.5rem;
      display: inline-flex;
      align-items: center;
      justify-content: center;
    }
    .post-hero__category {
      position: relative;
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 0.4rem 1rem;
      border-radius: 999px;
      background: rgba(15, 23, 42, 0.35);
      color: #ffffff;
      font-weight: 600;
      letter-spacing: 0.02em;
      margin-bottom: 1.25rem;
      text-transform: uppercase;
      font-size: 0.8rem;
    }
    .post-hero__title {
      position: relative;
      margin: 0 0 1.25rem;
      font-size: clamp(2.4rem, 4vw, 3.2rem);
      letter-spacing: -0.025em;
      line-height: 1.2;
      color: var(--text-primary);
    }
    .post-hero__meta {
      position: relative;
      display: flex;
      flex-wrap: wrap;
      gap: 1.25rem;
      color: var(--text-muted);
      font-size: 0.95rem;
      font-weight: 500;
    }
    .post-hero__meta span {
      display: inline-flex;
      align-items: center;
      gap: 0.45rem;
    }
    .post-layout {
      display: grid;
      grid-template-columns: minmax(220px, 300px) minmax(0, 1fr);
      gap: 2.75rem;
      align-items: flex-start;
    }
    .post-layout--single {
      grid-template-columns: minmax(0, 1fr);
    }
    .post-toc {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 22px;
      padding: 1.5rem 1.6rem 1.8rem;
      box-shadow: var(--shadow-lg);
      position: sticky;
      top: 120px;
      max-height: calc(100vh - 160px);
      overflow: hidden;
      display: flex;
      flex-direction: column;
    }
    .post-toc__header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 0.75rem;
      margin-bottom: 0.5rem;
    }
    .post-toc__title {
      text-transform: uppercase;
      font-size: 0.78rem;
      letter-spacing: 0.18em;
      font-weight: 700;
      color: var(--text-muted);
    }
    .post-toc__toggle {
      display: none;
      border: 1px solid var(--border);
      background: transparent;
      color: var(--text-secondary);
      border-radius: 999px;
      padding: 0.25rem 0.8rem;
      font-size: 0.85rem;
      font-weight: 600;
      cursor: pointer;
      align-items: center;
      gap: 0.4rem;
      transition: background 0.2s ease, border 0.2s ease, color 0.2s ease;
    }
    .post-toc__toggle:hover {
      background: rgba(96, 165, 250, 0.15);
      border-color: transparent;
      color: var(--accent);
    }
    .post-toc__content {
      margin-top: 0.6rem;
      overflow-y: auto;
      padding-right: 0.4rem;
      transition: max-height 0.25s ease, opacity 0.25s ease;
      max-height: calc(100vh - 220px);
    }
    .post-toc--collapsed .post-toc__content {
      max-height: 0;
      opacity: 0;
      margin-top: 0;
      pointer-events: none;
    }
    .post-toc__list {
      list-style: none;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      gap: 0.45rem;
    }
    .post-toc__sublist {
      margin-left: 0.85rem;
      padding-left: 0.85rem;
      border-left: 1px solid rgba(148, 163, 184, 0.35);
      margin-top: 0.4rem;
      gap: 0.35rem;
    }
    .post-toc__item {
      margin: 0;
    }
    .post-toc__link {
      color: var(--text-secondary);
      font-size: 0.95rem;
      line-height: 1.45;
      display: flex;
      align-items: flex-start;
      gap: 0.5rem;
      border-bottom: 1px dashed transparent;
      transition: color 0.2s ease, border-bottom 0.2s ease, transform 0.2s ease;
    }
    .post-toc__link:hover {
      color: var(--accent);
      border-bottom-color: rgba(96, 165, 250, 0.4);
      transform: translateX(2px);
    }
    .post-toc__link--active {
      color: var(--accent-strong);
      border-bottom-color: var(--accent-strong);
    }
    .post-toc__number {
      font-variant-numeric: tabular-nums;
      font-size: 0.85rem;
      color: var(--text-muted);
      min-width: 2.5ch;
      display: inline-flex;
      justify-content: flex-end;
      padding-top: 0.15rem;
    }
    .post-toc__text {
      flex: 1;
    }
    .post-body {
      background: var(--bg-card-strong);
      border: 1px solid var(--border);
      border-radius: 26px;
      padding: 2.5rem;
      box-shadow: var(--shadow-lg);
      font-size: 1.04rem;
      line-height: 1.75;
      color: var(--text-secondary);
    }
    .post-body h2 {
      margin-top: 2.75rem;
      margin-bottom: 1.25rem;
      font-size: clamp(1.9rem, 3vw, 2.35rem);
      color: var(--text-primary);
      letter-spacing: -0.01em;
    }
    .post-body h3 {
      margin-top: 2.2rem;
      margin-bottom: 1rem;
      font-size: 1.5rem;
      color: var(--text-primary);
    }
    .post-body h4 {
      margin-top: 1.8rem;
      margin-bottom: 0.75rem;
      font-size: 1.2rem;
      color: var(--text-primary);
    }
    .post-body p {
      margin-bottom: 1.4rem;
    }
    .post-body .post-warning {
      margin: 2rem 0;
      border-radius: 18px;
      border: 1px solid rgba(250, 204, 21, 0.35);
      background: rgba(254, 243, 199, 0.9);
      color: #4a3b0a;
      padding: 0 1.25rem 1rem;
      box-shadow: inset 0 0 0 1px rgba(255, 255, 255, 0.35);
    }
    body[data-theme="dark"] .post-body .post-warning {
      background: rgba(253, 230, 138, 0.12);
      border-color: rgba(251, 191, 36, 0.5);
      color: #f6e6b2;
      box-shadow: inset 0 0 0 1px rgba(250, 200, 88, 0.3);
    }
    .post-body .post-warning summary {
      list-style: none;
      cursor: pointer;
      font-weight: 600;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      padding: 1rem 0;
      color: inherit;
    }
    .post-body .post-warning summary::-webkit-details-marker {
      display: none;
    }
    .post-body .post-warning summary::before {
      content: '‚ö†Ô∏è';
      font-size: 1rem;
    }
    .post-body .post-warning[open] {
      padding-bottom: 1.25rem;
    }
    .post-body .post-warning p:last-child {
      margin-bottom: 0;
    }
    .post-body ul,
    .post-body ol {
      margin: 1.4rem 0 1.4rem 1.4rem;
      padding: 0;
    }
    .post-body li {
      margin-bottom: 0.8rem;
    }
    .post-body a {
      color: var(--accent);
      text-decoration: none;
      border-bottom: 1px solid rgba(96, 165, 250, 0.35);
      transition: color 0.2s ease, border-bottom 0.2s ease;
    }
    .post-body a:hover {
      color: var(--accent-strong);
      border-bottom-color: var(--accent-strong);
    }
    .post-body blockquote {
      margin: 2rem 0;
      padding: 1.5rem 1.75rem;
      border-left: 4px solid var(--accent);
      border-radius: 0 18px 18px 0;
      background: rgba(37, 99, 235, 0.12);
      color: var(--text-primary);
    }
    .post-body code {
      background: var(--code-inline-bg);
      color: var(--code-text);
      padding: 0.2rem 0.45rem;
      border-radius: 6px;
      font-family: 'JetBrains Mono', 'Fira Code', monospace;
      font-size: 0.9rem;
    }
    .post-body pre code {
      background: transparent;
      padding: 0;
      display: block;
      font-size: inherit;
      line-height: inherit;
    }
    .hljs {
      color: #e2e8f0;
      background: transparent;
    }
    .hljs-comment,
    .hljs-quote {
      color: #7dd79d;
      font-style: italic;
    }
    .hljs-keyword,
    .hljs-selector-tag,
    .hljs-literal,
    .hljs-name,
    .hljs-strong,
    .hljs-built_in {
      color: #7dd3fc;
      font-weight: 600;
    }
    .hljs-title,
    .hljs-section,
    .hljs-function,
    .hljs-meta .hljs-keyword {
      color: #38bdf8;
      font-weight: 600;
    }
    .hljs-string,
    .hljs-doctag,
    .hljs-addition,
    .hljs-attribute,
    .hljs-template-tag,
    .hljs-template-variable {
      color: #facc15;
    }
    .hljs-number,
    .hljs-symbol,
    .hljs-bullet,
    .hljs-link,
    .hljs-meta,
    .hljs-type {
      color: #f472b6;
    }
    .hljs-variable,
    .hljs-params {
      color: #cbd5f5;
    }
    .post-body pre {
      background: var(--code-block-bg);
      color: var(--code-text);
      padding: 1.2rem 1.4rem;
      padding-right: 3.6rem;
      border-radius: 18px;
      overflow-x: auto;
      font-family: 'JetBrains Mono', 'Fira Code', monospace;
      font-size: 0.95rem;
      box-shadow: var(--code-shadow);
      border: 1px solid var(--code-border);
      margin: 2rem 0;
      position: relative;
    }
    .code-copy-btn {
      position: absolute;
      top: 0.75rem;
      right: 0.75rem;
      background: rgba(15, 23, 42, 0.8);
      color: #e2e8f0;
      border: 1px solid rgba(148, 163, 184, 0.35);
      border-radius: 999px;
      padding: 0.25rem 0.85rem;
      font-size: 0.85rem;
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, border-color 0.2s ease, transform 0.2s ease;
    }
    .code-copy-btn:hover {
      background: rgba(96, 165, 250, 0.85);
      color: #ffffff;
      border-color: transparent;
      transform: translateY(-1px);
    }
    .code-copy-btn--copied {
      background: rgba(34, 197, 94, 0.85);
      color: #ffffff;
      border-color: transparent;
    }
    .code-copy-btn__icon {
      font-size: 0.95rem;
    }
    .code-copy-btn__text {
      display: inline-block;
    }
    body[data-theme="light"] .code-copy-btn {
      background: rgba(248, 250, 252, 0.85);
      color: #0f172a;
      border-color: rgba(148, 163, 184, 0.4);
    }
    body[data-theme="light"] .code-copy-btn--copied {
      background: rgba(34, 197, 94, 0.92);
      color: #ffffff;
    }
    .post-body img {
      max-width: 100%;
      border-radius: 18px;
      margin: 2.2rem 0;
      box-shadow: 0 24px 45px -28px rgba(15, 23, 42, 0.55);
    }
    .post-body .table-wrapper {
      margin: 2rem 0;
      border-radius: 18px;
      border: 1px solid var(--border);
      background: rgba(15, 23, 42, 0.55);
      box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.12);
      position: relative;
      overflow: hidden;
    }
    .post-body .table-wrapper__scroll {
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }
    .post-body .table-wrapper__scroll::-webkit-scrollbar {
      height: 10px;
    }
    .post-body .table-wrapper__scroll::-webkit-scrollbar-thumb {
      background: rgba(96, 165, 250, 0.4);
      border-radius: 999px;
    }
    .post-body .table-wrapper table {
      width: 100%;
      border-collapse: collapse;
      background: transparent;
    }
    .post-body .table-wrapper[data-table-size="medium"] table {
      min-width: 720px;
    }
    .post-body .table-wrapper[data-table-size="wide"] table {
      min-width: 960px;
    }
    .post-body .table-wrapper thead th {
      background: rgba(96, 165, 250, 0.12);
      color: var(--text-primary);
      font-weight: 600;
    }
    .post-body .table-wrapper th,
    .post-body .table-wrapper td {
      padding: 0.9rem 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(148, 163, 184, 0.18);
      white-space: nowrap;
    }
    .post-body .table-wrapper td {
      white-space: normal;
    }
    .post-body .table-wrapper tr:last-child td {
      border-bottom: none;
    }
    .post-body .table-wrapper__expand {
      position: absolute;
      top: 0.75rem;
      right: 0.75rem;
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.3);
      color: var(--accent);
      border-radius: 999px;
      padding: 0.35rem 0.9rem;
      font-size: 0.85rem;
      font-weight: 600;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease, transform 0.2s ease;
      z-index: 2;
    }
    .post-body .table-wrapper__expand:hover {
      background: rgba(37, 99, 235, 0.35);
      color: #ffffff;
      transform: translateY(-1px);
      border-color: transparent;
    }
    .table-overlay {
      position: fixed;
      inset: 0;
      background: rgba(15, 23, 42, 0.85);
      backdrop-filter: blur(6px);
      display: none;
      align-items: center;
      justify-content: center;
      padding: 2rem;
      z-index: 999;
    }
    .table-overlay--visible {
      display: flex;
    }
    .table-overlay__content {
      background: var(--bg-card-strong);
      border: 1px solid var(--border);
      border-radius: 24px;
      max-width: min(1080px, 92vw);
      max-height: 85vh;
      width: 100%;
      box-shadow: 0 32px 80px -40px rgba(15, 23, 42, 0.9);
      position: relative;
      overflow: hidden;
    }
    .table-overlay__close {
      position: absolute;
      top: 0.85rem;
      right: 0.85rem;
      background: rgba(15, 23, 42, 0.9);
      border: 1px solid rgba(148, 163, 184, 0.35);
      color: var(--text-primary);
      border-radius: 999px;
      padding: 0.4rem 1rem;
      font-size: 0.9rem;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.2s ease, color 0.2s ease;
    }
    .table-overlay__close:hover {
      background: rgba(37, 99, 235, 0.4);
      color: #ffffff;
      border-color: transparent;
    }
    .table-overlay__scroll {
      overflow: auto;
      max-height: 85vh;
      padding: 2.5rem 2rem 2rem;
    }
    .table-overlay__scroll table {
      width: 100%;
      border-collapse: collapse;
      background: transparent;
    }
    .table-overlay__scroll table[data-table-size="medium"] {
      min-width: 720px;
    }
    .table-overlay__scroll table[data-table-size="wide"] {
      min-width: 960px;
    }
    .table-overlay__scroll thead th {
      background: rgba(96, 165, 250, 0.12);
      color: var(--text-primary);
      font-weight: 600;
    }
    .table-overlay__scroll th,
    .table-overlay__scroll td {
      padding: 0.9rem 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(148, 163, 184, 0.18);
      white-space: nowrap;
    }
    .table-overlay__scroll td {
      white-space: normal;
    }
    .table-overlay__scroll tr:last-child td {
      border-bottom: none;
    }
    body[data-theme="light"] .post-body .table-wrapper {
      background: rgba(255, 255, 255, 0.96);
      box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.16);
    }
    body[data-theme="light"] .post-body .table-wrapper__expand {
      background: rgba(248, 250, 252, 0.9);
    }
    body[data-theme="light"] .table-overlay {
      background: rgba(15, 23, 42, 0.25);
    }
    body[data-theme="light"] .table-overlay__content {
      background: rgba(255, 255, 255, 0.98);
    }
    body.no-scroll {
      overflow: hidden;
    }
    footer {
      margin-top: 4rem;
      padding: 2rem 0;
      text-align: center;
      color: var(--text-muted);
      font-size: 0.92rem;
      border-top: 1px solid var(--border);
      background: rgba(15, 23, 42, 0.35);
    }
    body[data-theme="light"] footer {
      background: rgba(255, 255, 255, 0.72);
    }
    @media (max-width: 1024px) {
      .site-header__inner {
        padding: 1rem clamp(1.25rem, 4vw, 2rem);
      }
      main.page {
        padding: 2.75rem clamp(1.25rem, 4vw, 2rem) 4rem;
      }
      .post-layout {
        grid-template-columns: minmax(0, 1fr);
      }
      .post-toc {
        position: sticky;
        top: 88px;
        z-index: 6;
        max-height: calc(100vh - 140px);
        margin-bottom: 2rem;
        padding: 1.1rem 1.25rem 1.35rem;
      }
      .post-toc__toggle {
        display: inline-flex;
      }
      .post-toc__content {
        max-height: none;
        margin-top: 0.4rem;
        overflow: visible;
      }
    }
    @media (max-width: 720px) {
      .post-hero {
        padding: 2.1rem 1.65rem;
      }
      .post-body {
        padding: 1.9rem 1.5rem;
      }
      .site-header__inner {
        flex-direction: column;
        align-items: stretch;
        gap: 1rem;
      }
      .site-header__left {
        justify-content: space-between;
      }
      .header-controls {
        align-self: flex-end;
      }
      .post-hero__title {
        font-size: clamp(2rem, 6vw, 2.6rem);
      }
      .post-body .table-wrapper {
        margin: 1.6rem 0;
      }
      .post-body .table-wrapper__expand {
        top: 0.6rem;
        right: 0.6rem;
        font-size: 0.78rem;
        padding: 0.25rem 0.75rem;
      }
      .table-overlay__scroll {
        padding: 1.8rem 1.25rem 1.5rem;
      }
    }
  </style>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      },
    };
  </script>
  <script id="mathjax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body data-theme="dark">
  <header class="site-header">
    <div class="site-header__inner">
      <div class="site-header__left">
        <a class="logo" href="../../../index.html">
          <img src="../../../Assets/Logo.png" alt="Mirko Calcaterra logo" class="logo-img">
          <span class="logo-text">Mirko Calcaterra</span>
        </a>
        <nav class="site-nav">
          <a href="../../../index.html" data-it="Home" data-en="Home">Home</a>
          <a href="../../../blog/index.html" data-it="Blog" data-en="Blog">Blog</a>
        </nav>
      </div>
      <div class="header-controls">
        <button class="lang-btn" type="button">EN</button>
        <button class="theme-toggle" type="button" aria-label="Toggle theme">
          <span class="theme-thumb">‚òÄÔ∏è</span>
        </button>
      </div>
    </div>
  </header>
  <main class="page">
    <article class="post">
      <section class="post-hero">
        <div class="post-hero__icon">üß†</div>
        <span class="post-hero__category">AI Engineering Path</span>
        <h1 class="post-hero__title">GeoAI Stack: A Guide for 2025</h1>
        <div class="post-hero__meta">
          <span>üìÖ November 23, 2025</span>
          <span>‚è±Ô∏è 25 min</span>
        </div>
      </section>
      <section class="post-layout">
        <aside class="post-toc" data-collapsed="false">
        <div class="post-toc__header">
          <div class="post-toc__title" data-it="Indice" data-en="Table of contents">Table of contents</div>
          <button class="post-toc__toggle" type="button" aria-expanded="true" aria-label="Hide table of contents">
            <span class="post-toc__toggle-text">Table of contents</span>
            <span class="post-toc__toggle-icon" aria-hidden="true">‚ñæ</span>
          </button>
        </div>
        <div class="post-toc__content">
          <ul class="post-toc__list">
    <li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#1-architectural-overview-of-the-geospatial-ai-stack">
            <span class="post-toc__number">1</span>
            <span class="post-toc__text">1. Architectural Overview of the Geospatial AI Stack</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#local-vs-production-architecture">
            <span class="post-toc__number">1.1</span>
            <span class="post-toc__text">Local vs. Production Architecture</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="0">
          <a class="post-toc__link" href="#2-map-of-tools-and-resources-2025">
            <span class="post-toc__number">2</span>
            <span class="post-toc__text">2\. Map of Tools and Resources (2025)</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#21-python-environment-management-venv-conda-poetry-etc">
            <span class="post-toc__number">2.1</span>
            <span class="post-toc__text">2.1 Python Environment Management (venv, conda, poetry, etc.)</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#gpugeo-reproducibility">
            <span class="post-toc__number">2.1.1</span>
            <span class="post-toc__text">GPU/Geo Reproducibility</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#22-essential-ai-engineer-tooling">
            <span class="post-toc__number">2.2</span>
            <span class="post-toc__text">2.2 Essential AI Engineer Tooling</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#does-all-this-make-sense-today-with-ai">
            <span class="post-toc__number">2.2.1</span>
            <span class="post-toc__text">Does all this make sense today with AI?</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#23-docker-and-aigeo-containerization">
            <span class="post-toc__number">2.3</span>
            <span class="post-toc__text">2.3 Docker and AI+GEO Containerization</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#example-1-dockerfile-for-llmrag-fastapi-service-cpu">
            <span class="post-toc__number">2.3.1</span>
            <span class="post-toc__text">Example 1: Dockerfile for LLM/RAG + FastAPI service (CPU)</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#example-2-dockerfile-for-geospatial-pipeline-with-gdal-optional-gpu">
            <span class="post-toc__number">2.3.2</span>
            <span class="post-toc__text">Example 2: Dockerfile for geospatial pipeline (with GDAL, optional GPU)</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#gpu-management">
            <span class="post-toc__number">2.3.3</span>
            <span class="post-toc__text">GPU Management</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#24-managing-passwords-and-configurations">
            <span class="post-toc__number">2.4</span>
            <span class="post-toc__text">2.4 Managing &quot;passwords&quot; and configurations</span>
          </a>
          <ul class="post-toc__list post-toc__sublist">
    <li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#env-approach-local">
            <span class="post-toc__number">2.4.1</span>
            <span class="post-toc__text">.env Approach (local)</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#yamlini-file-approach-other-classes">
            <span class="post-toc__number">2.4.2</span>
            <span class="post-toc__text">YAML/INI file approach + other classes</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="2">
          <a class="post-toc__link" href="#secret-management-in-production">
            <span class="post-toc__number">2.4.3</span>
            <span class="post-toc__text">Secret management in production</span>
          </a>
          
        </li>
  </ul>
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#25-open-datasets-and-staccog-catalogs-for-disasters">
            <span class="post-toc__number">2.5</span>
            <span class="post-toc__text">2.5 Open Datasets and STAC/COG Catalogs for Disasters</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#26-geospatial-and-core-rs-libraries">
            <span class="post-toc__number">2.6</span>
            <span class="post-toc__text">2.6 Geospatial and Core RS Libraries</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#27-project-templates-reference-repos-and-best-practices">
            <span class="post-toc__number">2.7</span>
            <span class="post-toc__text">2.7 Project Templates, Reference Repos, and Best Practices</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#28-practical-plan-complete-setup-in-7-10-days">
            <span class="post-toc__number">2.8</span>
            <span class="post-toc__text">2.8 Practical Plan: Complete Setup in 7-10 Days</span>
          </a>
          
        </li><li class="post-toc__item" data-depth="1">
          <a class="post-toc__link" href="#29-immediate-actions-first-24h">
            <span class="post-toc__number">2.9</span>
            <span class="post-toc__text">2.9 Immediate Actions (first 24h)</span>
          </a>
          
        </li>
  </ul>
        </li>
  </ul>
        </div>
      </aside>
        <div class="post-body">
          <details class="post-warning">
<summary><strong>Article under review</strong> (click to open)</summary>

<p>This article is still being written and is under technical review. Some content may be incomplete or subject to change with short notice.</p>
</details>

<p>Second installment of our long journey to become a GeoAI engineer. Last time we generally described what an AI engineer does and what differentiates them.</p>
<p>Now we understand what an aspiring GeoAI engineer needs to get started: the tools, datasets, and necessary setup.</p>
<h2 id="1-architectural-overview-of-the-geospatial-ai-stack">1. Architectural Overview of the Geospatial AI Stack</h2>
<p>Let&#39;s start with the main objective and keep it in mind.</p>
<blockquote>
<p><strong>Objective:</strong> Integrate <strong>Language (LLM)</strong> models and <strong>Geospatial Vision</strong> pipelines in a reproducible environment, from local development to production.</p>
</blockquote>
<p>The typical architecture combines:</p>
<ul>
<li><strong>Geospatial data ingestion:</strong> access to optical satellite imagery (e.g., Sentinel-2) and SAR radar (e.g., Sentinel-1) via <strong>STAC/COG</strong> catalogs (Planetary Computer, Earth Data, etc.). We will discuss this in detail later.</li>
<li><strong>Pre-processing and remote sensing analysis:</strong> Python pipelines to read, align, and process large rasters (with <em>rasterio</em>/<em>GDAL</em>, <em>rioxarray</em>/<em>dask</em> for voluminous data) and vectors (with <em>geopandas</em>/<em>shapely</em>). This produces <em>features</em> such as damage maps, flood extent, extracted buildings, etc.</li>
<li><strong>Vision and geospatial models:</strong> application of specialized deep learning models on pre-processed data. For example, <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=Our%20main%20algorithm%20of%20choice,algorithm%20for%20semantic%20image%20segmentation">IBM used <strong>U-Net</strong></a> in one of its research activities for damage segmentation post-natural disaster. Or, models derived from <a href="https://arxiv.org/abs/2105.15203"><strong>SegFormer</strong></a> for change detection, such as <a href="https://arxiv.org/abs/2407.15317v1">Open-CD</a>.\ Libraries like <a href="https://github.com/torchgeo/torchgeo"><strong>TorchGeo</strong></a> provide ready-to-use datasets and pre-trained models specific to geospatial scenarios.</li>
</ul>
<p><img src="../../../Assets/Unet.png" alt="Image">
<em>Figure 1 ‚Äì U-Net architecture with encoder/decoder and skip connection for segmenting damage and classes in satellite images.</em></p>
<p><img src="../../../Assets/TorchGeo.png" alt="Image">
<em>Figure 2 ‚Äì TorchGeo collects ready-to-use datasets and pre-trained models designed for geospatial computer vision scenarios.</em></p>
<ul>
<li><p><strong>LLM/RAG Integration:</strong> a <strong>Retrieval-Augmented Generation (RAG)</strong> module connects geospatial results with an <strong>LLM</strong> (e.g., GPT-5 or Mistral Large) to enable Q&amp;A and reporting. The LLM can draw upon updated knowledge bases (documents, place descriptions) in addition to extracted data. This reduces the problem of hallucinations by providing verifiable context. For example, a user can ask <em>&quot;How many buildings were destroyed by the earthquake in Turkey?&quot;</em> and the system uses data extracted from the CV model + textual descriptions to generate an answer citing sources.</p>
</li>
<li><p><strong>Agents and automation:</strong> agent-based components (built with frameworks like <strong>LangChain</strong>, <strong>Haystack</strong>, or Datapizza-AI) orchestrate the steps and calls to specific tools. \ In particular, an <em>agent</em> can:</p>
<ol>
<li>query a <em>geospatial database</em> to find relevant post-disaster images;</li>
<li>execute the computer vision model to obtain metrics (number of damaged buildings, flooded area, etc.);</li>
<li>call the LLM to explain the results.</li>
</ol>
<p>  This enables complex &quot;question -&gt; actions -&gt; answer&quot; workflows in a modular way.</p>
</li>
<li><p><strong>Services and deployment:</strong> everything is containerized (Docker) and can be exposed via REST APIs (e.g., with <a href="https://fastapi.tiangolo.com/"><strong>FastAPI</strong></a>) or lightweight graphical interfaces. For example, a <em>Streamlit</em> dashboard can display interactive maps with damage layers and offer an LLM chat for disaster-related questions.</p>
</li>
</ul>
<h3 id="local-vs-production-architecture">Local vs. Production Architecture</h3>
<p>When developing the solution, it is good practice to work on test datasets (for example, on a few representative satellite scenes) using notebooks (like Jupyter Lab, Colab, etc.) or modular scripts (in src/).</p>
<p>In production, however, individual <strong>components</strong> are orchestrated into <strong>microservices</strong>: there can be a service for geospatial analysis (e.g., calculating risk maps) and a service for the LLM (e.g., generating responses), with logging and monitoring. Raw data in this case, such as images, resides in storage (either a local file system or a cloud bucket), while intermediate results, such as generated COGs (Cloud Optimized GeoTIFFs), shapefiles, vector embeddings, can be cached to speed up repeated requests.</p>
<p>I would like to note that language models are typically used via external APIs (OpenAI, etc.) or, in the case of optimized open-source models (e.g., gemma3n 4B), inference occurs on-premise.</p>
<p>As I read in <a href="https://arxiv.org/html/2502.18470v5#:~:text=On%20the%20other%20hand%2C%20large,zhang2024bb%20%2C%20but%20the%20resulting">this paper</a>, this <strong>hybrid AI + GIS</strong> architecture overcomes the limitations of individual systems: classic GIS systems struggle with natural language input, while <em>&quot;Large Language Models show strong linguistic capabilities but struggle with spatial reasoning and geospatial ground truth&quot;</em>. By combining them, we obtain a system where visual models provide &quot;eyes&quot; and structured data, and LLMs provide &quot;linguistic reasoning&quot; on such data, with the ability to consult real-time knowledge bases. In summary, the stack embraces the full cycle: <strong>Data (Geo) ‚Üí AI Vision ‚Üí Knowledge ‚Üí LLM</strong>. The following figure illustrates the key components and data flow in the system (from data collection to user response):</p>
<p><img src="../../../Assets/framework.png" alt="Image"></p>
<p><em>Figure 3 ‚Äì Summary diagram of the GeoAI stack connecting geospatial ingestion and analysis, CV models, RAG/LLM components, and agent-orchestrated deployment services.</em></p>
<h2 id="2-map-of-tools-and-resources-2025">2. Map of Tools and Resources (2025)</h2>
<p>In this chapter, we will look at the main options for each aspect of the stack: from Python environment management to development tools, covering container basics, open geospatial datasets, and key libraries. The goal will not only be to understand all of this but also to keep in mind a comparison of features, outlining the advantages of each method.</p>
<h3 id="21-python-environment-management-venv-conda-poetry-etc">2.1 Python Environment Management (venv, conda, poetry, etc.)</h3>
<p>One thing we all agree on: for a solid foundation, a <strong>reproducible Python environment</strong> is necessary, with all dependencies, including GPU packages and geospatial libraries.</p>
<p>The following table compares the most popular approaches today (in 2025):</p>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th>Approach</th>
<th>Type</th>
<th>Advantages (pros)</th>
<th>Disadvantages (cons)</th>
<th>Update</th>
</tr>
</thead>
<tbody><tr>
<td><strong>pip + venv</strong></td>
<td>Installer + isolated env</td>
<td>Simple and native; fast direct installation</td>
<td>Solver <a href="https://debuglab.net/2024/01/26/resolving-new-pip-backtracking-runtime-issue/"><strong>no longer greedy</strong></a> but still less powerful than SAT solvers; no native lockfile; requires <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=The%20OG%20of%20Python%20package,be%20completely%20decoupled%20from%20a">manual removal of unused sub-deps</a></td>
<td>pip 25.3 (2025)</td>
</tr>
<tr>
<td><strong>conda / mamba</strong></td>
<td>Package manager with C++ SAT solver</td>
<td><a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=Python%20GDAL%20requires%20,while%20using%20Conda%20package%20manager">Manages <strong>non-Python</strong> libraries</a> (e.g., GDAL, PROJ) in isolated envs; complete and fast resolution thanks to the <em>libmamba</em> solver</td>
<td>Heavy base environment (hundreds MB); lacks out-of-the-box lockfile; sometimes requires mixing pip ‚Üí possible conflicts</td>
<td>Conda 25.9.1 (2025)</td>
</tr>
<tr>
<td><strong>Poetry</strong></td>
<td>PyPI manager with lockfile</td>
<td>Uses unified <em>pyproject.toml</em> standard; generates multi-platform <strong>lockfile</strong> for reproducibility; automatically manages virtual env</td>
<td>Python solver relatively slow on large reqs. (DFS backtracking); voluminous lockfile; beware of excessive constraints (^version) which can cause conflicts in large teams.</td>
<td>Poetry 2.1 (2025)</td>
</tr>
<tr>
<td><strong>PDM / Hatch</strong></td>
<td>Modern PyPI managers</td>
<td><strong>PDM</strong> supports PEP 582 (local environment without activate); <strong>Hatch</strong> also acts as a complete build system and allows multi-Python version testing</td>
<td>Less common than the pip/conda/poetry triad; Hatch has a steeper learning curve and a packaging focus (not just env)</td>
<td>PDM 2.26 (2025), Hatch 1.15 (2025)</td>
</tr>
<tr>
<td><strong>uv (Astral)</strong></td>
<td>Unified all-in-one tool (Rust)</td>
<td><a href="https://docs.astral.sh/uv/"><strong>Extremely fast</strong></a> (10-100√ó pip) thanks to its Rust core; replaces pip, pipx, poetry, pyenv with a single tool; universal lockfile support and multi-project workspace; transparent integration with existing venvs (you can activate uv, then use normal pip if you want)</td>
<td>API and CLI still unstable (young project); emerging community</td>
<td>uv 0.9.9 (2025)</td>
</tr>
<tr>
<td><strong>pixi (prefix.dev)</strong></td>
<td>Conda-like package manager (Rust)</td>
<td><a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=benchmarks%20show%20that%20pixi%20is,on%20a%20M2%20MacBook%20Pro"><strong>Speed</strong></a> <del>4√ó micromamba, &gt;10√ó conda (resolution + install); supports its own cross-platform lockfile (solves one of conda&#39;s limitations); integrates PyPI packages into the unified solver (<a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative">uses uv libraries</a>); no conda base env to install ([standalone executable](<a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#">https://prefix.dev/blog/pixi_a_fast_conda_alternative#</a>:</del>:text=Reason%203%3A%20No%20more%20Miniconda,base%20environment))</td>
<td>New ecosystem (releases &lt;1 year); fewer pre-compiled packages compared to conda-forge; some commands are still evolving</td>
<td>pixi 0.59 (2025)</td>
</tr>
</tbody></table>
</div></figure><p>| <strong>pyenv</strong> | Python Version Manager | Allows easy installation and switching of different Python versions per project (useful for multi-version or legacy testing) | Does not manage packages; used in combination with venv/poetry; if used globally, can create confusion about active versions | pyenv 2.6.12 (2025) |</p>
<p><em>Table notes:</em> <strong>mamba</strong> is the C++ implementation of conda. Today <em>conda</em> itself incorporates <em>libmamba</em> by default, so the two converge in speed. In data science environments, conda/mamba remains popular for its ease with complex scientific packages, while in production, <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=Verdict">pip/poetry is often preferred</a> to avoid extra dependencies and have more control. Emerging tools like uv and pixi aim to unify the best of both worlds (speed and completeness). For example, <strong>uv</strong> is developed in Rust by the creators of Ruff and aims to become the &quot;Cargo for Python&quot; (a single tool to manage Python versions, dependencies, virtualenvs, publishing). <strong>Pixi</strong>, created by the mamba team, is a drop-in conda replacement written in Rust: it uses <em>uv</em> to resolve pip packages, generates lockfiles, and removes the need for a conda base environment, drastically improving speed and ergonomics for bringing conda environments to production.</p>
<h4 id="gpugeo-reproducibility">GPU/Geo Reproducibility</h4>
<p>For a local AI/RS project on GPU, conda often offers the simplest path (e.g., <code>conda install pytorch cudatoolkit gdal rasterio</code> in an env) because it manages compatible &quot;binaries&quot; (CUDA, GDAL). Alternatively, in containerized environments, you can opt for <strong>pip + Docker</strong> using base images with appropriate drivers (we&#39;ll discuss this shortly).</p>
<p>In all cases, it is recommended to <strong>pin versions</strong> in a lockfile (for example, using poetry.lock) or requirements files with hashes, and to document the setup (e.g., by providing a conda environment.yml + pip requirements.txt for safety).</p>
<p>An ideal project layout includes a structure similar to:</p>
<pre><code class="hljs language-bash">proj-root/  
‚îú‚îÄ‚îÄ src/ <span class="hljs-comment"># application code (main package)  </span>
‚îú‚îÄ‚îÄ notebooks/ <span class="hljs-comment"># exploratory Jupyter notebooks  </span>
‚îú‚îÄ‚îÄ data/ <span class="hljs-comment"># raw or sample data (gitignored if large)  </span>
‚îú‚îÄ‚îÄ models/ <span class="hljs-comment"># saved models or weights  </span>
‚îú‚îÄ‚îÄ tests/ <span class="hljs-comment"># unit/integrated tests  </span>
‚îú‚îÄ‚îÄ configs/ <span class="hljs-comment"># config for Hydra/pydantic (e.g., dev.yaml, prod.yaml)  </span>
‚îú‚îÄ‚îÄ Dockerfile <span class="hljs-comment"># container definition  </span>
‚îú‚îÄ‚îÄ pyproject.toml <span class="hljs-comment"># package/dep specifications (poetry/pdm)  </span>
‚îú‚îÄ‚îÄ requirements.txt <span class="hljs-comment"># dependencies (if pip)  </span>
‚îú‚îÄ‚îÄ Makefile <span class="hljs-comment"># useful commands (setup, run, lint, test, deploy)  </span>
‚îî‚îÄ‚îÄ README.md <span class="hljs-comment"># project documentation</span></code></pre><p>This organization partly follows the <a href="https://cookiecutter-data-science.drivendata.org/#directory-structure"><em>Cookiecutter Data Science</em></a> model (to clearly separate code, data, docs) and facilitates the transition <strong>from local development to production</strong>: the code is packaged (in src/ with an optional _<em>init</em>_.py), tests ensure functionality, and separate configs/secrets per environment allow for consistent deployments.</p>
<p>Let&#39;s say it&#39;s a good practice, it costs nothing and provides an order that is useful both to you and to those who work with you.</p>
<h3 id="22-essential-ai-engineer-tooling">2.2 Essential AI Engineer Tooling</h3>
<p>To ensure <strong>code quality and development speed</strong>, every developer adopts a set of lightweight DevOps/MLOps tools, which we will look at shortly.</p>
<p>Before giving advice on this, I&#39;ll provide some definitions that might be obvious to some but not to others.</p>
<blockquote>
<p>Linting is an automatic check of source code looking for:</p>
<ul>
<li>potential errors: unused variables, suspicious syntax, common bugs</li>
<li>style issues: formatting, inconsistent names, unrespected conventions.
&quot;code smell&quot;: patterns that are not errors, but can create problems later
In practice, files are analyzed without being executed, and points to be fixed are flagged, often with suggestions or automatic corrections.</li>
</ul>
</blockquote>
<ul>
<li><strong>Linting &amp; Format:</strong> <em>Black</em> for automatic code formatting (opinionated, PEP8) and <strong>Ruff</strong> for ultra-fast linting in Rust. Ruff includes hundreds of rules (replaces flake8, isort, etc.) and automatically fixes many issues. It has practically supplanted traditional linters in a short time thanks to its speed and coverage.<blockquote>
<p><strong>Tip:</strong> configure Black and Ruff not to overlap rules (e.g., disable in Ruff the rules that Black already handles, such as line length) - this can be done by <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac">centralizing the configuration</a> in pyproject.toml.</p>
</blockquote>
</li>
<li><strong>Type Checking:</strong> use <strong>static typing</strong> to prevent bugs. <em>Mypy</em> is the standard for type-checking in Python; alternatively, <em>Pyright</em> (integrated into VSCode/Pylance) offers very fast incremental analysis while writing. Setting a &quot;strict&quot; level (e.g., <code>warn_unused_configs</code>, <code>disallow_untyped_defs</code> in mypy) helps maintain robust code.</li>
<li><strong>Testing:</strong> <em>Pytest</em> is the de facto standard for unit and functional tests in Python. Organize tests in tests/ and perhaps use useful plugins: e.g., <strong>pytest-snapshot</strong> (or <a href="https://github.com/syrupy-project/syrupy"><strong>Syrupy</strong></a>) for <em>snapshot testing</em> of complex outputs (automatically compares current output with a previously saved one). This can be useful for validating, for example, API response JSON or raster analysis results. For geospatial pipelines, it might be useful to generate small synthetic datasets to test functions (e.g., create a 100x100 raster and a known geometry and verify that the overlay produces expected results).</li>
<li><strong>Pre-commit hooks:</strong> configure <em>pre-commit</em> (file <code>.pre-commit-config.yaml</code>) to automatically run quality tools before each commit. A recommended set of hooks: <strong>black</strong>, <strong>ruff</strong>, <strong>mypy</strong>, <em>isort</em> (if not using ruff for import sorting), <em>end-of-file-fixer</em> and <em>trailing-whitespace</em> (simple cleanups), optionally <strong>nbstripout</strong> or <strong>nbqa</strong> to normalize notebooks. This ensures that every commit adheres to code style standards and passes static tests. For example, a combined ruff/black hook makes the code consistent - a developer notes <em>&quot;Ruff + Black + isort configured together provide friction-free quality&quot;</em> in CI and editor.</li>
<li><strong>Minimum CI/CD:</strong> set up a simple GitHub Action that on every push executes: lint (ruff/black), type-check (mypy), and test (pytest) across a matrix of environments (at least Python 3.x). This automates quality control. For ML projects, a test on sample data can be included (e.g., run inference on a small image) to ensure pipelines and models function. A minimal YAML workflow would include steps to install dependencies (using poetry/mamba for speed in CI) and then <code>pre-commit run --all-files</code> followed by <code>pytest</code>.</li>
<li><strong>Notebooks and Collaboration:</strong> use <strong>JupyterLab 4</strong> (modern, supports plugins and real-time collaboration) or the <strong>VSCode notebook interface</strong> for prototyping. In shared or cloud contexts: <em>Deepnote</em>, <em>Google Colab</em>, or JupyterHub offer ready-to-use environments (Colab, for example, provides limited free GPUs). It&#39;s good practice to keep notebooks and code synchronized: <em>Jupytext</em> (notebooks as paired scripts) can be used for easy versioning. To visualize geospatial data in notebooks, tools like <strong>folium</strong> (interactive Leaflet maps) or <strong>ipyleaflet/leafmap</strong> allow direct visualization of tiles, shapefiles, and model results (this will be explored in a couple of sections).</li>
</ul>
<blockquote>
<p><strong>Tip:</strong> configure all these tools from the start. For example, add to pyproject.toml: black, ruff, isort, mypy with consistent config (same line-length, etc.). Install pre-commit and activate it (<code>pre-commit install</code>) so that every git commit triggers the checks. These automations make development <strong>&quot;no drama&quot;</strong>: the developer can focus on AI/Geo logic, while the tools keep the code clean and functional.</p>
</blockquote>
<h4 id="does-all-this-make-sense-today-with-ai">Does all this make sense today with AI?</h4>
<p>In recent years, the introduction of AI into development tools has revolutionized how we write, maintain, and version code. Yet, this revolution has not eliminated the need for good practices: it has simply shifted the focus of what truly matters. AI accelerates, suggests, generates, but does not guarantee quality and structural consistency. For this reason, many classic tools not only remain relevant but, in some cases, become more important than before.</p>
<p>Let&#39;s start with linting and formatting. It&#39;s no longer a matter of aesthetics, but of having more substance.</p>
<p>Formatters like <strong>Black</strong> remain indispensable. Even if AI produces already readable code, a consistent format is essential for clean diffs and friction-free reviews.</p>
<p>As for linters, tools like <strong>Ruff</strong> become fundamental not for aesthetic purposes (which AI already handles well), but for catching errors, unused imports, dead code, or risky patterns. The goal is not to make the code &quot;prettier,&quot; but to prevent bugs introduced by overly optimistic generations.</p>
<p>Let&#39;s move on to the guardrails introduced by type checking.</p>
<p>With AI proposing complex code or plausible but not always correct functions, tools like <strong>Mypy</strong> or <strong>Pyright</strong> become essential as a safety net. Static typing is not just a quality measure, but a structural guide that AI itself uses to generate more precise solutions. Particularly, in a project&#39;s core modules, a semi-strict type checking profile enormously reduces the risk of silent errors (meaning errors that are not visible now but emerge later).</p>
<p>Now let&#39;s analyze what I believe is the most important point: <strong>testing</strong>.</p>
<p>With well-structured tests and snapshot tests for complex outputs, it&#39;s possible to ensure that AI-driven optimizations or refactors do not alter, shall we say, delicate behaviors.</p>
<p>We&#39;re almost at the end; now it&#39;s time for pre-commit, what I define as the sentinel between us and Git.</p>
<p><strong>Pre-commit</strong> hooks continue to be useful as a final filter before code enters Git (here, by Git, I mean both GitHub and GitLab).</p>
<p>Black, Ruff, and some light cleanups like whitespace removal are often sufficient. Other heavier tools, like Mypy, can remain in CI to avoid local slowdowns. In personal projects, they can even be omitted, but in teams, it remains a mechanism that prevents small, unnecessary errors.</p>
<p>Finally, for independence and reproducibility, there&#39;s the classic CI/CD.</p>
<p>With AI facilitating the generation of entire code blocks, the probability of unintentionally destructive changes increases. A minimal CI pipeline, including linting, type checking, and tests, ensures that every push is valid in a clean and controlled environment.</p>
<p>This is a fundamental step to prevent AI-generated code from breaking distant functions or introducing regressions that are difficult to identify.</p>
<p>In conclusion, in a world where AI supports development, &quot;code safety&quot; tools do not disappear: they reposition themselves. Black standardizes, Ruff protects, Mypy or Pyright guide, Pytest ensures stability, CI guarantees reproducibility.</p>
<p>Therefore, AI does not eliminate the need for these practices. On the contrary, it makes them even more relevant because it increases the volume and speed of code produced.</p>
<p>Quality is no longer just a matter of writing, but of the ecosystem. And in this ecosystem, AI is a powerful accelerator, but not a substitute!</p>
<h3 id="23-docker-and-aigeo-containerization">2.3 Docker and AI+GEO Containerization</h3>
<p>This is a part that is very dear to me (so much so that I want to dedicate an entire series to Docker).</p>
<p>Containerizing the app allows for standardizing the development environment (especially for native libraries and GPU drivers) and thus facilitating the deployment phase. We present two examples of optimized <strong>Dockerfiles</strong>, one for a lightweight LLM/RAG service, the other for a heavy geospatial pipeline, and a table of recommended base images.</p>
<h4 id="example-1-dockerfile-for-llmrag-fastapi-service-cpu">Example 1: Dockerfile for LLM/RAG + FastAPI service (CPU)</h4>
<pre><code class="hljs language-docker"><span class="hljs-comment"># We use the lightweight Python version with Debian 12 &quot;bookworm&quot;</span>
<span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.12</span>-slim-bookworm as base

<span class="hljs-comment"># Update and install only git, without recommended packages, then clean the cache</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends git \
    &amp;&amp; <span class="hljs-built_in">rm</span> -rf /var/lib/apt/lists/*</span>

<span class="hljs-comment"># Create a non-root user to run the service</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> useradd -m appuser</span>

<span class="hljs-comment"># Set the working directory</span>
<span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span>

<span class="hljs-comment"># Install a stable version of Poetry and configure package installation</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> pip install --no-cache-dir poetry==1.8.2</span>

<span class="hljs-comment"># Copy project files and leverage Docker cache to speed up updates</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> pyproject.toml poetry.lock ./</span>
<span class="hljs-comment"># Install dependencies (production only) directly into the global site-packages</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> poetry config virtualenvs.create <span class="hljs-literal">false</span> \
    &amp;&amp; poetry install --no-dev</span>

<span class="hljs-comment"># Copy application code</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> src/ ./src/</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> main.py ./</span>

<span class="hljs-comment"># Run as non-root user</span>
<span class="hljs-keyword">USER</span> appuser

<span class="hljs-comment"># Start Uvicorn exposing FastAPI externally</span>
<span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">&quot;uvicorn&quot;</span>, <span class="hljs-string">&quot;main:app&quot;</span>, <span class="hljs-string">&quot;--host&quot;</span>, <span class="hljs-string">&quot;0.0.0.0&quot;</span>, <span class="hljs-string">&quot;--port&quot;</span>, <span class="hljs-string">&quot;8000&quot;</span>]</span>
</code></pre><p><strong>Note:</strong> Here we use python:3.12-slim (~50MB) because it&#39;s a good compromise between size and functionality, as it avoids issues arising from using Alpine.
I decided to propose version 3.12 for maximum compatibility with other libraries.</p>
<p>Dependency installation is done in a separate layer by copying only <code>pyproject/lock</code> (to leverage Docker cache: if only the code changes and not the dependencies, all packages are not reinstalled).</p>
<p>Uvicorn serves the FastAPI app. This container is CPU-only (suitable for LLMs via external API or small models). If we wanted to include a local model (e.g., Transformers), it would be enough to add <code>RUN pip install transformers</code> or include it directly in poetry.</p>
<h4 id="example-2-dockerfile-for-geospatial-pipeline-with-gdal-optional-gpu">Example 2: Dockerfile for geospatial pipeline (with GDAL, optional GPU)</h4>
<p>Let&#39;s now look at a more complex example, but also one more suitable for a GeoAI engineer.</p>
<p>For complex geospatial pipelines (raster analysis, photogrammetry, deep
learning on satellite imagery), an environment with many
native libraries (GDAL, PROJ, Rasterio) and often GPU support is necessary.</p>
<pre><code class="hljs language-docker"><span class="hljs-comment"># Stage 1 ‚Äì builder with Miniconda and mamba</span>
<span class="hljs-keyword">FROM</span> continuumio/miniconda3:latest as builder

<span class="hljs-comment"># The latest version of mamba is 2.3.3. Then we delete temporary files</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> conda install -n base -c conda-forge mamba==2.3.3 \
    &amp;&amp; conda clean -afy</span>

<span class="hljs-comment"># Copy the environment that lists geospatial packages (Python 3.12, GDAL,</span>
<span class="hljs-comment"># Rasterio, Geopandas, etc.)</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> environment.yaml /tmp/environment.yaml</span>

<span class="hljs-comment"># Update the base environment with the libraries specified in environment.yaml</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> mamba <span class="hljs-built_in">env</span> update -n base -f /tmp/environment.yaml \
    &amp;&amp; conda clean -afy</span>

<span class="hljs-comment"># Install additional packages with pip (e.g., Raster Vision)</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> pip install --no-cache-dir rastervision==0.31.2</span>

<span class="hljs-comment"># Stage 2 ‚Äì production image with CUDA 13.0.2 support</span>
<span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">13.0</span>.<span class="hljs-number">2</span>-runtime-ubuntu22.<span class="hljs-number">04</span> AS production

<span class="hljs-comment"># Copy the conda installation from the builder</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> --from=builder /opt/conda /opt/conda</span>

<span class="hljs-comment"># Update the PATH variable to include conda</span>
<span class="hljs-keyword">ENV</span> PATH=<span class="hljs-string">&quot;/opt/conda/bin:$PATH&quot;</span>

<span class="hljs-comment"># Set the working directory</span>
<span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span>

<span class="hljs-comment"># Copy the source code</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> src/ ./src/</span>
<span class="hljs-keyword">COPY</span><span class="language-bash"> entrypoint.py ./</span>

<span class="hljs-comment"># Pipeline startup command</span>
<span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;entrypoint.py&quot;</span>]</span>
</code></pre><p>For the <code>environment.yaml</code> file, you can use:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">geoenv</span>
<span class="hljs-attr">channels:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">conda-forge</span>
<span class="hljs-attr">dependencies:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">python=3.12</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">gdal=3.12</span>       <span class="hljs-comment"># GDAL 3.12 is available via OSGeo containers</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">rasterio</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">geopandas</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">numpy</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">pandas</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">pyproj</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">pip</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">pip:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">rastervision==0.31.2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">torch==2.7.0</span></code></pre><p>We have considered an example here taken from a <a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application">source</a> from which, at the time, I also studied. I tried to update it based on the library versions updated to the date of this article.</p>
<p>In this multi-stage Dockerfile, we use the <strong>Miniconda</strong> image with mamba as a builder to resolve dependencies (in <code>environment.yaml</code> we specify, for example: gdal, rasterio, geopandas, pytorch, etc. with the conda-forge channel). This approach automatically manages native libraries (GEOS, PROJ, etc.) avoiding pip errors (e.g., <code>pip install gdal</code> would fail without GDAL dev installed).</p>
<p>In the second part, we start from a very slim nvidia <strong>CUDA</strong> runtime, which includes only the necessary drivers for PyTorch Tensor GPU.</p>
<p>We copy the conda installation from the builder, avoiding bringing along the compilation layers. The result is a ready image with GPU support and geospatial libs.</p>
<p>Below is a table of common <strong>base images</strong> for various scenarios:</p>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th>Base Image</th>
<th>Content</th>
<th>Recommended Use</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://hub.docker.com/layers/library/python/3.12-slim/images/"><strong>python:3.12-slim</strong></a></td>
<td>Debian slim + Python 3.x</td>
<td>Lightweight Python services (APIs, agents) - minimal (&lt;50MB)</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/continuumio/miniconda3"><strong>continuumio/miniconda3</strong></a></td>
<td>Miniconda + conda (base env)</td>
<td>Data science/<a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=Unlike%20pip%2C%20Conda%20package%20manager,python%2C%20we%20get%20following%20error">Full Geo</a>; easy to install complex packages (e.g., GDAL)</td>
</tr>
<tr>
<td><a href="https://micromamba-docker.readthedocs.io/en/latest/"><strong>mambaorg/micromamba</strong></a></td>
<td>Micromamba in Alpine/CentOS</td>
<td>Build images with conda env in a lean way; ideal in multi-stage (downloads only required packages)</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/nvidia/cuda"><strong>nvidia/cuda:13.0.2-runtime-ubuntu22.04</strong></a></td>
<td>CUDA libraries + base runtime</td>
<td>Add GPU support. To be used with pip/conda to install PyTorch/TF with compatible CUDA. Note that we are at version 13.0.2 now</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/pytorch/pytorch"><strong>pytorch/pytorch:2.7.0-cuda12.8-cudnn8-devel-ubuntu20.04</strong></a></td>
<td>Python + PyTorch 2.0 + pre-installed CUDA</td>
<td>DL training/inference on GPU - avoids manual CUDA/cuDNN configurations. However, it also includes various libs (image ~&gt;10GB).</td>
</tr>
<tr>
<td><a href="https://github.com/OSGeo/gdal/pkgs/container/gdal"><strong>ghcr.io/osgeo/gdal:ubuntu-full-3.12.0</strong></a></td>
<td>Ubuntu + pre-compiled GDAL (full drivers)</td>
<td>Intensive GIS/RS pipelines.</td>
</tr>
<tr>
<td><a href="https://hub.docker.com/r/jupyter/scipy-notebook"><strong>jupyter/scipy-notebook</strong></a></td>
<td>Python with Jupyter Notebook + SciPy stack</td>
<td>Ready-to-use notebook environments (CPU). Includes numpy, pandas, matplotlib, etc. Useful for interactive development, also on cloud (e.g., JupyterHub Docker Stacks).</td>
</tr>
</tbody></table>
</div></figure><p>I don&#39;t exclude that there are</p>
<h4 id="gpu-management">GPU Management</h4>
<p>In on-prem deployments, enable GPU runtime with <code>--gpus all</code> on <code>Docker run</code> (using NVIDIA runtimes). In Kubernetes, use NVIDIA device plugins. If the host does not have a GPU, it will be sufficient for the image to still contain the correct libraries, and we can execute on CPU without errors, or delegate to Colab for GPU execution.</p>
<h3 id="24-managing-passwords-and-configurations">2.4 Managing &quot;passwords&quot; and configurations</h3>
<p>Today, the word AI often calls for the word <strong>API</strong>, and consequently also <strong>credentials (or config)</strong> for services (e.g., Mapbox token, OpenAI key, database URL). It is crucial <strong>not to embed</strong> these values <strong>in the source code</strong>, but to <strong>use configuration systems</strong>.</p>
<h4 id="env-approach-local">.env Approach (local)</h4>
<p>The simplest and most functional approach is certainly to use the classic <code>.env</code> file, which means putting the keys in a <code>.env</code> file excluded from git commands, and then loading it with <a href="https://pypi.org/project/python-dotenv/">python-dotenv</a>.</p>
<p>For <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration">example</a>, the <code>config.py</code> file could be written as:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> BaseSettings  

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Settings</span>(<span class="hljs-title class_ inherited__">BaseSettings</span>):  
    openai_api_key: <span class="hljs-built_in">str</span>  
    db_url: <span class="hljs-built_in">str</span>  
    debug: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>  
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Config</span>:  
    env_file = <span class="hljs-string">&quot;.env&quot;</span> <span class="hljs-comment"># reads variables from .env  </span>
    env_prefix = <span class="hljs-string">&quot;MYAPP_&quot;</span> <span class="hljs-comment"># optional: prefix required in env vars  </span>
settings = Settings()</code></pre><p>This code uses <strong>Pydantic BaseSettings</strong>. Using this code, at each startup, the application would read the environment variables (or from the .env file) and build a settings object.</p>
<p>This offers the advantage of <strong>type validation</strong> (e.g., if db_url must be a URL, Pydantic can validate it). Defaults can be defined, and Pydantic automatically converts types (int, bool, etc.) and handles nested configurations. Pydantic is excellent when <em>&quot;the configuration structure is relatively simple and based on env var/.env&quot;</em>, i.e., with few main parameters.</p>
<p>Furthermore, by integrating it with FastAPI, settings can be used as dependencies.</p>
<h4 id="yamlini-file-approach-other-classes">YAML/INI file approach + other classes</h4>
<p>In larger projects or with many configurations, using YAML or TOML files for different environments can be more organized. For example, a schema:</p>
<pre><code class="hljs language-1c">config/
<span class="hljs-string">|_default.yaml  </span>
<span class="hljs-string">|_dev.yaml  </span>
<span class="hljs-string">|_prod.yaml</span></code></pre><p>Libraries like <strong>Dynaconf</strong> support <em>multi-layer configuration</em> by loading multiple files and merging them based on an environment key.</p>
<p><a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration">Dynaconf</a> allows defining configs in different formats (YAML, TOML, Python) and distinguishes <em>development</em> vs <em>production</em> contexts, in addition to supporting encrypted secrets. It is indicated when <em>&quot;complex, multi-source configurations with clear separation between environments are needed&quot;</em>.</p>
<p>Alternatively, <strong>Hydra</strong> (from Facebook) allows composing configurations from modular files and overriding them via CLI. Hydra is common in research contexts for managing many parameters (e.g., model architecture, hyperparameters) and varying experiments simply by launching. Hydra automatically creates versioned output directories with the used config.</p>
<p><strong>Practical config tips:</strong> If the app is relatively simple (few parameters and secrets), <strong>Pydantic BaseSettings</strong> offers simplicity and robustness (type-safe). If it grows in complexity (e.g., dozens of entries, multiple files), <strong>Dynaconf</strong> might be useful to avoid boilerplate and manage multiple sources. <strong>Hydra</strong> is excellent if you plan to do many run variations (typical in ML model training), but for a web service, it might be overkill.</p>
<h4 id="secret-management-in-production">Secret management in production</h4>
<p>Never commit credentials. In cloud environments, use dedicated services: AWS Secrets Manager, GCP Secret Manager, Hashicorp Vault.</p>
<p>For example, on AWS, you can store the OPENAI_API_KEY and retrieve it dynamically in the ECS container or insert it as an environment variable through the configuration system (like Terraform).</p>
<p>Many CI/CD services (GitHub Actions, GitLab CI) offer an integrated vault to save secrets and make them available as env vars during deployment. Therefore, the recommended pattern is: <strong>locally .env</strong>, in CI/prod <strong>env var</strong> or encrypted config.</p>
<p>In summary, investing time in robust config/secret management ensures that the app can transition from dev to prod without manual code changes, minimizing leak risks (no keys in the repository, please).</p>
<h3 id="25-open-datasets-and-staccog-catalogs-for-disasters">2.5 Open Datasets and STAC/COG Catalogs for Disasters</h3>
<p>For more on satellite sensors and data types, see the article <a href="../../../blog/en/geodata/">What Data Do Satellites Record?</a>.</p>
<p>For <strong>Disaster Intelligence</strong> projects, drawing on updated open datasets is crucial. Fortunately, between 2022-2025, <strong>STAC</strong> (SpatioTemporal Asset Catalog) catalogs and specialized datasets have increased. Here is a selection of <em>state-of-the-art</em> datasets and resources:</p>
<figure class="table-wrapper" data-enhanced-table><div class="table-wrapper__scroll"><table>
<thead>
<tr>
<th>Dataset / Catalog</th>
<th>Data (type and resolution)</th>
<th>Coverage/Size</th>
<th>Task / Use</th>
<th>Source (link)</th>
</tr>
</thead>
<tbody><tr>
<td><strong><a href="https://arxiv.org/abs/1911.09296">xBD</a> / <a href="https://xview2.org/">xView2</a></strong> (2019)</td>
<td>Maxar pre- and post-event satellite imagery (RGB, ~0.3 m/px, 1024√ó1024 tile); building annotations + damage class</td>
<td><a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20images%20capture%2019%20natural,from%20all%20over%20the%20world">19 events of 5 different types</a> (earthquakes, hurricanes, fires); <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20data%20used%20for%20the,resolution%20color%20images">850k annotated buildings over ~45k km¬≤</a>; 9k pre/post image pairs for training</td>
<td><strong>Building Damage Assessment</strong> - building segmentation and damage classification (none, minor, moderate, major, destroyed)</td>
<td><a href="https://xview2.org/">DIU xView2</a> - <a href="https://arxiv.org/abs/1911.09296">xBD Paper</a>)</td>
</tr>
<tr>
<td><strong><a href="https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html">C2SMS Floods</a></strong> (ended in 2023)</td>
<td>Sentinel-1 (SAR) + Sentinel-2 (Optical) co-registered, 512√ó512 px; binary masks with water (flood vs permanent water)</td>
<td>~900 chip pairs from 18 global flood events</td>
<td><strong><a href="https://github.com/microsoft/PlanetaryComputerExamples/blob/main/competitions/s1floods/benchmark_tutorial.ipynb">Flood segmentation (multimodal)</a></strong>: training models to detect floodwater by combining SAR and optical</td>
<td><a href="https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf#:~:text=The%20C2S,oz32gz">Microsoft/Cloud to Street</a></td>
</tr>
<tr>
<td><strong>Sen1Floods11</strong> (2020)</td>
<td>Sentinel-1 GRD (SAR) chips 512√ó512 px; binary water mask</td>
<td>4,831 chips from 11 flood events across 5 continents<a href="https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1#:~:text=,to%20train%20and%20evaluate">[59]</a><a href="https://eod-grss-ieee.com/dataset-detail/ekFTRmNnWmtGOE52LzgrVUE4Ykd4dz09#:~:text=Sen1Floods11%20is%20a%20surface%20water,consists%20of%204%2C831%20512">[60]</a> (including Japan Tsunami 2011, Harvey 2017, etc.)</td>
<td><strong>Flood segmentation (SAR)</strong> - benchmarking on radar only (robust even with clouds)</td>
<td>Cloud to Street - [IEEE Paper]<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf#:~:text=,ter%20data%20set">[61]</a> (HuggingFace Datasets available)</td>
</tr>
<tr>
<td><strong>NASA/UN Flood Extents</strong> (2023)</td>
<td>Sentinel-1 rasters (sigma0 backscatter, ~10 m) with vector flood delineation polygons</td>
<td>Recent global events (e.g., Pakistan 2022, cyclones 2023) - data released via UNOSAT or NASA. Size varies (e.g., Pakistan ~1000 rasters)</td>
<td><strong>Rapid flood mapping</strong> - water segmentation via automated pipelines (Google, UN)</td>
<td>UNOSAT / NASA (HRC) - e.g., internal datasets shared on Radiant MLHub</td>
</tr>
</tbody></table>
</div></figure><p>| <strong>Maxar Open Data</strong> (2017-2025) | High-resolution optical satellite imagery (30-50 cm) pre and post-disaster (GeoTIFF) | &gt;100 global events (Nepal Earthquake 2015, Beirut Explosion 2020, Libya Flood 2023, etc.) - coverage varies per event (tens of images each)<a href="https://gee-community-catalog.org/projects/maxar_opendata/#:~:text=MAXAR%20Open%20Data%20Events%20,events%20like%20earthquakes%20and">[62]</a> | <strong>Visual damage mapping</strong> - quickly provides post-event CC BY 4.0 imagery for humanitarian uses, e.g., mapping collapsed buildings | Maxar Open Data Program (AWS OpenData)<a href="https://www.reddit.com/r/AirlinerAbduction2014/comments/17190ge/maxar_technologies_open_data_list_of_the_last/#:~:text=Maxar%20Technologies%20Open%20Data%20List,some%20of%20these%20datasets">[63]</a> (includes STAC index on registry.opendata.aws) |
| <strong>Copernicus EMS</strong> (2015-) | <em>Analyst-derived</em> products from imagery (GeoTIFF and shapefile): e.g., polygons of destroyed buildings, flood extent, damage grade maps (EMS-98) | Emergency activations in Europe and worldwide (thousands of maps for earthquakes, floods, fires) - resolution depends on imagery (Sentinel-2 10 m, Pl√©iades 0.5 m, etc.) | <strong>Rapid Mapping</strong> - official results from analysts in a few hours/days (useful ground truth for model training or verification) | Copernicus Emergency Mgt Service (public downloads per activation) |
| <strong>Landslide4Sense / GDCLD</strong> (2024) | Multi-source HR imagery (PlanetScope <del>3 m, Gaofen-6 2 m, UAV ~0.1 m) with landslide masks (pixel polygons) | 9 global seismic events (Nepal 2015, Amatrice 2016, etc.) with various geological contexts[[64]](<a href="https://essd.copernicus.org/articles/16/4817/2024/#">https://essd.copernicus.org/articles/16/4817/2024/#</a>:</del>:text=globally%20distributed%2C%20event,art%20semantic%20segmentation%20algorithms.%20These); <del>60,000 labeled landslides. + Test on 1 rain-induced event. | <strong>Landslide detection</strong> - landslide segmentation in mountainous contexts. Allows training robust models on global datasets. | <em>Globally Distributed Coseismic Landslide Dataset</em> - [DOI Zenodo][[64]](<a href="https://essd.copernicus.org/articles/16/4817/2024/#">https://essd.copernicus.org/articles/16/4817/2024/#</a>:</del>:text=globally%20distributed%2C%20event,art%20semantic%20segmentation%20algorithms.%20These) (Fang et al, ESSD 2024) |
| <strong>Planetary Computer STAC</strong> (ongoing) | Cloud catalog of <del>100 environmental/EO datasets: Sentinel-1, Sentinel-2, Landsat, MODIS, NAIP collections, etc., with STAC API and Azure storage for on-demand tiles | Global or national coverage depending on the dataset. Examples: Sentinel-2 L2A global (2017-2025), NAIP USA (aerial 60 cm). | <strong>Base data hub</strong> - standardized access to open geo imagery and data for custom analytics (subset, mosaic, etc.). Allows spatial and temporal queries via API.[[65]](<a href="https://github.com/microsoft/PlanetaryComputerExamples#">https://github.com/microsoft/PlanetaryComputerExamples#</a>:</del>:text=quickstarts%2C%20dataset%20examples%2C%20and%20tutorials) | Microsoft Planetary Computer - [Docs]<a href="https://github.com/microsoft/PlanetaryComputerExamples#:~:text=If%20you%27re%20viewing%20this%20repository,quickstarts%2C%20dataset%20examples%2C%20and%20tutorials">[66]</a> (free API key required for high throughput) |
| <strong>Radiant MLHub</strong> (ongoing) | Catalog of <strong>geospatial datasets for ML</strong> with labels: e.g., LandCoverNet (global land use classification), BigEarthNet, Functional Map of World, etc. + pre-trained models | Various - global (LandCoverNet covers 5 continents), regional (crop type in Rwanda, etc.). Data often in chips/COCO format. | <strong>Training data repository</strong> - unifies curated datasets used in competitions and papers, ready for use with libraries (torchgeo, tf.data). | Radiant Earth - [Registry on AWS]<a href="https://registry.opendata.aws/radiant-mlhub/#:~:text=Radiant%20MLHub%20,as%20other%20training%20data">[67]</a><a href="https://medium.com/radiant-earth-insights/geospatial-models-now-available-in-radiant-mlhub-a41eb795d7d7#:~:text=Radiant%20MLHub%20has%20been%20the,ML%29%20algorithms%20since%202019">[68]</a> (free API key available) |
| <strong>NOAA NGS Aerial</strong> (various) | Post-event nadir oblique aerial imagery (<del>10-20 cm/px) | USA (coasts and areas affected by hurricanes, tornadoes, fires). Example: ~15k aerial photos after Hurricane Ian 2022 Florida[[69]](<a href="https://storms.ngs.noaa.gov/storms/ian/index.html#">https://storms.ngs.noaa.gov/storms/ian/index.html#</a>:</del>:text=Hurricane%20IAN%20Imagery%20Hurricane%20IAN,by%20the%20NOAA%20Remote). | <strong>Damage inspection</strong> - used for visual damage assessment immediately after events (also input for CV - e.g., damaged roof detection). | NOAA NGS Emergency Response Imagery<a href="https://storms.ngs.noaa.gov/storms/ian/index.html#:~:text=Hurricane%20IAN%20Imagery%20Hurricane%20IAN,by%20the%20NOAA%20Remote">[69]</a> (viewer storms.ngs.noaa.gov, data downloadable via API) |</p>
<p>| <strong>Planet Disaster Data</strong> (2020-) | PlanetScope (3-5 m) and SkySat (<del>0.8 m) imagery with temporary free license for major disasters | Global, selective. Examples: Australia Fires 2020, Beirut 2020, etc. Often full event area coverage for subsequent days/weeks. | <strong>Rapid Monitoring</strong> - high-frequency (daily) optical imagery for monitoring disaster evolution. Useful for before/after time series not available elsewhere. | Planet/Open Region - (requires registration, data provided upon request or via partners)[[70]](<a href="https://esri-disasterresponse.hub.arcgis.com/pages/imagery#">https://esri-disasterresponse.hub.arcgis.com/pages/imagery#</a>:</del>:text=disasterresponse,major%20earthquakes%2C%20floods%2C%20storms) |</p>
<p>These datasets allow for training and validating models for <em>damage assessment</em>, <em>flood mapping</em>, <em>change detection</em>, etc., leveraging real data from past events. We note the trend towards multi-modal datasets (SAR+optical combined), multi-event (generalization), and with <strong>rich labeling</strong> (not just binary, but degrees of damage, land cover type, etc.). For example, xBD remains the reference for damage detection with its unprecedented breadth<a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20data%20used%20for%20the,resolution%20color%20images">[54]</a><a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20images%20capture%2019%20natural,from%20all%20over%20the%20world">[55]</a>, while Cloud to Street&#39;s flood datasets combine Sentinel-1 and 2 to mitigate cloud cover issues<a href="https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html#:~:text=The%20C2S,of%20chips%2C%20and%20water%20labels">[57]</a>.</p>
<p><strong>Active STAC Catalogs:</strong> The aforementioned Planetary Computer and Radiant MLHub offer uniform APIs for searching data by area and date. For example, with <strong>pystac-client</strong> we can query Planetary Computer:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> pystac_client <span class="hljs-keyword">import</span> Client  
catalog = Client.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;https://planetarycomputer.microsoft.com/api/stac/v1&quot;</span>)  
search = catalog.search(collections=[<span class="hljs-string">&quot;sentinel-2-l2a&quot;</span>],  
intersects={<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;Point&quot;</span>, <span class="hljs-string">&quot;coordinates&quot;</span>: [<span class="hljs-number">30.5</span>,<span class="hljs-number">50.5</span>]},  
datetime=<span class="hljs-string">&quot;2023-06-01/2023-06-30&quot;</span>)  
items = <span class="hljs-built_in">list</span>(search.get_items())  
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Found <span class="hljs-subst">{<span class="hljs-built_in">len</span>(items)}</span> Sentinel-2 images in June 2023 in the AOI.&quot;</span>)  
<span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items[:<span class="hljs-number">5</span>]:  
<span class="hljs-built_in">print</span>(item.<span class="hljs-built_in">id</span>, item.assets[<span class="hljs-string">&quot;B04&quot;</span>].href) <span class="hljs-comment"># prints link to band 4 (red)</span></code></pre><p><em>(Example of using pystac-client to search for Sentinel-2 images in an AOI and data period. It allows us to obtain URLs (often signed) to directly access the COG file, or download subsets.)</em></p>
<p>Similarly, Radiant MLHub provides SDKs and APIs for downloading ML datasets. For example, with the radiant_mlhub package, we can query collections by name and download annotations.</p>
<p><strong>Other Notable Datasets 2025:</strong> SpaceNet Challenges (1-8) have produced open datasets on building footprints, road network extraction, multi-year building change detection (SpaceNet 7), etc. Although somewhat dated, they remain valuable for benchmarks. For example, SpaceNet8 (2022) provides pre/post-flood PlanetScope imagery + flood masks and building footprints, useful for multimodal approaches (similar to xBD but for floods). Furthermore, datasets like <strong>CASA Landslide</strong> and <strong>DMLD</strong> (China) offer thousands of examples of landslides mapped in different regions<a href="https://www.mdpi.com/2072-4292/16/11/1886#:~:text=The%20Diverse%20Mountainous%20Landslide%20Dataset,across%20different%20terrain%20in">[71]</a><a href="https://www.sciencedirect.com/science/article/pii/S2666592124000568#:~:text=Landslide%20detection%20based%20on%20deep,detect%20landslides%20in%20Linzhi%20City">[72]</a>. The open community is also moving towards <strong>geospatial foundation models</strong>: e.g., <em>BigEarthNet</em> (520k Sentinel-2 patches labeled with land cover) is used to pre-train ResNet/ViT-like models on satellite data.</p>
<h3 id="26-geospatial-and-core-rs-libraries">2.6 Geospatial and Core RS Libraries</h3>
<p>The geospatial Python ecosystem is mature. Key tools:</p>
<ul>
<li><p><strong>GDAL (osgeo)</strong> - the <em>foundation</em> library for GIS rasters and vectors. In Python, it is used indirectly via bindings (osgeo.gdal) but more often through more Pythonic wrappers:</p>
</li>
<li><p><strong>rasterio</strong> (for rasters) and <strong>fiona</strong> (for shapefiles) are Pythonic <em>wrappers</em> of GDAL/OGR. <em>Rasterio</em> offers GeoTIFF reading/writing, cropping, reprojection, and numpy-friendly functions.</p>
</li>
<li><p><strong>Geopandas</strong> (for vectors) combines Fiona + Shapely: it allows reading shapefiles/GeoJSON into DataFrames and using geographic operations (buffer, intersection) via Shapely in a vectorized manner. Excellent for non-enormous vector layers (up to ~100k entities).</p>
</li>
<li><p><strong>Shapely 2.0</strong> - geometric library (in C) used by geopandas; with v2, it natively supports <em>arrays</em> (thanks to PyGEOS), making it very efficient on large sets of geometries.</p>
</li>
<li><p><strong>pyproj</strong> - manages coordinate systems and transformations (uses PROJ underneath).</p>
</li>
<li><p><strong>rasterio vs rioxarray vs xarray</strong>: <em>rasterio</em> is ideal for high-performance file-by-file raster operations and batch processing (scripts); <em>xarray</em> is preferable for analysis on data stacks like temporal cubes or large mosaics, thanks to <em>dask</em> for parallelization. <em>rioxarray</em> is an extension of xarray that adds geospatial awareness (CRS, transforms) using rasterio under the hood. <strong>Ruff</strong> - <em>&quot;Rasterio is for speed, batch processing of many files, Rioxarray shines for interactive analytical work in notebooks&quot;</em> users note<a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/#:~:text=%E2%80%A2%20%207mo%20ago">[73]</a>. However, be careful: rioxarray/xarray can consume a lot of memory if not well chunked, and on large datasets they sometimes crash, whereas rasterio still performs at lower speeds (&quot;when large datasets slow down or crash with rioxarray, I switch to GDAL/rasterio which solves it in a few seconds&quot; quote)<a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/#:~:text=%E2%80%A2%20%206mo%20ago">[74]</a><a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/#:~:text=This,to">[75]</a>. <strong>Tip:</strong> use <em>xarray + dask</em> for distributed processing (e.g., calculating NDVI on 1000 scenes) - dask will create a graph and execute in parallel (even on a cluster).</p>
</li>
<li><p><strong>rio-tiler</strong>: library for working with COG (Cloud Optimized GeoTIFF) and generating tiles (e.g., 256x256) for web visualization. It allows quickly extracting image portions via HTTP range requests and creating XYZ or WMTS tiles. Indispensable if building a map service (e.g., providing an interactive map of model predictions). Example: with rio-tiler you can get the PNG tile for zoom 15, x=17342,y=11945 from a COG in S3 with a single call, possibly including color mapping application.</p>
</li>
<li><p><strong>folium / ipyleaflet / leafmap</strong>: for visualizing results on interactive maps in Jupyter. <em>Folium</em> is simple: it produces a Leaflet HTML map with markers, raster/vector layers (you can even add the URL of a tile server or a COG). <em>ipyleaflet</em> is more advanced (bidirectional, reacts to Python in real-time) - e.g., it allows drawing a polygon on the map and having it in Python for analysis. <em>Leafmap</em> provides a high-level API and also supports interfaces like Google Earth Engine. Integrating these libraries helps to <strong>visually validate</strong> model outputs (e.g., showing damaged buildings in red on a map).</p>
</li>
<li><p><strong>TorchGeo</strong>: deserves mention again here - it is an official <em>PyTorch Domain Library</em><a href="https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/#:~:text=Geospatial%20deep%20learning%20with%20TorchGeo,models%20specific%20to%20geospatial%20data">[76]</a> which provides dataset loaders for many geospatial datasets (like those above), samplers for extracting geolocated patches by balancing classes, and pre-trained models (e.g., a ResNet50 pre-trained on BigEarthNet). It simplifies the creation of training pipelines with PyTorch Lightning for tasks such as building segmentation, land use classification, etc., reducing the custom code to write. For example, TorchGeo includes the datamodule for xView2, so you can easily get pre/post pairs and damage masks with a simple API<a href="https://github.com/torchgeo/torchgeo#:~:text=from%20torchgeo,trainers%20import%20SemanticSegmentationTask">[77]</a><a href="https://github.com/torchgeo/torchgeo#:~:text=datasets%20can%20be%20challenging%20to,reprojected%20into%20a%20matching%20CRS">[78]</a>.</p>
</li>
<li><p><strong>Others</strong>: <em>scikit-image</em> for raster filters (e.g., median filter to remove SAR speckle), <em>OpenCV</em> if fast transformations are needed, <em>pySAR</em> (or <em>Gamma</em>) for SAR processing (e.g., generating interferograms - specialist field). <strong>SNAP</strong> (from ESA) has Java APIs that can be called in Python (not trivial, but useful for Sentinel-1 pre-processing).</p>
</li>
<li><p><strong>Geospatial databases</strong>: if vector data becomes large, consider <em>PostGIS</em> (PostgreSQL + GIS extension) to store and query efficiently. For example, building detection results for a city (millions of points) can be put into PostGIS and queried with spatial indexes (e.g., find damaged buildings within a 1 km radius of X coordinates). Or, in geospatial retrieval for LLM agents (see Spatial RAG), a spatial database can quickly filter candidates (as suggested by Yu et al. 2024<a href="https://arxiv.org/html/2502.18470v5#:~:text=Answering%20real,the%20answering%20process%20as%20a">[79]</a><a href="https://arxiv.org/html/2502.18470v5#:~:text=geographic%20relationships%20and%20semantic%20user,retriever%20that%20combines%20sparse%20spatial">[80]</a>). With Python, you interact via SQLAlchemy/GeoAlchemy or directly with geopandas (which can read from PostGIS with read_postgis).</p>
</li>
</ul>
<p><strong>When to use what:</strong></p>
<ul>
<li>If you only need to read a few images, perform pixel-wise calculations or masks =&gt; <em>rasterio</em> + numpy is simple and sufficient.</li>
<li>If you need to manage a large data stack (time series, data cube over a large area) =&gt; <em>xarray + dask</em> is more appropriate (but requires thinking about chunking and memory).</li>
<li>For streaming pipelines (many independent images) =&gt; leverage <em>concurrent.futures</em> or <em>dask.bag</em> with rasterio (which releases the GIL during I/O operations).</li>
<li>For vectors, if &lt;100k features =&gt; <em>geopandas</em> is fine; &gt;100k consider PostGIS or use <em>spatialindex</em>/Rtree for in-memory spatial queries.</li>
<li>For <em>serving tile maps</em> =&gt; <em>rio-tiler</em> for the backend (cuts images) and <em>folium/ipyleaflet</em> for frontend map.</li>
<li>For <em>mixed raster-vector analysis</em> (zonal stats, clip raster with mask) =&gt; <em>rasterstats</em> is a useful package (built on rasterio).</li>
<li>If you are working with <em>3D or LiDAR data</em> =&gt; <em>PDAL</em> (Point Data Abstraction Lib) with Python bindings or <em>pylas</em> for LAS.</li>
</ul>
<p><strong>Example Pipeline (Optical + SAR):</strong> let&#39;s assume we want to identify flooded areas by combining Sentinel-1 and Sentinel-2:</p>
<ol>
<li><strong>Data loading:</strong> we download a post-event Sentinel-2 image (e.g., infrared and red band) and a coeval Sentinel-1 image. We can use pystac-client to get the asset URLs and download them. We load with rasterio: <code>s2 = rasterio.open(&#39;sentinel2.tif&#39;)</code>, <code>s1 = rasterio.open(&#39;sentinel1.tif&#39;)</code>.</li>
<li><strong>Pre-process:</strong> we align resolution and CRS - e.g., we bring Sentinel-1 (10 m, VV polarization) into the Sentinel-2 grid (10 m). With rasterio: <code>rasterio.warp.reproject(source=s1.read(1), src_transform=s1.transform, src_crs=s1.crs, destination=arr, dst_transform=s2.transform, dst_crs=s2.crs, dst_shape=s2.shape)</code>. We obtain the registered SAR array.</li>
<li><strong>Stack &amp; filter:</strong> we normalize the bands (e.g., Sentinel-1 in dB, applying a threshold to remove noise) and stack them: we build a 3-channel array: NIR, Red, SAR. We can do this with numpy on small images, or with xarray (a multi-band DataArray). We might apply a median filter on the SAR to reduce speckle.</li>
<li><strong>Model inference:</strong> we pass the tensor <code>[H,W,3]</code> to the segmentation model (e.g., U-Net trained on a flood dataset). We obtain a predicted binary mask.</li>
<li><strong>Post-process &amp; vectorize:</strong> using <em>rasterio.features</em>, we transform the mask into a shapefile (flooded polygons). <code>shapes = rasterio.features.shapes(mask_pred, transform=s2.transform)</code> provides geometries; we filter those with minimum area and write them with geopandas into a GeoJSON.</li>
<li><strong>Visualize/validate:</strong> we open the map with folium: <code>folium.Map(...)</code>. Add Sentinel-2 RGB layer, then add semi-transparent flood polygons in blue.</li>
</ol>
<p>This pipeline combines <strong>rasterio</strong> for I/O and warp, <strong>numpy/scikit-image</strong> for filters, <strong>deep learning (PyTorch)</strong> for inference, <strong>rasterio.features/geopandas</strong> for vector output. If the scale grows (very large or many images), we can distribute on dask: e.g., read images as dask arrays via rioxarray (<code>rioxarray.open_rasterio(..., chunks=...)</code>), perform lazy calculations, use map_blocks for model inference (if the model is deployed in a way that dask can call it - not trivial, but possible with dask.delayed). Or process iteratively by tile (e.g., 512x512).</p>
<p><strong>Integration with LLMs:</strong> the final geospatial results (e.g., shapefiles of flooded areas with attributes) can be inserted into a textual <em>knowledge base</em>: for example, generate a brief summary in JSON or CSV (50 km¬≤ flooded in region X). An LLM agent can then perform simple geospatial queries via Python code (e.g., using <em>shapely</em> to check if a point is within a flooded area) - as seen in recent research <em>Spatial-QA</em> approaches.</p>
<p>In short, <strong>Python geospatial libraries offer power similar to desktop GIS</strong> but programmable: from simple buffers to complex spatial joins or mass reprojections. The key is to choose the right tool based on <strong>scalability and complexity</strong>: a mix of rasterio/geopandas for targeted tasks, xarray/dask for big data, dedicated databases/services if necessary. Fortunately, mutual compatibility is good (geopandas can use shapely/dask-geopandas for parallelism, rasterio interacts with numpy/xarray easily). All integrated with the PyData world (numpy, pandas) and now PyTorch/TensorFlow for ML models creates a <strong>powerful ecosystem for geospatial AI</strong>.</p>
<h3 id="27-project-templates-reference-repos-and-best-practices">2.7 Project Templates, Reference Repos, and Best Practices</h3>
<p>To build a <strong>production-ready</strong> stack, it&#39;s useful to study existing open-source projects that address similar problems. Here are <strong>10 reference repositories and templates (2025)</strong> from which to draw lessons, with motivation:</p>
<ul>
<li><p><strong>LangChain</strong> - „Äê67‚Ä†repo„Äë (framework for agents and LLM apps). <strong>Why:</strong> It&#39;s the de facto standard for orchestrating LLMs with memory, tools, and data augmentation. Study how it structures modular code (chains, agents), its <strong>pyproject.toml</strong> with <em>uv</em> (they maintain it with uv and release frequently, <del>14k commits), and how they manage integrations (e.g., vectorstores). <em>Update:</em> v1.0 released 2025, very active repo (commits daily)[[81]](<a href="https://github.com/hwchase17/langchain/commits#">https://github.com/hwchase17/langchain/commits#</a>:</del>:text=Commits%20%C2%B7%20langchain,%C2%B7%20fix%28infra%29%3A).</p>
</li>
<li><p><strong>LlamaIndex (GPT Index)</strong> - <a href="https://github.com/run-llama/llama_index/releases#:~:text=Release%20Notes.%20%5B2025,core%20%5B0.14.0%5D.%20breaking%3A%20bumped">[82]</a> (data-centric RAG framework). <strong>Why:</strong> It provides patterns for indexing documents and images and performing efficient retrieval for LLMs. It has components for spatial queries (e.g., you could index coordinates and perform filtering). See how it implements various Storage and <strong>readers for heterogeneous data</strong>. Active Python repo (0.14.x releases in 2025).</p>
</li>
<li><p><strong>Haystack</strong> (deepset.ai) - <a href="https://github.com/deepset-ai/haystack">GitHub</a>. <strong>Why:</strong> End-to-end open-source QA pipeline, with support for document retrieval, generation, and even agents. They have a design oriented towards <strong>producing APIs</strong> (e.g., GraphQL) and composable YAML pipelines. Study how they define components (Reader, Retriever, Generator) and configuration management. Very production-friendly (used in companies). Regularly updated (v1.x in 2025).</p>
</li>
<li><p><strong>TorchGeo</strong> - „Äê49‚Ä†repo„Äë (PyTorch GeoAI library). <strong>Why:</strong> Example of a well-designed <em>domain-specific</em> library: see structure (modularity in datasets, samplers, models), thorough tests, and use of CI (they have integration with OSGeo, etc.). Actively maintained by Microsoft Research (recent commits 2025). Useful for understanding best practices in <em>packaging</em> geospatial models (e.g., in HuggingFace Hub).</p>
</li>
<li><p><strong>Segment Anything (Meta AI)</strong> - <a href="https://github.com/facebookresearch/segment-anything">GitHub</a>. <strong>Why:</strong> Although not geospatial, <em>SAM</em> has also had an impact in RS (used to segment generic objects in satellite images). The repo shows how to deploy a foundation model with interactive annotations. Study the <strong>optimized inference pipeline</strong> part (batched mask generation on large images) and the design of the demo notebook/web. Last update: 2023-11 (Meta released SAM-1).</p>
</li>
<li><p><strong>xView2 First Place Solution</strong> - <a href="https://github.com/DIUx-xView/xView2_first_place#:~:text=DIUx,idea%20will%20be%20improved%20further">[83]</a> (DIUx-xView GitHub). <strong>Why:</strong> Code from a winning team for building damage. It allows seeing a complete project: data preprocessing from xBD, model (U-Net ensemble), inference and post-processing, and final packaging (they have scripts for inference on image directories). Although from 2019, many principles remain valid. PyTorch structure with JSON config, separate training and inference. Useful for understanding <em>tricks</em> to improve performance (e.g., split model into two stages for localization vs classification<a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20U,sampler%20or%20the%20decoder">[84]</a>).</p>
</li>
<li><p><strong>SpaceNet 8 Baseline (Flood Detection)</strong> - <a href="https://github.com/SpaceNetChallenge/SpaceNet8#:~:text=Algorithmic%20baseline%20for%20SpaceNet%208,%C2%B7%20Finetune%20the">[85]</a> (spacenet8 repo). <strong>Why:</strong> An example of a <strong>multimodal pipeline</strong>: reads Sentinel-2 and -1, uses models to extract roads, buildings, and floods. It contains notebooks and scripts, and shows how to evaluate results with custom metrics. It teaches how to organize a challenge project and apply specific augmentations. Last update 2023.</p>
</li>
<li><p><strong>Raster Vision</strong> - <a href="https://nocomplexity.com/documents/fossml/generatedfiles/rastervision.html#:~:text=Complexity%20nocomplexity,metric%21%20Stars%20count%20are">[86]</a> (Azavea). <strong>Why:</strong> General framework for CV on geospatial images, written with a plugin architecture. Allows defining training pipelines with JSON configs. Study its <em>config-driven design</em>, the use of Docker to isolate environments, and its components (scenario, dataset, backend). Demonstrates how a tool can abstract common tasks (clip, retiling, train, predict) for various problems. Updated to the latest version 0.31 in 2025<a href="https://nocomplexity.com/documents/fossml/generatedfiles/rastervision.html#:~:text=Complexity%20nocomplexity,metric%21%20Stars%20count%20are">[86]</a>.</p>
</li>
<li><p><strong>Lightning + Hydra Template</strong> - <em>Lightning-Hydra-Template</em> (ashleve) - <a href="https://github.com/ashleve/lightning-hydra-template">GitHub</a>. <strong>Why:</strong> A very popular scaffolding repository for deep learning projects. Combines PyTorch Lightning for modular training and Hydra for managing experiment configs. Offers a clean structure (src with pl_modules, data_module, separate configs for dataset/model/training). Although general, it&#39;s useful for seeing best practices in <em>ML code organization</em>, logging (integrated neptune/mlflow), and train/val automation. Continuously updated with new best practices (v2024).</p>
</li>
<li><p><strong>Cookiecutter MLOps</strong> - (DrivenData Cookiecutter Data Science 2.0) - <a href="https://github.com/drivendata/cookiecutter-data-science">GitHub</a>. <strong>Why:</strong> A project template that emphasizes the complete cycle: data, modeling, deployment. Although more data-science oriented, it provides a reference on how to structure repositories with folders for raw data, intermediate data, saved models, reports, etc., and how to document decisions. Useful for not forgetting anything (e.g., it also includes CI setup and docs scaffolding).</p>
</li>
</ul>
<p>In addition to these, monitor tech company repositories on <em>disaster response</em> - e.g.: <strong>Google&#39;s flood forecasting</strong> (proprietary code, but detailed publications), <strong>UNOSAT open products</strong> (GIS scripts). And communities like <strong>OSGeo</strong> and <strong>OMEN (Open Mapping)</strong> for automatic mapping tools.</p>
<p><strong>Ideal Repository Structure (AI+Geo):</strong> combining inspiration from the above, we recommend:</p>
<ul>
<li><strong>Modularize</strong> by components: e.g., src/models/ for ML models, src/data/ for loading utilities, src/utils/geo.py for geospatial functions (buffer, reproject, etc.), src/api/ for FastAPI code.</li>
<li><strong>Configs outside code</strong>: use Hydra or Pydantic as discussed. Provide example configs for datasets (especially if open) so that new contributors can easily replicate results by pointing to the correct paths.</li>
<li><strong>Makefile/CLI</strong>: provide quick commands: make data (downloads open dataset), make train (starts training if applicable), make infer (performs inference on example input), make serve (launches API). This reduces friction in executing parts of the project.</li>
<li><strong>Comprehensive README</strong>: explain architecture, how to set up the environment, run tests, etc. See LangChain&#39;s README<a href="https://github.com/langchain-ai/langchain#:~:text=LangChain%20is%20a%20framework%20for,as%20the%20underlying%20technology%20evolves">[6]</a> or TorchGeo for examples of clear documentation and build badges.</li>
<li><strong>Tests &amp; CI</strong>: include some tests even for geospatial functions (e.g., a test that creates a geometry, applies a buffer, and compares the expected area). Or tests that the loaded model produces output with correct dimensions on a dummy input.</li>
</ul>
<p>Finally, general <strong>best practices</strong>: version models (use semver or date to know with which data they were trained), keep track of <em>cron jobs</em> if any (e.g., daily data update), and include monitoring tools if in production (Prometheus exporter for resources and perhaps a counter for how many times a certain inference function is called).</p>
<h3 id="28-practical-plan-complete-setup-in-7-10-days">2.8 Practical Plan: Complete Setup in 7-10 Days</h3>
<p>We now propose a <em>day-by-day</em> roadmap (approximately 2-3 hours per day for a week) to build the local stack and move it towards minimal production.</p>
<p><strong>Day 1: Environment and Initial Structure</strong><br><strong>Objective:</strong> Set up the Python environment and the basic project structure.<br>- <em>Activity:</em> Install pyenv and create a new Python 3.11 version if necessary. Initialize a git repository (git init). Choose the <strong>environment manager</strong> (e.g., Poetry vs Conda): to start, you can use Poetry for simplicity. Run poetry init and immediately add main dependencies: poetry add rasterio geopandas shapely torch torchvision fastapi uvicorn.<br>- <em>Commands:</em> pyenv install 3.11.5 &amp;&amp; pyenv local 3.11.5 (or use the system/conda interpreter), poetry init, poetry add .... Create the folder structure as in ¬ß2.1 (you can use cookiecutter data science as a reference).<br>- <em>Result:</em> Project with directory layout, active Python env with core packages installed. Verify crucial imports (open REPL: import rasterio, torch, geopandas to confirm they work - if geopandas complains about gdal, you might need apt-get install libgdal-dev or install via conda instead). In case of dependency issues (e.g., GDAL), adjust using conda: e.g., create env with mamba create -n myenv python=3.11 gdal rasterio geopandas.</p>
<p><strong>Day 2: Tooling and Code Quality</strong><br><strong>Objective:</strong> Configure pre-commit, lint, formatter, test scaffolding.<br>- <em>Activity:</em> Add <em>Black, Ruff, Mypy</em> to the project (poetry add --dev black ruff mypy pre-commit pytest). Create .pre-commit-config.yaml with hooks:  </p>
<p>repos:<br>- repo: <a href="https://github.com/astral-sh/ruff-pre-commit">https://github.com/astral-sh/ruff-pre-commit</a><br>rev: v0.5.4<br>hooks:<br>- id: ruff<br>args: [&quot;--fix&quot;] # let ruff apply simple fixes<br>- repo: <a href="https://github.com/psf/black">https://github.com/psf/black</a><br>rev: 23.9.1<br>hooks:<br>- id: black<br>- repo: <a href="https://github.com/pre-commit/mirrors-mypy">https://github.com/pre-commit/mirrors-mypy</a><br>rev: v1.5.1<br>hooks:<br>- id: mypy<br>- repo: local<br>hooks:<br>- id: pytest<br>name: pytest<br>entry: pytest<br>language: system<br>files: &quot;^tests/&quot;</p>
<p>Install the pre-commit hooks (pre-commit install). Create configuration files in pyproject.toml for black/ruff (line-length, etc. as in Medium combos<a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac#:~:text=%23%20pyproject.toml%20%5Btool.black%5D%20line,version%20%3D%20%5B%22py311">[41]</a>). Create a test stub in tests/test_basic.py with a trivial assertion (e.g., assert 2+2==4).<br>- <em>Commands:</em> pre-commit run --all-files to test the hooks immediately. pytest to see that the suite (with 1 trivial test) passes.<br>- <em>Result:</em> Every commit will now automatically format the code and flag issues. The test base is set up (even if trivial). The introduction of lint or type-check errors will be blocked by the hook, ensuring standards from the first code written.</p>
<p><strong>Day 3: Example Data Ingestion</strong><br><strong>Objective:</strong> Download a small example dataset (e.g., 1 event) and implement reading scripts.<br>- <em>Activity:</em> Choose a target dataset - e.g., images and labels of Hurricane Harvey 2017 (flood). Use the planetary STAC API or directly known links (Maxar Open Data, etc.) to download 1-2 pre and post images and some vector labels. Write a Python script in src/data/download_data.py that downloads these files (you can use requests or pystac-client + urllib). Implement a function to load a pair of rasters into memory (use rasterio).<br>- <em>Commands:</em> Example: poetry add requests pystac-client. Run the script: python src/data/download_data.py --event harvey (parameterize based on the event). This should save files in data/raw/harvey/.<br>- <em>Result:</em> You have a subset of local data. Verify by opening with rasterio that the files are readable and check dimensions. This prepares the ground for developing pipelines without downloading every time (add data to .gitignore). - <em>Extra:</em> You could write a test for the load function (e.g., assert img.shape == (‚Ä¶.) expected). Or generate a synthetic image for testing (useful for CI: you won&#39;t want to download 100MB of data in CI, but you can create a 10x10 GeoTIFF in memory and pass it to functions to test the flow).</p>
<p><strong>Day 4: Model and Geospatial Pipeline</strong><br><strong>Objective:</strong> Implement the optical+SAR -&gt; prediction -&gt; vectorization pipeline on a test case.<br>- <em>Activity:</em> If you have a pre-trained model (e.g., you could use placeholders like: if SAR pixel &gt; X and NDVI &lt; Y =&gt; flood), implement it now in src/models/model.py as the predict_flood(mask, arr_s2, arr_s1) function. Otherwise, if you have time/data, you can train a small model: install Lightning (poetry add lightning) and write a segmentation LightningModule. But given the time constraints, let&#39;s assume we use a simple heuristic or a loaded pre-trained model (e.g., use torchvision segmentation weights and adapt them).<br>- <em>Commands:</em> Run the complete pipeline on a test tile: read the images (Day3), pass them to the model, get the mask. Use geopandas to convert to vector and save the shapefile in data/predictions/. Visualize the result (you can create a small script with folium that loads the image and overlays the shapefile - open it in a browser).<br>- <em>Result:</em> Functional local pipeline on at least one input. Qualitative validation: if the model is not trained, it&#39;s fine to just see that it produces <em>something</em>. This step consolidates the integration between geospatial and ML components. - <em>Extra:</em> Add logs or print results (e.g., % flooded pixels: 12%). You can also calculate a metric if you have ground truth for that image (if there&#39;s a manual mask in the dataset, compare it with the predicted one).</p>
<p><strong>Day 5: Service API and LLM Integration</strong><br><strong>Objective:</strong> Create a FastAPI microservice that exposes the functionality and integrates an LLM for Q&amp;A.<br>- <em>Activity:</em> Write a FastAPI app in src/api/app.py with endpoints: /predict that might accept coordinates or an event ID and returns how many buildings/floods were found; /ask that accepts a user question and uses an LLM chain to answer. For the latter, you need to integrate the LLM: you can use OpenAI API (if you have a key) or a local open model (install pip install openai or transformers + an HF model). Implement simply: receive a question, in the backend call the internal function (which eventually consults geospatial results - if the question is like &quot;how many buildings damaged at X&quot;, query geopandas data). Finally, compose the answer - if you have OpenAI, send it with a prompt containing the data as context.<br>- <em>Commands:</em> Start the local server: uvicorn src.api.app:app --reload and test queries with curl or in the browser (FastAPI doc UI). Example: GET /predict?event=harvey -&gt; returns JSON {&quot;flooded_area_km2&quot;: ..., &quot;buildings_damaged&quot;: ...}. Then POST /ask with body {&quot;question&quot;: &quot;How many buildings were damaged by Hurricane Harvey?&quot;} -&gt; the code takes buildings_damaged from the previous output and formulates &quot;Approximately XYZ buildings were damaged.&quot; (In the future, this would be done via an agent).<br>- <em>Result:</em> A prototype <strong>disaster API</strong> service. Not yet robust or secure, but it demonstrates the idea. - <em>Extra:</em> Integrate the actual LLM - if OpenAI, add the key to .env and use openai.Completion.create(prompt=...). Or if local, load a small model (e.g.: pip install sentence-transformers and use a Q/A model like &quot;deepset/roberta-base-squad2&quot; with Transformers QA pipeline).</p>
<p><strong>Day 6: Dockerization and Portability</strong><br><strong>Objective:</strong> Create the Docker image of the app and test it (CPU).<br>- <em>Activity:</em> Write the Dockerfile based on the examples (¬ß2.3). Since the focus is lightweight (no heavy training), you could use Python slim + pip. Pay attention to geospatial dependencies: add apt-get for geos/gdal in the Dockerfile. Example:  </p>
<pre><code class="hljs language-dockerfile"><span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.11</span>-slim
<span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get update &amp;&amp; apt-get install -y libgdal-dev libgeos-dev</span>
...
<span class="hljs-keyword">RUN</span><span class="language-bash"> pip install -r requirements.txt</span></code></pre><p>(Or use a miniconda image and conda install). Copy the app and entrypoint.<br>- <em>Commands:</em> docker build -t myapp:dev . (from the root) and then docker run --rm -p 8000:8000 myapp:dev. Verify that by making the same requests as Day5, you get responses. If it works in Docker, you&#39;ve resolved any native dependency gaps. If it fails (e.g., Import GDAL errors), experiment with a different base image (conda).<br>- <em>Result:</em> Functional local container. This simulates the production environment. Current image size? Perhaps ~1-2 GB with all libs (geopandas and Torch inflate it a bit). Consider optimizing with multi-stage if time permits. - <em>Extra:</em> Test multi-arch: if you only have local CPU, at least you&#39;ve tested on linux/amd64.</p>
<p><strong>Day 7: Cloud Deployment or Documentation</strong><br><em>(Optional, if you wish to proceed further)</em><br><strong>Objective:</strong> Prepare the ground for deployment and write final documentation.<br>- <em>Activity:</em> Write user documentation: how to use the API, how to replicate results (perhaps in README). Optionally, create a <em>smoke test</em> script in tests/test_api.py that starts a FastAPI test client and calls /predict on a sample, verifying the response format.<br>- Consider where to deploy: you could try <strong>Heroku</strong> (simple for FastAPI, but without GPU) or <strong>Railway</strong>. Configure the Dockerfile for production (e.g., use gunicorn).<br>- Set up optional CI/CD: for example, a GitHub Actions workflow that builds the image and publishes it to Docker Hub on push to main.<br>- <em>Result:</em> Project ready to be shared: code, Docker, documentation, and perhaps a live link (even if performance is limited in the free tier).</p>
<p>Naturally, each phase may require adjustments. For example, you might discover a missing package ‚Äì resolved by adding it and regenerating poetry.lock or environment.yml (a reason to fix the environment early).</p>
<p>The plan anticipates ~7 days, but could extend to 10 if you dedicate time to custom model training or refining the LLM agent. In that case: Day 8-9 could be dedicated to fine-tuning a model (using open datasets and saving the weights), and Day 10 to integrating that output into the service (e.g., a /retrain endpoint if you want continuous MLOps).</p>
<h3 id="29-immediate-actions-first-24h">2.9 Immediate Actions (first 24h)</h3>
<p>To get started immediately, here&#39;s a quick checklist of the first actions to take:</p>
<ul>
<li><strong>1. Prepare the development environment:</strong> Install Anaconda/Miniconda or Poetry on your Linux system. Update NVIDIA drivers if you plan to test on a remote GPU (Colab). Create a new, clean Python 3.11 environment.</li>
<li><strong>2. Initialize the project:</strong> Structure the folders as described and start a git repo. Create a remote repository (GitHub) and make the first commit with README.md and pyproject.toml/requirements.txt.</li>
<li><strong>3. Configure Dev tools:</strong> Immediately set up pre-commit and Black/Ruff. Run pre-commit install so that every file you create will already be formatted and linted on commit. This prevents you from accumulating technical style debt.</li>
<li><strong>4. Retrieve sample datasets:</strong> Identify a small event (even just a pre/post image from Maxar Open Data or Copernicus EMS). Download them manually for now (via browser or AWS CLI) and place them in data/raw/. In parallel, request any necessary API keys (e.g., OpenAI API if you intend to use it).</li>
<li><strong>5. Study references:</strong> Dedicate a few hours to reading documentation for 1-2 key tools you&#39;ll use first. For example: basic rasterio syntax (read the official tutorial), basic FastAPI usage (how to define an endpoint and run uvicorn), and if you&#39;re unfamiliar, the pystac-client guide to understand how to query STAC data<a href="https://pystac-client.readthedocs.io/en/stable/usage.html#:~:text=The%20following%20code%20creates%20an,Microsoft%20Planetary%20Computer%20root%20catalog">[87]</a><a href="https://pystac-client.readthedocs.io/en/stable/usage.html#:~:text=,Microsoft%20Planetary%20Computer%20STAC%20API">[88]</a>. This initial investment will save you headaches later.</li>
<li><strong>6. Set config variables:</strong> Create an .env.example file containing expected keys (e.g., OPENAI_API_KEY=) and add it to git. Copy it to .env and populate for dev. Include in the README how to obtain and set these variables (e.g., link to the OpenAI page to create a key).</li>
<li><strong>7. Plan computational resources:</strong> Since you don&#39;t have a local GPU, decide how to test heavy parts. Example: configure a notebook on Colab with a linked GitHub repo, so you can perform inference on GPU there if needed. Or ensure the code is parameterized to use .to(&#39;cpu&#39;) as default and still functions.</li>
<li><strong>8. Communicate and document:</strong> Even if you are the sole developer, get used to maintaining clear changelogs or commit messages. For example, after Day1, write a commit &quot;setup environment, add base deps (rasterio, torch, etc.) - verify imports OK&quot;. This will help you track progress and facilitate potential involvement of other collaborators in the future.</li>
</ul>
<p>By following these actions in the first 24 hours, you will lay solid foundations: a ready environment, a repo configured with quality gates, and data available to start developing the <strong>core functionality</strong>. From there, you can iterate, gradually building the pipeline, knowing you have a professional <em>scaffolding</em> around you (tests, lint, containers) that supports you in producing reliable and reproducible code.</p>
<p><a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=Our%20main%20algorithm%20of%20choice,algorithm%20for%20semantic%20image%20segmentation">[1]</a> <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20data%20used%20for%20the,resolution%20color%20images">[54]</a> <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20images%20capture%2019%20natural,from%20all%20over%20the%20world">[55]</a> <a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge#:~:text=The%20U,sampler%20or%20the%20decoder">[84]</a> The xView2 AI Challenge | IBM</p>
<p><a href="https://www.ibm.com/think/insights/the-xview2-ai-challenge">https://www.ibm.com/think/insights/the-xview2-ai-challenge</a></p>
<p><a href="https://github.com/torchgeo/torchgeo#:~:text=Image%3A%20TorchGeo%20logo">[2]</a> <a href="https://github.com/torchgeo/torchgeo#:~:text=First%20we%27ll%20import%20various%20classes,used%20in%20the%20following%20sections">[3]</a> <a href="https://github.com/torchgeo/torchgeo#:~:text=from%20torchgeo,trainers%20import%20SemanticSegmentationTask">[77]</a> <a href="https://github.com/torchgeo/torchgeo#:~:text=datasets%20can%20be%20challenging%20to,reprojected%20into%20a%20matching%20CRS">[78]</a> GitHub - torchgeo/torchgeo: TorchGeo: datasets, samplers, transforms, and pre-trained models for geospatial data</p>
<p><a href="https://github.com/torchgeo/torchgeo">https://github.com/torchgeo/torchgeo</a></p>
<p><a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline#:~:text=search%20engines%20with%20LLMs%20to,more%20accurate%20and%20reliable%20answers">[4]</a> <a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline#:~:text=This%20teamwork%20reduces%20the%20chance,existent%20street">[5]</a> <a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline#:~:text=A%20RAG%20pipeline%20works%20like,The%20team%20includes">[9]</a> <a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline#:~:text=capture%20the%20essence%20of%20each,or%20another%20LLM%2C%20the%20model%E2%80%99s">[10]</a> <a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline#:~:text=Image%3A%20rag%20pipeline%20architecture%20diagram">[11]</a> <a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline#:~:text=find%20documents%20discussing%20%E2%80%9Creturns%2C%E2%80%9D%2C%20%E2%80%9Cdownloads%2C%20%E2%80%9D,of%20the%20context%20it%20receives">[12]</a> How to Build a RAG Pipeline: A Step-by-Step Guide</p>
<p><a href="https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline">https://www.meilisearch.com/blog/how-to-build-a-rag-pipepline</a></p>
<p><a href="https://github.com/langchain-ai/langchain#:~:text=LangChain%20is%20a%20framework%20for,as%20the%20underlying%20technology%20evolves">[6]</a> <a href="https://github.com/langchain-ai/langchain#:~:text=Why%20use%20LangChain%3F">[7]</a> GitHub - langchain-ai/langchain: The platform for reliable agents.</p>
<p><a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></p>
<p><a href="https://arxiv.org/html/2502.18470v5#:~:text=On%20the%20other%20hand%2C%20large,zhang2024bb%20%2C%20but%20the%20resulting">[8]</a> <a href="https://arxiv.org/html/2502.18470v5#:~:text=Answering%20real,the%20answering%20process%20as%20a">[79]</a> <a href="https://arxiv.org/html/2502.18470v5#:~:text=geographic%20relationships%20and%20semantic%20user,retriever%20that%20combines%20sparse%20spatial">[80]</a> Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Geospatial Reasoning Questions</p>
<p><a href="https://arxiv.org/html/2502.18470v5">https://arxiv.org/html/2502.18470v5</a></p>
<p><a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=The%20OG%20of%20Python%20package,be%20completely%20decoupled%20from%20a">[13]</a> <a href="https://dublog.net/blog/so-many%2Dpython%2Dpackage%2Dmanagers/#:~:text=One%20of%20the%20key%20faults,that%20are%20no%20longer%20useful">[14]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=Furthermore%2C%20as%20of%202024%2C%20the,actual%20conflict%20in%20the%20DAG">[17]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=">[18]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=The%20core%20tradeoff%20with%20,possible%20leading%20to%20a%20potentially">[19]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=tools%20like%20,to%20build%20and%20publish%20Python">[20]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=specify%20upper%20and%20lower%20bounds,intended%20to%20be%20used%20widely">[21]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=pdm">[22]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=Unlike%20the%20other%20tools%20on,on%20multiple%20versions%20of%20python">[23]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=The%20downside%20to%20,%E2%80%9Cidiomatic%E2%80%9D%20in%20the%20long%20run%E2%80%A6">[24]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=,it%20was%20released%20in%202022">[27]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=written%20in%20Rust%20and%20is,rye">[28]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=dependencies.%20In%20mid%202024%2C%20,for%20reproducibility">[30]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=One%20thing%20to%20note%20about,were%20using%20for%20different%20projects">[33]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=Verdict">[34]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=drop,it%20was%20released%20in%202022">[39]</a> <a href="https://dublog.net/blog/so-many-python-package-managers/#:~:text=linter%20notes,it%20was%20released%20in%202022">[40]</a> Python has too many package managers</p>
<p><a href="https://dublog.net/blog/so-many-python-package-managers/">https://dublog.net/blog/so-many-python-package-managers/</a></p>
<p><a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=Unlike%20pip%2C%20Conda%20package%20manager,python%2C%20we%20get%20following%20error">[15]</a> <a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=Python%20GDAL%20requires%20,while%20using%20Conda%20package%20manager">[16]</a> <a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application#:~:text=We%20are%20starting%20from%20Miniconda,of%20dockerfile%20are%20as%20follows">[45]</a> Docker image for geospatial python application</p>
<p><a href="https://www.geosynopsis.com/posts/docker-image-for-geospatial-application">https://www.geosynopsis.com/posts/docker-image-for-geospatial-application</a></p>
<p><a href="https://docs.astral.sh/uv/#:~:text=,boost%20with%20a%20familiar%20CLI">[25]</a> uv</p>
<p><a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a></p>
<p><a href="https://www.reddit.com/r/learnpython/comments/1fyvk0v/poetry_conda_pipenv_or_just_pip_what_are_you_using/#:~:text=updoot%20for%20,Here%20is%20their%20site">[26]</a> Poetry, Conda, Pipenv or just Pip. What are you using? : r/learnpython</p>
<p><a href="https://www.reddit.com/r/learnpython/comments/1fyvk0v/poetry_conda_pipenv_or_just_pip_what_are_you_using/">https://www.reddit.com/r/learnpython/comments/1fyvk0v/poetry_conda_pipenv_or_just_pip_what_are_you_using/</a></p>
<p><a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=benchmarks%20show%20that%20pixi%20is,on%20a%20M2%20MacBook%20Pro">[29]</a> <a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=In%20comparison%20with%20conda%2C%20pixi,conda%20packages%20for%20real%20reproducibility">[31]</a> <a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=Reason%203%3A%20No%20more%20Miniconda,base%20environment">[32]</a> <a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=At%20prefix%2C%20we%E2%80%99re%20solving%20conda,tasks%20for%20collaboration%2C%20and%20more">[35]</a> <a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative#:~:text=Image">[36]</a> 7 Reasons to Switch from Conda to Pixi | prefix.dev</p>
<p><a href="https://prefix.dev/blog/pixi_a_fast_conda_alternative">https://prefix.dev/blog/pixi_a_fast_conda_alternative</a></p>
<p><a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac#:~:text=%5Btool.ruff%5D%20target,by%20Black%20fix%20%3D%20true">[37]</a> <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac#:~:text=line,by%20Black%20fix%20%3D%20true">[38]</a> <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac#:~:text=%23%20pyproject.toml%20%5Btool.black%5D%20line,version%20%3D%20%5B%22py311">[41]</a> <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac#:~:text=%5Btool.mypy%5D%20python_version%20%3D%20,true%20plugins%20%3D">[42]</a> <a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac#:~:text=friction%20Python%20code%20quality%20with,commit%2C%20CI%2C%20and%20editor%20tips">[44]</a> 10 mypy/ruff/black/isort Combos for Zero-Friction Quality | by Syntal | Oct, 2025 | Medium</p>
<p><a href="https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac">https://medium.com/@sparknp1/10-mypy-ruff-black-isort-combos-for-zero-friction-quality-770a0fde94ac</a></p>
<p><a href="https://github.com/syrupy-project/syrupy#:~:text=syrupy,assert%20immutability%20of%20computed%20results">[43]</a> syrupy-project/syrupy: :pancakes: The sweeter pytest snapshot plugin</p>
<p><a href="https://github.com/syrupy-project/syrupy">https://github.com/syrupy-project/syrupy</a></p>
<p><a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=">[46]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=%5Bproduction%5D%20DATABASE_URL%20%3D%20,Dynaconf%20can%20handle%20encrypted%20secrets">[47]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=API_KEY%20%3D%20%22%40STRONGLY_ENCRYPTED%3Aprod_api_key_encrypted_value%22%20,can%20handle%20encrypted%20secrets">[48]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=">[49]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=%5Bdevelopment%5D%20DATABASE_URL%20%3D%20">[50]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=DATABASE_URL%20%3D%20">[51]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=">[52]</a> <a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration#:~:text=print%28f,Debug%20Mode%3A%20%7Bsettings.get%28%27DEBUG_MODE">[53]</a> Pydantic BaseSettings vs. Dynaconf A Modern Guide to Application Configuration | Leapcell</p>
<p><a href="https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration">https://leapcell.io/blog/pydantic-basesettings-vs-dynaconf-a-modern-guide-to-application-configuration</a></p>
<p><a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf#:~:text=Imagery%20openaccess,of%20imagery%20from%2015%20countries">[56]</a> [PDF] A Dataset for Assessing Building Damage from Satellite Imagery</p>
<p><a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf</a></p>
<p><a href="https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html#:~:text=The%20C2S,of%20chips%2C%20and%20water%20labels">[57]</a> Cloud to Street - Microsoft flood dataset - CMR Search</p>
<p><a href="https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html">https://cmr.earthdata.nasa.gov/search/concepts/C2781412798-MLHUB.html</a></p>
<p><a href="https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf#:~:text=The%20C2S,oz32gz">[58]</a> [PDF] C2SMS Floods - NET</p>
<p><a href="https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf">https://radiantearth.blob.core.windows.net/mlhub/c2smsfloods/Documentation.pdf</a></p>
<p><a href="https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1#:~:text=,to%20train%20and%20evaluate">[59]</a> (PDF) Sen1Floods11: a georeferenced dataset to train and test deep ...</p>
<p><a href="https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1">https://www.researchgate.net/publication/343275667_Sen1Floods11_a_georeferenced_dataset_to_train_and_test_deep_learning_flood_algorithms_for_Sentinel-1</a></p>
<p><a href="https://eod-grss-ieee.com/dataset-detail/ekFTRmNnWmtGOE52LzgrVUE4Ykd4dz09#:~:text=Sen1Floods11%20is%20a%20surface%20water,consists%20of%204%2C831%20512">[60]</a> Sen1Floods11 - Earth Observation Database</p>
<p><a href="https://eod-grss-ieee.com/dataset-detail/ekFTRmNnWmtGOE52LzgrVUE4Ykd4dz09">https://eod-grss-ieee.com/dataset-detail/ekFTRmNnWmtGOE52LzgrVUE4Ykd4dz09</a></p>
<p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf#:~:text=,ter%20data%20set">[61]</a> [PDF] Sen1Floods11: A Georeferenced Dataset to Train and Test Deep ...</p>
<p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf</a></p>
<p><a href="https://gee-community-catalog.org/projects/maxar_opendata/#:~:text=MAXAR%20Open%20Data%20Events%20,events%20like%20earthquakes%20and">[62]</a> MAXAR Open Data Events - awesome-gee-community-catalog</p>
<p><a href="https://gee-community-catalog.org/projects/maxar_opendata/">https://gee-community-catalog.org/projects/maxar_opendata/</a></p>
<p><a href="https://www.reddit.com/r/AirlinerAbduction2014/comments/17190ge/maxar_technologies_open_data_list_of_the_last/#:~:text=Maxar%20Technologies%20Open%20Data%20List,some%20of%20these%20datasets">[63]</a> Maxar Technologies Open Data List of the Last Decade of Global ...</p>
<p><a href="https://www.reddit.com/r/AirlinerAbduction2014/comments/17190ge/maxar_technologies_open_data_list_of_the_last/">https://www.reddit.com/r/AirlinerAbduction2014/comments/17190ge/maxar_technologies_open_data_list_of_the_last/</a></p>
<p><a href="https://essd.copernicus.org/articles/16/4817/2024/#:~:text=globally%20distributed%2C%20event,art%20semantic%20segmentation%20algorithms.%20These">[64]</a> ESSD - A globally distributed dataset of coseismic landslide mapping via multi-source high-resolution remote sensing images</p>
<p><a href="https://essd.copernicus.org/articles/16/4817/2024/">https://essd.copernicus.org/articles/16/4817/2024/</a></p>
<p><a href="https://github.com/microsoft/PlanetaryComputerExamples#:~:text=quickstarts%2C%20dataset%20examples%2C%20and%20tutorials">[65]</a> <a href="https://github.com/microsoft/PlanetaryComputerExamples#:~:text=If%20you%27re%20viewing%20this%20repository,quickstarts%2C%20dataset%20examples%2C%20and%20tutorials">[66]</a> GitHub - microsoft/PlanetaryComputerExamples: Examples of using the Planetary Computer</p>
<p><a href="https://github.com/microsoft/PlanetaryComputerExamples">https://github.com/microsoft/PlanetaryComputerExamples</a></p>
<p><a href="https://registry.opendata.aws/radiant-mlhub/#:~:text=Radiant%20MLHub%20,as%20other%20training%20data">[67]</a> Radiant MLHub - Registry of Open Data on AWS</p>
<p><a href="https://registry.opendata.aws/radiant-mlhub/">https://registry.opendata.aws/radiant-mlhub/</a></p>
<p><a href="https://medium.com/radiant-earth-insights/geospatial-models-now-available-in-radiant-mlhub-a41eb795d7d7#:~:text=Radiant%20MLHub%20has%20been%20the,ML%29%20algorithms%20since%202019">[68]</a> Geospatial Models Now Available in Radiant MLHub - Medium</p>
<p><a href="https://medium.com/radiant-earth-insights/geospatial-models-now-available-in-radiant-mlhub-a41eb795d7d7">https://medium.com/radiant-earth-insights/geospatial-models-now-available-in-radiant-mlhub-a41eb795d7d7</a></p>
<p><a href="https://storms.ngs.noaa.gov/storms/ian/index.html#:~:text=Hurricane%20IAN%20Imagery%20Hurricane%20IAN,by%20the%20NOAA%20Remote">[69]</a> Hurricane IAN Imagery</p>
<p><a href="https://storms.ngs.noaa.gov/storms/ian/index.html">https://storms.ngs.noaa.gov/storms/ian/index.html</a></p>
<p><a href="https://esri-disasterresponse.hub.arcgis.com/pages/imagery#:~:text=disasterresponse,major%20earthquakes%2C%20floods%2C%20storms">[70]</a> Imagery - Esri Disaster Response Program - ArcGIS Online</p>
<p><a href="https://esri-disasterresponse.hub.arcgis.com/pages/imagery">https://esri-disasterresponse.hub.arcgis.com/pages/imagery</a></p>
<p><a href="https://www.mdpi.com/2072-4292/16/11/1886#:~:text=The%20Diverse%20Mountainous%20Landslide%20Dataset,across%20different%20terrain%20in">[71]</a> The Diverse Mountainous Landslide Dataset (DMLD) - MDPI</p>
<p><a href="https://www.mdpi.com/2072-4292/16/11/1886">https://www.mdpi.com/2072-4292/16/11/1886</a></p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2666592124000568#:~:text=Landslide%20detection%20based%20on%20deep,detect%20landslides%20in%20Linzhi%20City">[72]</a> Landslide detection based on deep learning and remote sensing ...</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2666592124000568">https://www.sciencedirect.com/science/article/pii/S2666592124000568</a></p>
<p><a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/#:~:text=%E2%80%A2%20%207mo%20ago">[73]</a> <a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/#:~:text=%E2%80%A2%20%206mo%20ago">[74]</a> <a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/#:~:text=This,to">[75]</a> Rasterio vs Rioxarray : r/remotesensing</p>
<p><a href="https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/">https://www.reddit.com/r/remotesensing/comments/1k4wuf5/rasterio_vs_rioxarray/</a></p>
<p><a href="https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/#:~:text=Geospatial%20deep%20learning%20with%20TorchGeo,models%20specific%20to%20geospatial%20data">[76]</a> Geospatial deep learning with TorchGeo - PyTorch</p>
<p><a href="https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/">https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/</a></p>
<p><a href="https://github.com/hwchase17/langchain/commits#:~:text=Commits%20%C2%B7%20langchain,%C2%B7%20fix%28infra%29%3A">[81]</a> Commits ¬∑ langchain-ai/langchain - GitHub</p>
<p><a href="https://github.com/hwchase17/langchain/commits">https://github.com/hwchase17/langchain/commits</a></p>
<p><a href="https://github.com/run-llama/llama_index/releases#:~:text=Release%20Notes.%20%5B2025,core%20%5B0.14.0%5D.%20breaking%3A%20bumped">[82]</a> Releases ¬∑ run-llama/llama_index - GitHub</p>
<p><a href="https://github.com/run-llama/llama_index/releases">https://github.com/run-llama/llama_index/releases</a></p>
<p><a href="https://github.com/DIUx-xView/xView2_first_place#:~:text=DIUx,idea%20will%20be%20improved%20further">[83]</a> DIUx-xView/xView2_first_place: 1st place solution for &quot;xView2 - GitHub</p>
<p><a href="https://github.com/DIUx-xView/xView2_first_place">https://github.com/DIUx-xView/xView2_first_place</a></p>
<p><a href="https://github.com/SpaceNetChallenge/SpaceNet8#:~:text=Algorithmic%20baseline%20for%20SpaceNet%208,%C2%B7%20Finetune%20the">[85]</a> Algorithmic baseline for SpaceNet 8 Challenge - GitHub</p>
<p><a href="https://github.com/SpaceNetChallenge/SpaceNet8">https://github.com/SpaceNetChallenge/SpaceNet8</a></p>
<p><a href="https://nocomplexity.com/documents/fossml/generatedfiles/rastervision.html#:~:text=Complexity%20nocomplexity,metric%21%20Stars%20count%20are">[86]</a> Raster Vision - Free and Open Machine Learning - NO Complexity</p>
<p><a href="https://nocomplexity.com/documents/fossml/generatedfiles/rastervision.html">https://nocomplexity.com/documents/fossml/generatedfiles/rastervision.html</a></p>
<p><a href="https://pystac-client.readthedocs.io/en/stable/usage.html#:~:text=The%20following%20code%20creates%20an,Microsoft%20Planetary%20Computer%20root%20catalog">[87]</a> <a href="https://pystac-client.readthedocs.io/en/stable/usage.html#:~:text=,Microsoft%20Planetary%20Computer%20STAC%20API">[88]</a> Usage - pystac-client 0.8.5 documentation</p>
<p><a href="https://pystac-client.readthedocs.io/en/stable/usage.html">https://pystac-client.readthedocs.io/en/stable/usage.html</a></p>
        </div>
      </section>
    </article>
  </main>
  <footer>
    <span data-it="¬© 2026 Mirko Calcaterra. Tutti i diritti riservati."
          data-en="¬© 2026 Mirko Calcaterra. All rights reserved.">
      ¬© 2026 Mirko Calcaterra. Tutti i diritti riservati.
    </span>
  </footer>
  <script>
    const BLOG_LANG_KEY = 'blogLang';
    const BLOG_THEME_KEY = 'blogTheme';
    const CURRENT_LANG = "en";
    const OTHER_LANG = "it";
    const OTHER_LANG_LINK = "../../../blog/it/geoai-startingpoint/index.html";
    (function() {
      const body = document.body;
      const themeToggle = document.querySelector('.theme-toggle');
      const themeThumb = document.querySelector('.theme-toggle .theme-thumb');
      const langBtn = document.querySelector('.lang-btn');
      const tocElement = document.querySelector('.post-toc');
      const tocToggle = tocElement ? tocElement.querySelector('.post-toc__toggle') : null;
      const tocToggleText = tocElement ? tocElement.querySelector('.post-toc__toggle-text') : null;
      const tocTitle = tocElement ? tocElement.querySelector('.post-toc__title') : null;
      const tocLinks = tocElement ? Array.from(tocElement.querySelectorAll('.post-toc__link')) : [];
      const headingEntries = tocLinks
        .map((link) => {
          const id = link.getAttribute('href').slice(1);
          const target = document.getElementById(id);
          return target ? { link, target } : null;
        })
        .filter(Boolean);
      const tocLabels = CURRENT_LANG === 'it'
        ? { title: 'Indice', show: 'Mostra indice', hide: 'Nascondi indice' }
        : { title: 'Table of contents', show: 'Show table of contents', hide: 'Hide table of contents' };
      const tableWrappers = Array.from(document.querySelectorAll('.table-wrapper[data-enhanced-table]'));
      const tableLabels = CURRENT_LANG === 'it'
        ? { expand: 'Apri a schermo intero', close: 'Chiudi' }
        : { expand: 'Open full view', close: 'Close' };
      const codeBlocks = Array.from(document.querySelectorAll('.post-body pre'));
      const codeCopyLabels = {
        it: { copy: 'Copia', copied: 'Copiato!' },
        en: { copy: 'Copy', copied: 'Copied!' },
      };
      let tableOverlay = null;
      let tableOverlayScroll = null;
      let tableOverlayClose = null;
      if (tocTitle) {
        tocTitle.textContent = tocLabels.title;
      }
      if (tocToggleText) {
        tocToggleText.textContent = tocLabels.title;
      }
      let tocCollapsed = false;
      let tocManualOverride = false;
      const tocMediaQuery = window.matchMedia ? window.matchMedia('(max-width: 1024px)') : null;
      function ensureTableOverlay() {
        if (tableOverlay) {
          return;
        }
        tableOverlay = document.createElement('div');
        tableOverlay.className = 'table-overlay';
        tableOverlay.innerHTML =
          '<div class="table-overlay__content">' +
          '<button type="button" class="table-overlay__close">' + tableLabels.close + '</button>' +
          '<div class="table-overlay__scroll"></div>' +
          '</div>';
        body.appendChild(tableOverlay);
        tableOverlayScroll = tableOverlay.querySelector('.table-overlay__scroll');
        tableOverlayClose = tableOverlay.querySelector('.table-overlay__close');
        if (tableOverlayClose) {
          tableOverlayClose.setAttribute('aria-label', tableLabels.close);
          tableOverlayClose.addEventListener('click', closeTableOverlay);
        }
        tableOverlay.addEventListener('click', (event) => {
          if (event.target === tableOverlay) {
            closeTableOverlay();
          }
        });
      }
      function closeTableOverlay() {
        if (!tableOverlay) {
          return;
        }
        tableOverlay.classList.remove('table-overlay--visible');
        body.classList.remove('no-scroll');
        if (tableOverlayScroll) {
          tableOverlayScroll.innerHTML = '';
        }
      }
      function openTableOverlay(wrapper) {
        ensureTableOverlay();
        if (!tableOverlay || !tableOverlayScroll) {
          return;
        }
        tableOverlayScroll.innerHTML = '';
        const table = wrapper.querySelector('table');
        if (table) {
          const clone = table.cloneNode(true);
          const tableSize = table.dataset.tableSize;
          if (tableSize) {
            clone.dataset.tableSize = tableSize;
          }
          tableOverlayScroll.appendChild(clone);
        }
        tableOverlay.classList.add('table-overlay--visible');
        body.classList.add('no-scroll');
        if (tableOverlayClose) {
          tableOverlayClose.focus();
        }
      }
      function enhanceTables() {
        if (!tableWrappers.length) {
          return;
        }
        tableWrappers.forEach((wrapper) => {
          if (wrapper.dataset.enhanced === 'true') {
            return;
          }
          const table = wrapper.querySelector('table');
          if (!table) {
            return;
          }
          const headerCells = table.querySelectorAll('thead th');
          const referenceCells = headerCells.length ? headerCells : table.querySelectorAll('tr:first-child > *');
          const columnCount = referenceCells.length;
          let tableSize = '';
          if (columnCount >= 6) {
            tableSize = 'wide';
          } else if (columnCount >= 4) {
            tableSize = 'medium';
          }
          if (tableSize) {
            wrapper.setAttribute('data-table-size', tableSize);
            table.dataset.tableSize = tableSize;
          }
          const expandBtn = document.createElement('button');
          expandBtn.type = 'button';
          expandBtn.className = 'table-wrapper__expand';
          expandBtn.innerHTML = '<span aria-hidden="true">üîç</span> ' + tableLabels.expand;
          expandBtn.setAttribute('aria-label', tableLabels.expand);
          expandBtn.addEventListener('click', () => openTableOverlay(wrapper));
          wrapper.appendChild(expandBtn);
          wrapper.dataset.enhanced = 'true';
        });
      }
      function fallbackCopy(text) {
        const textarea = document.createElement('textarea');
        textarea.value = text;
        textarea.setAttribute('readonly', '');
        textarea.style.position = 'fixed';
        textarea.style.opacity = '0';
        textarea.style.left = '-9999px';
        document.body.appendChild(textarea);
        textarea.select();
        let successful = false;
        try {
          successful = document.execCommand('copy');
        } catch (error) {
          successful = false;
        }
        textarea.remove();
        return successful;
      }
      function showCopyFeedback(button, labels) {
        if (button._copyTimeout) {
          clearTimeout(button._copyTimeout);
        }
        const labelEl = button.querySelector('.code-copy-btn__text');
        button.classList.add('code-copy-btn--copied');
        if (labelEl) {
          labelEl.textContent = labels.copied;
        }
        button._copyTimeout = window.setTimeout(() => {
          button.classList.remove('code-copy-btn--copied');
          if (labelEl) {
            labelEl.textContent = labels.copy;
          }
        }, 2000);
      }
      function enhanceCodeBlocks() {
        if (!codeBlocks.length) {
          return;
        }
        const labels = codeCopyLabels[CURRENT_LANG] || codeCopyLabels.en;
        codeBlocks.forEach((pre) => {
          if (pre.dataset.copyEnhanced === 'true') {
            return;
          }
          const code = pre.querySelector('code');
          if (!code) {
            return;
          }
          const button = document.createElement('button');
          button.type = 'button';
          button.className = 'code-copy-btn';
          button.setAttribute('aria-label', labels.copy);
          button.innerHTML =
            '<span class="code-copy-btn__icon" aria-hidden="true">üìã</span>' +
            '<span class="code-copy-btn__text">' + labels.copy + '</span>';
          button.addEventListener('click', async () => {
            const text = (code.textContent || '').replace(/s+$/, '');
            if (!text) {
              return;
            }
            let copied = false;
            if (navigator.clipboard && typeof navigator.clipboard.writeText === 'function') {
              try {
                await navigator.clipboard.writeText(text);
                copied = true;
              } catch (error) {
                copied = false;
              }
            }
            if (!copied) {
              copied = fallbackCopy(text);
            }
            if (copied) {
              showCopyFeedback(button, labels);
            }
          });
          pre.appendChild(button);
          pre.dataset.copyEnhanced = 'true';
        });
      }
      function setTocCollapsed(collapsed, { manual = false } = {}) {
        if (!tocElement) {
          return;
        }
        tocCollapsed = Boolean(collapsed);
        if (manual) {
          tocManualOverride = true;
        }
        tocElement.classList.toggle('post-toc--collapsed', tocCollapsed);
        tocElement.setAttribute('data-collapsed', tocCollapsed ? 'true' : 'false');
        if (tocToggle) {
          tocToggle.setAttribute('aria-expanded', tocCollapsed ? 'false' : 'true');
          tocToggle.setAttribute('aria-label', tocCollapsed ? tocLabels.show : tocLabels.hide);
        }
      }
      function initToc() {
        if (!tocElement) {
          return;
        }
        if (tocToggle) {
          tocToggle.addEventListener('click', () => {
            setTocCollapsed(!tocCollapsed, { manual: true });
          });
        }
        if (tocMediaQuery) {
          const handleMediaChange = (event) => {
            if (tocManualOverride) {
              return;
            }
            setTocCollapsed(event.matches);
          };
          if (typeof tocMediaQuery.addEventListener === 'function') {
            tocMediaQuery.addEventListener('change', handleMediaChange);
          } else if (typeof tocMediaQuery.addListener === 'function') {
            tocMediaQuery.addListener(handleMediaChange);
          }
          setTocCollapsed(tocMediaQuery.matches);
        } else {
          setTocCollapsed(false);
        }
      }
      const storedTheme = (localStorage.getItem(BLOG_THEME_KEY) || '').toLowerCase();
      const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
      const initialTheme = storedTheme === 'light' ? 'light' : (storedTheme === 'dark' ? 'dark' : (prefersDark ? 'dark' : 'light'));
      let activeLink = null;
      let ticking = false;
      function applyTheme(theme) {
        const resolved = theme === 'dark' ? 'dark' : 'light';
        body.setAttribute('data-theme', resolved);
        if (themeToggle) {
          themeToggle.classList.toggle('active', resolved === 'dark');
        }
        if (themeThumb) {
          themeThumb.textContent = resolved === 'dark' ? 'üåô' : '‚òÄÔ∏è';
        }
        localStorage.setItem(BLOG_THEME_KEY, resolved);
      }
      function setActive(link) {
        if (activeLink === link) {
          return;
        }
        if (activeLink) {
          activeLink.classList.remove('post-toc__link--active');
        }
        if (link) {
          link.classList.add('post-toc__link--active');
        }
        activeLink = link;
      }
      function updateActiveHeading() {
        if (!headingEntries.length) {
          return;
        }
        const scrollPosition = window.scrollY + 160;
        let current = headingEntries[0];
        for (const item of headingEntries) {
          if (item.target.offsetTop <= scrollPosition) {
            current = item;
          } else {
            break;
          }
        }
        setActive(current.link);
      }
      function onScroll() {
        if (ticking) {
          return;
        }
        ticking = true;
        window.requestAnimationFrame(() => {
          updateActiveHeading();
          ticking = false;
        });
      }
      document.addEventListener('keydown', (event) => {
        if (event.key === 'Escape') {
          closeTableOverlay();
        }
      });
      enhanceTables();
      enhanceCodeBlocks();
      initToc();
      applyTheme(initialTheme);
      if (themeToggle) {
        themeToggle.addEventListener('click', () => {
          applyTheme(body.getAttribute('data-theme') === 'dark' ? 'light' : 'dark');
        });
      }
      if (langBtn) {
        langBtn.textContent = CURRENT_LANG === 'it' ? 'EN' : 'IT';
        if (OTHER_LANG_LINK) {
          langBtn.addEventListener('click', () => {
            localStorage.setItem(BLOG_LANG_KEY, OTHER_LANG);
            window.location.href = OTHER_LANG_LINK;
          });
        } else {
          langBtn.disabled = true;
          langBtn.classList.add('lang-btn--disabled');
        }
      }
      localStorage.setItem(BLOG_LANG_KEY, CURRENT_LANG);
      if (headingEntries.length) {
        headingEntries.sort((a, b) => a.target.offsetTop - b.target.offsetTop);
        updateActiveHeading();
        window.addEventListener('scroll', onScroll, { passive: true });
      }
    })();
  </script>
</body>
</html>